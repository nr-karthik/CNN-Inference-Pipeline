{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3cc8982-7eff-4414-aeb6-cfcb9ef760da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math, shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision, torchvision.transforms as T\n",
    "\n",
    "from models import *\n",
    "from models.vgg_quant_part2 import VGG16_quant_part2   \n",
    "from models.quant_layer import QuantConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268ddef2-cfd2-4af7-b7ac-b956aa0b4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Config -----------------\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE  = 128\n",
    "PRINT_FREQ  = 100\n",
    "EPOCHS      = 200\n",
    "LR_BASE     = 0.1\n",
    "WEIGHT_DECAY= 1e-4\n",
    "\n",
    "# Part 2 spec\n",
    "NBIT_W      = 4    # weights 4-bit\n",
    "NBIT_A      = 2    # activations 2-bit\n",
    "RESULT_DIR  = \"result/Part2_VGG_2bitA_4bitW\"\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# This is the squeezed 16->16 conv index in model.features\n",
    "SQUEEZE_IDX = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c15595-18f3-4f3e-a4c7-f008b7752ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Helpers -----------------\n",
    "class AverageMeter:\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0; self.avg = 0; self.sum = 0; self.count = 0\n",
    "    def update(self, v, n=1):\n",
    "        self.val = v\n",
    "        self.sum += v * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / max(self.count, 1)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def save_ckpt(state, is_best, fdir, tag):\n",
    "    p = os.path.join(fdir, f\"ckpt_{tag}.pth\")\n",
    "    torch.save(state, p)\n",
    "    if is_best:\n",
    "        shutil.copyfile(p, os.path.join(fdir, f\"best_{tag}.pth\"))\n",
    "\n",
    "MILESTONES = (60, 120, 150)\n",
    "GAMMA_LR   = 0.1\n",
    "def set_lr_epoch(optim, base_lr, epoch):\n",
    "    drops = sum(m <= epoch for m in MILESTONES)\n",
    "    lr = base_lr * (GAMMA_LR ** drops)\n",
    "    for g in optim.param_groups:\n",
    "        g[\"lr\"] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b109170a-6d26-4bfb-b720-a9bc1cde625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Data -----------------\n",
    "normalize = T.Normalize(mean=[0.491,0.482,0.447],\n",
    "                        std=[0.247,0.243,0.262])\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "val_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_tf)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=val_tf)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fab7649-855f-46d8-a669-6d044a77c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Model utils -----------------\n",
    "def set_bitwidth(model, nbit_w:int, nbit_a:int):\n",
    "    \"\"\"\n",
    "    Set bitwidth on all QuantConv2d layers.\n",
    "    Works with templates that have attributes:\n",
    "      - nbit_w, nbit_a, or\n",
    "      - weight_quant.bitwidth / act_quant.bitwidth\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, QuantConv2d):\n",
    "            if hasattr(m, 'nbit_w'): m.nbit_w = nbit_w\n",
    "            if hasattr(m, 'nbit_a'): m.nbit_a = nbit_a\n",
    "            if hasattr(m, 'weight_quant') and hasattr(m.weight_quant, 'bitwidth'):\n",
    "                m.weight_quant.bitwidth = nbit_w\n",
    "            if hasattr(m, 'act_quant') and hasattr(m.act_quant, 'bitwidth'):\n",
    "                m.act_quant.bitwidth = nbit_a\n",
    "    return model\n",
    "\n",
    "def build_vgg_part2():\n",
    "    # We already modified vgg_quant so one conv layer has 16 in/out channels\n",
    "    # and the BN after that layer is removed.\n",
    "    model = VGG16_quant_part2()          \n",
    "    model = set_bitwidth(model, NBIT_W, NBIT_A)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------- Train / Validate -----------------\n",
    "def train_one_epoch(loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    top1  = AverageMeter()\n",
    "    st = time.time()\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x = x.to(DEVICE); y = y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        prec1 = accuracy(out, y)[0]\n",
    "        losses.update(loss.item(), x.size(0))\n",
    "        top1.update(prec1.item(), x.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            print(f\"Epoch[{epoch}] Iter[{i}/{len(loader)}] \"\n",
    "                  f\"Loss {losses.val:.4f}({losses.avg:.4f})  \"\n",
    "                  f\"Acc {top1.val:.2f}%({top1.avg:.2f}%)\")\n",
    "\n",
    "def validate(loader, model, criterion):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1  = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x = x.to(DEVICE); y = y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "\n",
    "            prec1 = accuracy(out, y)[0]\n",
    "            losses.update(loss.item(), x.size(0))\n",
    "            top1.update(prec1.item(), x.size(0))\n",
    "\n",
    "            if i % PRINT_FREQ == 0:\n",
    "                print(f\"Val Iter[{i}/{len(loader)}] \"\n",
    "                      f\"Loss {losses.val:.4f}({losses.avg:.4f})  \"\n",
    "                      f\"Acc {top1.val:.2f}%({top1.avg:.2f}%)\")\n",
    "    print(f\"* Acc {top1.avg:.2f}%\")\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ffd6c-60e3-4e17-9cc1-d9af487e99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Train Part 2 model -----------------\n",
    "model = build_vgg_part2().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR_BASE,\n",
    "                      momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "resume_path = os.path.join(RESULT_DIR, \"best_vgg_2A4W.pth\")\n",
    "start_epoch = 0\n",
    "best_acc = 0.0\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    ckpt = torch.load(resume_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "    if \"optimizer\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    best_acc = float(ckpt.get(\"best\", 0.0))\n",
    "    start_epoch = int(ckpt.get(\"epoch\", 0)) + 1\n",
    "    print(f\"Resumed best@{best_acc:.2f}% from epoch {start_epoch-1}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    lr = set_lr_epoch(optimizer, LR_BASE, epoch)\n",
    "    print(f\"\\n=== Epoch {epoch}  LR={lr:.5f} ===\")\n",
    "    train_one_epoch(trainloader, model, criterion, optimizer, epoch)\n",
    "    acc = validate(testloader, model, criterion)\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(best_acc, float(acc))\n",
    "    save_ckpt({\n",
    "        \"epoch\": epoch,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"best\": best_acc,\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "    }, is_best, RESULT_DIR, tag=\"vgg_2A4W\")\n",
    "\n",
    "print(f\"Best Part2 VGG Acc (2A/4W): {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed07fcb-e3aa-4563-b3a4-49dbf583e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- psum_recovered for squeezed 16x16 layer -----------------\n",
    "# We now:\n",
    "# 1) Grab input to squeezed conv (pre-hook)\n",
    "# 2) Grab input to NEXT layer (pre-hook) => output after conv+ReLU\n",
    "# 3) Reconstruct psum_recovered from integer conv and compare.\n",
    "\n",
    "def try_get_attr(obj, names):\n",
    "    for n in names:\n",
    "        if hasattr(obj, n):\n",
    "            return getattr(obj, n)\n",
    "    return None\n",
    "\n",
    "def qparams(alpha: torch.Tensor, nbit: int, signed: bool):\n",
    "    if signed:\n",
    "        qmax = (2**(nbit-1)) - 1\n",
    "        qmin = -(2**(nbit-1))\n",
    "    else:\n",
    "        qmax = (2**nbit) - 1\n",
    "        qmin = 0\n",
    "    delta = alpha / qmax\n",
    "    return qmin, qmax, delta\n",
    "\n",
    "def to_broadcast(alpha: torch.Tensor, w: torch.Tensor):\n",
    "    # alpha scalar or [outC]\n",
    "    if alpha.dim() == 0:\n",
    "        return alpha\n",
    "    if alpha.dim() == 1 and alpha.numel() == w.size(0):\n",
    "        return alpha.view(-1,1,1,1)\n",
    "    return alpha.max()  # fallback scalar\n",
    "\n",
    "\n",
    "# Load best ckpt for analysis\n",
    "if os.path.isfile(resume_path):\n",
    "    ckpt = torch.load(resume_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "model.eval()\n",
    "\n",
    "features = getattr(model, \"features\", None)\n",
    "assert isinstance(features, nn.Sequential), \"Expected model.features to be nn.Sequential\"\n",
    "\n",
    "# --- auto-find the 16x16 squeezed QuantConv2d ---\n",
    "squeeze_idx = None\n",
    "for i, m in enumerate(features):\n",
    "    if isinstance(m, QuantConv2d) and getattr(m, \"in_channels\", None) == 16 and getattr(m, \"out_channels\", None) == 16:\n",
    "        squeeze_idx = i\n",
    "        break\n",
    "\n",
    "assert squeeze_idx is not None, \"Could not find 16x16 QuantConv2d\"\n",
    "squeezed_layer = features[squeeze_idx]\n",
    "\n",
    "# \"next layer\": first module AFTER this block (after conv+ReLU)\n",
    "# skip BN / ReLU / Identity and hook the next real layer (conv or pool)\n",
    "next_idx = squeeze_idx + 1\n",
    "while isinstance(features[next_idx], (nn.BatchNorm2d, nn.ReLU, nn.Identity)):\n",
    "    next_idx += 1\n",
    "next_layer = features[next_idx]\n",
    "\n",
    "_cached = {}\n",
    "\n",
    "def pre_squeezed_hook(m, inp):\n",
    "    _cached[\"x_in\"] = inp[0].detach().to(DEVICE)\n",
    "\n",
    "def pre_next_hook(m, inp):\n",
    "    _cached[\"x_next_in\"] = inp[0].detach().to(DEVICE)\n",
    "\n",
    "h1 = squeezed_layer.register_forward_pre_hook(pre_squeezed_hook)\n",
    "h2 = next_layer.register_forward_pre_hook(pre_next_hook)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xb, _ = next(iter(testloader))\n",
    "    xb = xb.to(DEVICE)\n",
    "    _ = model(xb)\n",
    "\n",
    "h1.remove(); h2.remove()\n",
    "x_in     = _cached[\"x_in\"]        # input to 16x16 conv\n",
    "x_next   = _cached[\"x_next_in\"]   # input to next layer (after conv+ReLU)\n",
    "\n",
    "# Get quant parameters from squeezed_layer (QuantConv2d)\n",
    "assert isinstance(squeezed_layer, QuantConv2d), \"Squeezed layer is not QuantConv2d\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Weights (quantized or float)\n",
    "    w_q = try_get_attr(squeezed_layer, [\"weight_q\"])\n",
    "    if w_q is None:\n",
    "        w_float = squeezed_layer.weight.detach()\n",
    "        w_alpha_fb = w_float.abs().max()\n",
    "        qmin_w, qmax_w, delta_w = qparams(w_alpha_fb, NBIT_W, signed=True)\n",
    "        w_int_tmp = torch.clamp(torch.round(w_float / delta_w), qmin_w, qmax_w)\n",
    "        w_q = (w_int_tmp * delta_w).to(w_float.dtype)\n",
    "    else:\n",
    "        w_q = w_q.detach()\n",
    "\n",
    "    # Weight alpha\n",
    "    w_alpha = None\n",
    "    wq_mod = try_get_attr(squeezed_layer, [\"weight_quant\"])\n",
    "    if wq_mod is not None:\n",
    "        w_alpha = try_get_attr(wq_mod, [\"alpha\", \"scale\", \"s\", \"delta\", \"a\"])\n",
    "        if isinstance(w_alpha, (float, int)):\n",
    "            w_alpha = torch.tensor(w_alpha, device=DEVICE, dtype=w_q.dtype)\n",
    "        if w_alpha is not None:\n",
    "            w_alpha = w_alpha.detach()\n",
    "    if w_alpha is None:\n",
    "        w_alpha = w_q.abs().max()\n",
    "    w_alpha_b = to_broadcast(w_alpha, w_q)\n",
    "\n",
    "    # Activation alpha\n",
    "    aq_mod = try_get_attr(squeezed_layer, [\"act_quant\"])\n",
    "    x_signed = bool(try_get_attr(aq_mod, [\"signed\"])) if aq_mod is not None else False\n",
    "    x_alpha = try_get_attr(aq_mod, [\"alpha\", \"scale\", \"s\", \"delta\", \"a\"])\n",
    "    if isinstance(x_alpha, (float, int)):\n",
    "        x_alpha = torch.tensor(x_alpha, device=DEVICE, dtype=x_in.dtype)\n",
    "    if x_alpha is None:\n",
    "        # post-ReLU activations are >=0 â†’ unsigned\n",
    "        x_alpha = x_in.detach().max()\n",
    "    x_alpha = x_alpha.to(DEVICE)\n",
    "\n",
    "    # qparams for weights / activations\n",
    "    qmin_w, qmax_w, delta_w = qparams(w_alpha_b, NBIT_W, signed=True)\n",
    "    qmin_x, qmax_x, delta_x = qparams(x_alpha,   NBIT_A, signed=x_signed)\n",
    "\n",
    "    # Quantize x to integers\n",
    "    x_int = torch.clamp(torch.round(x_in / delta_x), qmin_x, qmax_x).to(torch.int32)\n",
    "    x_q   = x_int.float() * delta_x\n",
    "\n",
    "    # Quantize weights to integers (based on w_q and delta_w)\n",
    "    w_int = torch.round(w_q / delta_w).to(torch.int32)\n",
    "\n",
    "    # Integer conv (no bias), then scale back, add bias, ReLU\n",
    "    stride   = squeezed_layer.stride\n",
    "    padding  = squeezed_layer.padding\n",
    "    groups   = squeezed_layer.groups\n",
    "    bias     = squeezed_layer.bias\n",
    "\n",
    "    psum_int = F.conv2d(x_int.float(), w_int.float(), bias=None,\n",
    "                        stride=stride, padding=padding, groups=groups)\n",
    "    psum_fp  = psum_int * (delta_x * delta_w)  # broadcast if per-channel\n",
    "\n",
    "    if bias is not None:\n",
    "        psum_fp = psum_fp + bias.view(1, -1, 1, 1)\n",
    "\n",
    "    psum_relu = torch.clamp(psum_fp, min=0.0)  # ReLU\n",
    "\n",
    "# Reference: conv(x_q, w_q) with same stride/padding, then ReLU\n",
    "with torch.no_grad():\n",
    "    conv_ref = nn.Conv2d(\n",
    "        in_channels=w_q.size(1),\n",
    "        out_channels=w_q.size(0),\n",
    "        kernel_size=w_q.shape[2:],\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        bias=(bias is not None)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    conv_ref.weight = nn.Parameter(w_q.clone())\n",
    "    if bias is not None:\n",
    "        conv_ref.bias = nn.Parameter(bias.clone())\n",
    "\n",
    "    y_ref = conv_ref(x_q)              # float conv on quantized activations\n",
    "    y_ref_relu = torch.clamp(y_ref, min=0.0)\n",
    "\n",
    "    mse = (psum_relu - y_ref_relu).pow(2).mean().item()\n",
    "    max_abs = (psum_relu - y_ref_relu).abs().max().item()\n",
    "\n",
    "print(f\"[Part2 psum_recovered] MSE vs conv_ref: {mse:.6e}\")\n",
    "print(f\"[Part2 psum_recovered] Max abs error  : {max_abs:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f801c-24a1-406e-be5a-d2378c67d84f",
   "metadata": {},
   "source": [
    "### Report Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d2c1ca-296f-4189-a6e9-7f838a9e6383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL REPORT =====\n",
      "Val Iter[0/79] Loss 0.2193(0.2193)  Acc 92.97%(92.97%)\n",
      "* Acc 90.27%\n",
      "VGG16 2A/4W  Acc: 90.27%   QuantErr(MSE): 7.330e-13   MaxAbs: 3.052e-05\n",
      "===== DONE =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _try_get_attr(obj, names):\n",
    "    for n in names:\n",
    "        if hasattr(obj, n):\n",
    "            return getattr(obj, n)\n",
    "    return None\n",
    "\n",
    "def _qparams(alpha: torch.Tensor, nbit: int, signed: bool):\n",
    "    if signed:\n",
    "        qmax = (2**(nbit-1)) - 1\n",
    "        qmin = -(2**(nbit-1))\n",
    "    else:\n",
    "        qmax = (2**nbit) - 1\n",
    "        qmin = 0\n",
    "    delta = alpha / qmax\n",
    "    return qmin, qmax, delta\n",
    "\n",
    "def _to_broadcast(alpha: torch.Tensor, w: torch.Tensor):\n",
    "    if alpha.dim() == 0:\n",
    "        return alpha\n",
    "    if alpha.dim() == 1 and alpha.numel() == w.size(0):\n",
    "        return alpha.view(-1,1,1,1)\n",
    "    return alpha.max()\n",
    "\n",
    "def _set_bitwidth(model, nbit_w:int, nbit_a:int):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, QuantConv2d):\n",
    "            if hasattr(m, 'nbit_w'): m.nbit_w = nbit_w\n",
    "            if hasattr(m, 'nbit_a'): m.nbit_a = nbit_a\n",
    "            if hasattr(m, 'weight_quant') and hasattr(m.weight_quant, 'bitwidth'):\n",
    "                m.weight_quant.bitwidth = nbit_w\n",
    "            if hasattr(m, 'act_quant') and hasattr(m.act_quant, 'bitwidth'):\n",
    "                m.act_quant.bitwidth = nbit_a\n",
    "    return model\n",
    "\n",
    "def compute_quant_error(model, testloader, nbit_a, nbit_w):\n",
    "    model.eval()\n",
    "    features = model.features\n",
    "    assert isinstance(features, nn.Sequential)\n",
    "\n",
    "    # find squeezed 16x16 QuantConv2d\n",
    "    squeeze_idx = None\n",
    "    for i, m in enumerate(features):\n",
    "        if isinstance(m, QuantConv2d) and getattr(m, \"in_channels\", None) == 16 and getattr(m, \"out_channels\", None) == 16:\n",
    "            squeeze_idx = i\n",
    "            break\n",
    "    assert squeeze_idx is not None, \"Could not find 16x16 QuantConv2d\"\n",
    "    squeezed_layer = features[squeeze_idx]\n",
    "\n",
    "    # hook input to that layer\n",
    "    _cached = {}\n",
    "    def pre_hook(m, inp):\n",
    "        _cached[\"x_in\"] = inp[0].detach().to(DEVICE)\n",
    "\n",
    "    h = squeezed_layer.register_forward_pre_hook(pre_hook)\n",
    "    with torch.no_grad():\n",
    "        xb, _ = next(iter(testloader))\n",
    "        xb = xb.to(DEVICE)\n",
    "        _ = model(xb)\n",
    "    h.remove()\n",
    "    x_in = _cached[\"x_in\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # weights (quantized or derive)\n",
    "        w_q = _try_get_attr(squeezed_layer, [\"weight_q\"])\n",
    "        if w_q is None:\n",
    "            w_float = squeezed_layer.weight.detach()\n",
    "            w_alpha_fb = w_float.abs().max()\n",
    "            qmin_w, qmax_w, delta_w = _qparams(w_alpha_fb, nbit_w, signed=True)\n",
    "            w_int_tmp = torch.clamp(torch.round(w_float / delta_w), qmin_w, qmax_w)\n",
    "            w_q = (w_int_tmp * delta_w).to(w_float.dtype)\n",
    "        else:\n",
    "            w_q = w_q.detach()\n",
    "\n",
    "        # weight alpha\n",
    "        w_alpha = None\n",
    "        wq_mod = _try_get_attr(squeezed_layer, [\"weight_quant\"])\n",
    "        if wq_mod is not None:\n",
    "            w_alpha = _try_get_attr(wq_mod, [\"alpha\", \"scale\", \"s\", \"delta\", \"a\"])\n",
    "            if isinstance(w_alpha, (float, int)):\n",
    "                w_alpha = torch.tensor(w_alpha, device=DEVICE, dtype=w_q.dtype)\n",
    "            if w_alpha is not None:\n",
    "                w_alpha = w_alpha.detach()\n",
    "        if w_alpha is None:\n",
    "            w_alpha = w_q.abs().max()\n",
    "        w_alpha_b = _to_broadcast(w_alpha, w_q)\n",
    "\n",
    "        # activation alpha\n",
    "        aq_mod = _try_get_attr(squeezed_layer, [\"act_quant\"])\n",
    "        x_signed = bool(_try_get_attr(aq_mod, [\"signed\"])) if aq_mod is not None else False\n",
    "        x_alpha = _try_get_attr(aq_mod, [\"alpha\", \"scale\", \"s\", \"delta\", \"a\"])\n",
    "        if isinstance(x_alpha, (float, int)):\n",
    "            x_alpha = torch.tensor(x_alpha, device=DEVICE, dtype=x_in.dtype)\n",
    "        if x_alpha is None:\n",
    "            x_alpha = x_in.detach().max()\n",
    "        x_alpha = x_alpha.to(DEVICE)\n",
    "\n",
    "        # qparams\n",
    "        qmin_w, qmax_w, delta_w = _qparams(w_alpha_b, nbit_w, signed=True)\n",
    "        qmin_x, qmax_x, delta_x = _qparams(x_alpha,   nbit_a, signed=x_signed)\n",
    "\n",
    "        # quantize\n",
    "        x_int = torch.clamp(torch.round(x_in / delta_x), qmin_x, qmax_x).to(torch.int32)\n",
    "        x_q   = x_int.float() * delta_x\n",
    "        w_int = torch.round(w_q / delta_w).to(torch.int32)\n",
    "\n",
    "        stride   = squeezed_layer.stride\n",
    "        padding  = squeezed_layer.padding\n",
    "        groups   = squeezed_layer.groups\n",
    "        bias     = squeezed_layer.bias\n",
    "\n",
    "        # recovered path\n",
    "        psum_int = F.conv2d(x_int.float(), w_int.float(), bias=None,\n",
    "                            stride=stride, padding=padding, groups=groups)\n",
    "        psum_fp  = psum_int * (delta_x * delta_w)\n",
    "        if bias is not None:\n",
    "            psum_fp = psum_fp + bias.view(1, -1, 1, 1)\n",
    "        psum_relu = torch.clamp(psum_fp, min=0.0)\n",
    "\n",
    "        # reference path: float conv on quantized tensors\n",
    "        conv_ref = nn.Conv2d(\n",
    "            in_channels=w_q.size(1),\n",
    "            out_channels=w_q.size(0),\n",
    "            kernel_size=w_q.shape[2:],\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=(bias is not None)\n",
    "        ).to(DEVICE)\n",
    "        conv_ref.weight = nn.Parameter(w_q.clone())\n",
    "        if bias is not None:\n",
    "            conv_ref.bias = nn.Parameter(bias.clone())\n",
    "\n",
    "        y_ref = conv_ref(x_q)\n",
    "        y_ref_relu = torch.clamp(y_ref, min=0.0)\n",
    "\n",
    "        mse = (psum_relu - y_ref_relu).pow(2).mean().item()\n",
    "        max_abs = (psum_relu - y_ref_relu).abs().max().item()\n",
    "\n",
    "    return mse, max_abs\n",
    "\n",
    "def eval_ckpt(ckpt_path, nbit_a, nbit_w):\n",
    "    model = VGG16_quant_part2().to(DEVICE)\n",
    "    model = _set_bitwidth(model, nbit_w, nbit_a)\n",
    "    ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "    crit = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    acc = validate(testloader, model, crit)\n",
    "    mse, max_abs = compute_quant_error(model, testloader, nbit_a, nbit_w)\n",
    "    return acc, mse, max_abs\n",
    "\n",
    "CKPT_2A4W = \"result/Part2_VGG_2bitA_4bitW/best_vgg_2A4W.pth\"\n",
    "\n",
    "print(\"\\n===== FINAL REPORT =====\")\n",
    "if os.path.isfile(CKPT_2A4W):\n",
    "    acc, mse, mx = eval_ckpt(CKPT_2A4W, nbit_a=2, nbit_w=4)\n",
    "    print(f\"VGG16 2A/4W  Acc: {acc:.2f}%   QuantErr(MSE): {mse:.3e}   MaxAbs: {mx:.3e}\")\n",
    "else:\n",
    "    print(\"VGG16 2A/4W  (checkpoint not found, skipped)\")\n",
    "print(\"===== DONE =====\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167b476-e796-4c6f-ae7c-fb7177215bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

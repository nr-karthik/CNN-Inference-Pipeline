{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant_aware_trained\"\n",
    "model = VGG16_quant()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # error = torch.mean(torch.abs(captured['psum_recovered'] - captured['next_input']))\n",
    "\n",
    "        # After output = model(input), inside train() or validate()\n",
    "        # if 'psum_recovered' in captured and 'next_input' in captured:\n",
    "        #     # Apply ReLU to recovered psum to match next layer's input\n",
    "        #     psum_relu = F.relu(captured['psum_recovered'])\n",
    "        #     next_input = captured['next_input']\n",
    "        #     error = torch.mean(torch.abs(psum_relu - next_input)).item()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "\n",
    "            # if 'psum_recovered' in captured and 'next_input' in captured:\n",
    "            #     # Apply ReLU to recovered psum to match next layer's input\n",
    "            #     psum_relu = F.relu(captured['psum_recovered'])\n",
    "            #     next_input = captured['next_input']\n",
    "            #     error = torch.mean(torch.abs(psum_relu - next_input)).item()\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8504f-3dbd-4f40-b7b2-0f77090dc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter) ## If you run this line, the next data batch is called subsequently.\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 0.02\n",
    "weight_decay = 5e-4\n",
    "epochs = 170\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1).cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "714dd931-5ea1-473a-9f61-eee2619cd747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for base case\n",
      "Test: [0/79]\tTime 0.107 (0.107)\tLoss 0.2514 (0.2514)\tPrec 94.531% (94.531%)\n",
      " * Prec 92.020% \n",
      "First conv layer’s weights’ absolute sum =  tensor(117.6888, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fdir = 'result/VGG16_quant/model_best.pth.tar'\n",
    "\n",
    "checkpoint = torch.load(fdir)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "print('Accuracy for base case')\n",
    "prec = validate(testloader, model, criterion)\n",
    "print('First conv layer’s weights’ absolute sum = ', model.features[0].weight.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c36cb6-29fa-471f-8b96-82bff631a059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A/4W Quantization MSE: 2.620530e-07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.quant_layer import QuantConv2d\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- find the 8×8 squeezed QuantConv2d ----\n",
    "features = model.features\n",
    "sq_idx = None\n",
    "for i, m in enumerate(features):\n",
    "    if isinstance(m, QuantConv2d) and m.in_channels == 8 and m.out_channels == 8:\n",
    "        sq_idx = i\n",
    "        break\n",
    "assert sq_idx is not None, \"No 8×8 squeezed conv found\"\n",
    "sq = features[sq_idx]\n",
    "\n",
    "# next real layer\n",
    "nx = sq_idx + 1\n",
    "while isinstance(features[nx], (nn.BatchNorm2d, nn.ReLU, nn.Identity)):\n",
    "    nx += 1\n",
    "next_layer = features[nx]\n",
    "\n",
    "# ---- hook inputs ----\n",
    "_cache = {}\n",
    "captured = {}  \n",
    "def hook_sq(m, inp): captured[\"x\"] = inp[0].detach().to(DEVICE)\n",
    "def hook_nx(m, inp): captured[\"y\"] = inp[0].detach().to(DEVICE)\n",
    "\n",
    "h1 = sq.register_forward_pre_hook(hook_sq)\n",
    "h2 = next_layer.register_forward_pre_hook(hook_nx)\n",
    "\n",
    "# ---- run 1 batch ----\n",
    "dummy = torch.randn(1, 3, 32, 32).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    model(dummy)\n",
    "\n",
    "h1.remove()\n",
    "h2.remove()\n",
    "\n",
    "assert \"x\" in captured\n",
    "x_in = captured[\"x\"]\n",
    "\n",
    "# ---- quantization params ----\n",
    "def qparams(alpha, nbit, signed):\n",
    "    if signed:\n",
    "        qmax = (2**(nbit-1))-1\n",
    "        qmin = -(2**(nbit-1))\n",
    "    else:\n",
    "        qmax = (2**nbit)-1\n",
    "        qmin = 0\n",
    "    scale = alpha / qmax  # use scale only\n",
    "    return scale\n",
    "\n",
    "# ---- quantize weights ----\n",
    "w_q = getattr(sq, \"weight_q\", None)\n",
    "w = sq.weight.detach()\n",
    "alpha_w = w.abs().max()\n",
    "d_w = qparams(alpha_w, 4, True)  # compute scale in all cases\n",
    "\n",
    "if w_q is None:\n",
    "    w_int = torch.clamp(torch.round(w / d_w), -(2**3), (2**3)-1)\n",
    "    w_q = w_int * d_w\n",
    "else:\n",
    "    w_q = w_q.detach()\n",
    "\n",
    "# ---- quantize activation ----\n",
    "aq = getattr(sq, \"act_quant\", None)\n",
    "alpha_x = getattr(aq, \"alpha\", x_in.abs().max())\n",
    "signed_x = getattr(aq, \"signed\", False)\n",
    "\n",
    "d_x = qparams(alpha_x, 4, signed_x)  # only scale\n",
    "x_int = torch.round(x_in / d_x)\n",
    "x_q = x_int * d_x\n",
    "\n",
    "# ---- integer conv ----\n",
    "stride, pad, groups = sq.stride, sq.padding, sq.groups\n",
    "bias = sq.bias\n",
    "\n",
    "psum_int = F.conv2d(x_int.float(), torch.round(w_q/d_w).float(),\n",
    "                    bias=None, stride=stride, padding=pad, groups=groups)\n",
    "psum_fp = psum_int * (d_x * d_w)\n",
    "if bias is not None:\n",
    "    psum_fp = psum_fp + bias.view(1,-1,1,1)\n",
    "psum_relu = torch.clamp(psum_fp, min=0.0)\n",
    "\n",
    "# ---- reference conv(x_q, w_q) ----\n",
    "conv_ref = nn.Conv2d(\n",
    "    in_channels=w_q.size(1),\n",
    "    out_channels=w_q.size(0),\n",
    "    kernel_size=w_q.shape[2:],\n",
    "    stride=stride,\n",
    "    padding=pad,\n",
    "    bias=(bias is not None)\n",
    ").to(DEVICE)\n",
    "conv_ref.weight = nn.Parameter(w_q.clone())\n",
    "if bias is not None:\n",
    "    conv_ref.bias = nn.Parameter(bias.clone())\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_ref = torch.clamp(conv_ref(x_q), min=0.0)\n",
    "\n",
    "# ---- final quantization MSE ----\n",
    "mse = (psum_relu - y_ref).pow(2).mean().item()\n",
    "print(f\"4A/4W Quantization MSE: {mse:.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f15574-2995-42f9-8930-54188185d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparsity(model):\n",
    "    total_zeros = 0\n",
    "    total_params = 0\n",
    "    layer_wise = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            weight = module.weight.data\n",
    "            zeros = torch.sum(weight == 0).item()\n",
    "            params = weight.numel()\n",
    "            sparsity = zeros / params * 100\n",
    "            total_zeros += zeros\n",
    "            total_params += params\n",
    "            layer_wise[name] = sparsity\n",
    "    total_sparsity = total_zeros / total_params * 100\n",
    "    return layer_wise, total_sparsity\n",
    "\n",
    "def compute_macs_vgg_quant(model, input_size=(3,32,32)):\n",
    "    C_in, H, W = input_size\n",
    "    total_macs = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            C_out, C_in_layer, kH, kW = module.weight.shape\n",
    "            strideH, strideW = module.stride\n",
    "            padH, padW = module.padding\n",
    "            H_out = (H + 2*padH - kH)//strideH + 1\n",
    "            W_out = (W + 2*padW - kW)//strideW + 1\n",
    "            total_macs += H_out * W_out * C_out * C_in_layer * kH * kW\n",
    "            H, W, C_in = H_out, W_out, C_out\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            total_macs += module.weight.shape[0] * module.weight.shape[1]\n",
    "    return total_macs\n",
    "\n",
    "def compute_model_size(model, bits_per_weight=32):\n",
    "    total_params = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            total_params += module.weight.numel()\n",
    "    return total_params * bits_per_weight / 8 / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abeaba8-0591-45ac-b86b-8c734bafd5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Unpruned Model ---\n",
      "Accuracy: 92.02%\n",
      "MACs: 12656579584\n",
      "Model Size (int4): 5.90 MB\n",
      "\n",
      "Metrics for Pruned Model ---\n",
      "Accuracy: 88.82%\n",
      "MACs: 12656579584\n",
      "Model Size (int4): 2.95 MB\n",
      "Estimated Memory Bandwidth Reduction: 49.98%\n",
      "\n",
      "Total model sparsity: 49.98%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------- Helper functions ----------------\n",
    "def compute_sparsity(model):\n",
    "    total_zeros = 0\n",
    "    total_params = 0\n",
    "    layer_wise = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            weight = module.weight.data\n",
    "            zeros = torch.sum(weight == 0).item()\n",
    "            params = weight.numel()\n",
    "            sparsity = zeros / params * 100\n",
    "            total_zeros += zeros\n",
    "            total_params += params\n",
    "            layer_wise[name] = sparsity\n",
    "    total_sparsity = total_zeros / total_params * 100\n",
    "    return layer_wise, total_sparsity\n",
    "\n",
    "def compute_macs_vgg_quant(model, input_size=(3,32,32)):\n",
    "    C_in, H, W = input_size\n",
    "    total_macs = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            C_out, C_in_layer, kH, kW = module.weight.shape\n",
    "            strideH, strideW = module.stride\n",
    "            padH, padW = module.padding\n",
    "            H_out = (H + 2*padH - kH)//strideH + 1\n",
    "            W_out = (W + 2*padW - kW)//strideW + 1\n",
    "            total_macs += H_out * W_out * C_out * C_in_layer * kH * kW\n",
    "            H, W, C_in = H_out, W_out, C_out\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            total_macs += module.weight.shape[0] * module.weight.shape[1]\n",
    "    return total_macs\n",
    "\n",
    "def compute_model_size(model, bits_per_weight=32):\n",
    "    total_params = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            weight = module.weight.data\n",
    "            total_params += torch.sum(weight != 0).item()  # count only non-zero weights\n",
    "    return total_params * bits_per_weight / 8 / 1024**2  # in MB\n",
    "\n",
    "# ---------------- Load Unpruned Model ----------------\n",
    "PATH = 'result/VGG16_quant/model_best.pth.tar'\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# Compute Accuracy for Unpruned\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "unpruned_acc = 100. * correct / len(testloader.dataset)\n",
    "unpruned_macs = compute_macs_vgg_quant(model, input_size=(3,32,32))\n",
    "unpruned_model_size = compute_model_size(model, bits_per_weight=4)  # quantized int4\n",
    "\n",
    "print(\"Metrics for Unpruned Model ---\")\n",
    "print(f\"Accuracy: {unpruned_acc:.2f}%\")\n",
    "print(f\"MACs: {unpruned_macs}\")\n",
    "print(f\"Model Size (int4): {unpruned_model_size:.2f} MB\")\n",
    "\n",
    "# ---------------- Load Pruned Model ----------------\n",
    "save_path = \"result/VGG16_quant/model_pruned_finetuned.pth.tar\"\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# Compute Accuracy for Pruned\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "pruned_acc = 100. * correct / len(testloader.dataset)\n",
    "pruned_macs = compute_macs_vgg_quant(model, input_size=(3,32,32))\n",
    "pruned_model_size = compute_model_size(model, bits_per_weight=4)  # quantized int4\n",
    "\n",
    "print(\"\\nMetrics for Pruned Model ---\")\n",
    "print(f\"Accuracy: {pruned_acc:.2f}%\")\n",
    "print(f\"MACs: {pruned_macs}\")\n",
    "print(f\"Model Size (int4): {pruned_model_size:.2f} MB\")\n",
    "\n",
    "# ---------------- Memory Bandwidth Reduction ----------------\n",
    "reduction_percent = (unpruned_model_size - pruned_model_size) / unpruned_model_size * 100\n",
    "print(f\"Estimated Memory Bandwidth Reduction: {reduction_percent:.2f}%\")\n",
    "layer_wise_sparsity, total_sparsity = compute_sparsity(model)\n",
    "print(f\"\\nTotal model sparsity: {total_sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send an input and grap the value by using prehook like HW3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f79977-1448-45fb-9ddd-66e53576dc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference: 1.5890691429376602e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "conv_layer = model.features[29]  # your squeezed quantized conv layer\n",
    "w_bit = 4\n",
    "x_bit = 4\n",
    "\n",
    "# ---------- Stochastic rounding function ----------\n",
    "def stochastic_round(tensor):\n",
    "    floor = torch.floor(tensor)\n",
    "    prob = tensor - floor\n",
    "    return floor + torch.bernoulli(prob)\n",
    "\n",
    "# ---------- Quantized weights ----------\n",
    "weight_q = conv_layer.weight_q                      # stored during training\n",
    "w_alpha = conv_layer.weight_quant.wgt_alpha.data    \n",
    "\n",
    "# Symmetric quantization delta (corrected)\n",
    "w_delta = w_alpha / (2**(w_bit - 1) - 1)\n",
    "\n",
    "# Use stochastic rounding instead of torch.round\n",
    "weight_int = stochastic_round(weight_q / w_delta).to(torch.int32)\n",
    "# print(weight_int.size())\n",
    "W_int= torch.reshape(weight_int, (weight_int.size(0),weight_int.size(1),-1))\n",
    "# print(W_int.size())\n",
    "Kij = 8;\n",
    "W = W_int[:,:,Kij]\n",
    "# print(W.size())\n",
    "\n",
    "# ---------- Quantized input activations ----------\n",
    "x = captured['x']                                    # input tensor to layer 29\n",
    "x_alpha = conv_layer.act_alpha.data\n",
    "\n",
    "# Symmetric activations quantization (0 to α → ReLU)\n",
    "x_delta = x_alpha / (2**x_bit - 1)\n",
    "\n",
    "# Quantize using same function used during training\n",
    "act_quant_fn = conv_layer.act_alq\n",
    "x_q = act_quant_fn(x, x_alpha)\n",
    "\n",
    "x_int = stochastic_round(x_q / x_delta).to(torch.int32)\n",
    "# print(x_int.size())\n",
    "x_int_b0 = x_int[0,:,:,:]\n",
    "# print(x_int_b0.size())\n",
    "X = torch.reshape(x_int_b0, (x_int_b0.size(0),-1)).float().cuda()\n",
    "# print(X.size())\n",
    "\n",
    "\n",
    "# ---------- INT psum simulation (Hardware-like) ----------\n",
    "output_int = F.conv2d(x_int.float(), weight_int.float(), bias=None,\n",
    "                      stride=conv_layer.stride,\n",
    "                      padding=conv_layer.padding)\n",
    "# print(output_int.size())\n",
    "\n",
    "output_recovered = torch.relu(output_int * (x_delta * w_delta))\n",
    "\n",
    "output_fp32 = torch.relu(conv_layer(x))\n",
    "\n",
    "difference = torch.abs(output_fp32 - output_recovered)\n",
    "print('Mean difference:', difference.mean().item())\n",
    "# np.savetxt(\"weight_int_layer29.txt\", weight_int.cpu().numpy().flatten(), fmt=\"%d\", delimiter=\",\")\n",
    "# np.savetxt(\"output_recovered.txt\", output_recovered.cpu().numpy().flatten(), fmt=\"%d\", delimiter=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57f613-2433-46a6-9ea9-885ee86bc35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('activation_final.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(X.size(1)):  # time step\n",
    "    for j in range(X.size(0)): # row #\n",
    "        X_bin = '{0:04b}'.format(round(X[7-j,i].item()))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "p_nij = range(X.size(1))\n",
    "psum = torch.zeros(8,16,9).cuda()\n",
    "print(W_int[0,0,0])\n",
    "\n",
    "print(psum.size())\n",
    "\n",
    "psum = torch.zeros(8, 16, 9).cuda()\n",
    "\n",
    "for kij in range(9):\n",
    "    for nij in range(16):\n",
    "        # Use raw signed weights, do NOT convert them to 0..15\n",
    "        weight = W_int[:, :, kij].float().cuda()   # keep negative values intact\n",
    "        x_input = X[:, nij].unsqueeze(0)          # shape [1,8]\n",
    "        psum[:, nij, kij] = torch.matmul(weight, x_input.squeeze(0))  # matmul preserves sign\n",
    "\n",
    "\n",
    "\n",
    "kij = 8\n",
    "\n",
    "psum_int = psum[:,:,kij]\n",
    "print(W_int[0,0,0])\n",
    "print(X[0,0])\n",
    "print(psum_int[0,0])\n",
    "\n",
    "bit_precision = 16\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('weight_kij_8.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(W.size(1)):  \n",
    "    for j in range(W.size(0)): \n",
    "       if (W[7-j,i] < 0):\n",
    "        W[7-j,i] = W[7-j,i] + 16\n",
    "       W_bin = '{0:04b}'.format(round(W[7-j,i].item()))\n",
    "       for k in range(bit_precision):\n",
    "           file.write(W_bin[k])        \n",
    "       #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "def int_to_unsigned_bits(x, bits=4):\n",
    "    \"\"\"Convert integer x to unsigned binary string of length `bits` (0 to 2^bits-1).\"\"\"\n",
    "    max_val = (1 << bits) - 1  # 15 for 4 bits\n",
    "    x_clipped = max(min(int(x), max_val), 0)  # clip to 0..15\n",
    "    return format(x_clipped, '0{}b'.format(bits))\n",
    "\n",
    "def write_activation_stream_row_major_unsigned(x_int, filename=\"x_int_layer29_hw_row_unsigned.txt\",\n",
    "                                               batch_index=0, bits=4):\n",
    "    \"\"\"\n",
    "    Converts quantized tensor x_int to 4-bit unsigned binary stream, **row by row**.\n",
    "    x_int: [B, 8, H, W]\n",
    "    batch_index: which batch to export\n",
    "    bits: bit precision\n",
    "    \"\"\"\n",
    "    assert x_int.shape[1] == 8, \"This helper expects 8 channels.\"\n",
    "    X = x_int[batch_index].cpu().detach().clone()  # [C=8, H, W]\n",
    "    C, H, W = X.shape\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('# Activation stream (row-major, unsigned): batch_index={}, 8 channels, {}-bit\\n'.format(batch_index, bits))\n",
    "        f.write('# Each line: one cycle, channels 7->0 concatenated\\n')\n",
    "        f.write('# Scanning order: for h in range(H): for w in range(W)\\n')\n",
    "\n",
    "        for h in range(H):        # row-first\n",
    "            for w in range(W):    # column inside the row\n",
    "                line = ''\n",
    "                for ch in reversed(range(C)):  # ch7 -> ch0\n",
    "                    val = X[ch, h, w].item()\n",
    "                    line += int_to_unsigned_bits(val, bits)\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "    print(f\"Written hardware stream (row-major, unsigned) to {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Usage\n",
    "write_activation_stream_row_major_unsigned(x_int, filename=\"x_int_layer29_hw_row_unsigned.txt\", batch_index=0, bits=4)\n",
    "\n",
    "# ---------- Function to convert signed int to 2's complement bits ----------\n",
    "def int_to_signed_bits(x, bits=4):\n",
    "    \"\"\"Convert integer x in range [-2^(bits-1), 2^(bits-1)-1] to 2's complement binary string.\"\"\"\n",
    "    min_val = -(1 << (bits-1))\n",
    "    max_val = (1 << (bits-1)) - 1\n",
    "    x_clipped = max(min(int(x), max_val), min_val)\n",
    "    if x_clipped < 0:\n",
    "        x_clipped = (1 << bits) + x_clipped\n",
    "    return format(x_clipped, '0{}b'.format(bits))\n",
    "\n",
    "# ---------- Write weights row-major for weight-stationary array ----------\n",
    "def write_weights_row_major_signed(weight_int, filename=\"weight_layer29_hw_row.txt\", bits=4):\n",
    "    \"\"\"\n",
    "    Converts conv weights to 4-bit signed binary stream, row by row (input channel dimension is row).\n",
    "    weight_int: [out_channels, in_channels, kH, kW]\n",
    "    \"\"\"\n",
    "    O, I, kH, kW = weight_int.shape\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f'# Weight stream (row-major, signed): {O}x{I}x{kH}x{kW}, {bits}-bit\\n')\n",
    "        f.write('# Each line: one cycle, input channels 0->7 concatenated\\n')\n",
    "        # For weight-stationary: scan output channels (rows) and kernel spatial positions\n",
    "        for oc in range(O):\n",
    "            for ic in range(I):\n",
    "                for kh in range(kH):\n",
    "                    for kw in range(kW):\n",
    "                        val = weight_int[oc, ic, kh, kw].item()\n",
    "                        bits_str = int_to_signed_bits(val, bits)\n",
    "                        f.write(bits_str)\n",
    "                    f.write('\\n')  # each line is one kernel element across channels\n",
    "    # print(f\"Written weight stream (row-major, signed) to {filename}\")\n",
    "    return filename\n",
    "\n",
    "# ---------- Usage ----------\n",
    "write_weights_row_major_signed(weight_int, filename=\"weight_layer29_hw_row.txt\", bits=4)\n",
    "\n",
    "# ---------- Write INT or Recovered outputs ----------\n",
    "def write_output_stream_row_major(output_tensor, filename=\"output_hw_row.txt\", bits=4, signed=False, batch_index=0):\n",
    "    \"\"\"\n",
    "    Converts output tensor to 4-bit row-major binary stream for hardware.\n",
    "    output_tensor: [B, C, H, W]\n",
    "    signed: whether to use signed 2's complement (-8..7) or unsigned (0..15)\n",
    "    \"\"\"\n",
    "    X = output_tensor[batch_index].cpu().detach().clone()  # [C, H, W]\n",
    "    C, H, W = X.shape\n",
    "\n",
    "    if signed:\n",
    "        int_to_bits = int_to_signed_bits\n",
    "    else:\n",
    "        # unsigned helper\n",
    "        def int_to_bits(x, bits=4):\n",
    "            x_clipped = max(min(int(round(x)), (1<<bits)-1), 0)\n",
    "            return format(x_clipped, '0{}b'.format(bits))\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f'# Output stream (row-major, {\"signed\" if signed else \"unsigned\"}): {C}x{H}x{W}, {bits}-bit\\n')\n",
    "        f.write('# Each line: one cycle, channels 7->0 concatenated\\n')\n",
    "        for h in range(H):        # row-first\n",
    "            for w in range(W):    # column inside row\n",
    "                line = ''\n",
    "                for ch in reversed(range(C)):  # ch7->ch0\n",
    "                    val = X[ch, h, w].item()\n",
    "                    line += int_to_bits(val, bits)\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "    # print(f\"Written output stream (row-major) to {filename}\")\n",
    "    return filename\n",
    "\n",
    "# ---------- Usage ----------\n",
    "write_output_stream_row_major(output_int, filename=\"output_int_hw_row.txt\", bits=4, signed=True)\n",
    "write_output_stream_row_major(output_recovered, filename=\"output_recovered_hw_row.txt\", bits=4, signed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b54845e3-33b7-45d8-b510-22e40db9bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading the original model again for structured pruning: \n",
      "\n",
      "Applying 50% Structured Pruning:\n",
      "Checking the accuracy after performing 50% structured pruning:\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 2.6400 (2.6400)\tPrec 10.156% (10.156%)\n",
      " * Prec 10.000% \n",
      "\n",
      " Training the structured-pruned model for gaining back accuracy :\n",
      "Epoch: [0][0/391]\tTime 0.249 (0.249)\tData 0.209 (0.209)\tLoss 2.4074 (2.4074)\tPrec 15.625% (15.625%)\n",
      "Epoch: [0][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 2.1087 (2.2385)\tPrec 20.312% (15.780%)\n",
      "Epoch: [0][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 1.8001 (2.1164)\tPrec 33.594% (19.831%)\n",
      "Epoch: [0][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 1.6515 (1.9922)\tPrec 39.062% (24.307%)\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 1.4444 (1.4444)\tPrec 48.438% (48.438%)\n",
      " * Prec 46.560% \n",
      "Epoch: [1][0/391]\tTime 0.295 (0.295)\tData 0.264 (0.264)\tLoss 1.3570 (1.3570)\tPrec 49.219% (49.219%)\n",
      "Epoch: [1][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 1.1242 (1.2156)\tPrec 62.500% (56.049%)\n",
      "Epoch: [1][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 1.0093 (1.1122)\tPrec 65.625% (60.479%)\n",
      "Epoch: [1][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.5857 (0.9944)\tPrec 80.469% (65.264%)\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.5255 (0.5255)\tPrec 82.812% (82.812%)\n",
      " * Prec 80.760% \n",
      "Epoch: [2][0/391]\tTime 0.340 (0.340)\tData 0.310 (0.310)\tLoss 0.4631 (0.4631)\tPrec 86.719% (86.719%)\n",
      "Epoch: [2][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.5288 (0.4743)\tPrec 85.938% (85.125%)\n",
      "Epoch: [2][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.4960 (0.4502)\tPrec 84.375% (85.735%)\n",
      "Epoch: [2][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.3404 (0.4342)\tPrec 89.062% (86.156%)\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.3804 (0.3804)\tPrec 89.844% (89.844%)\n",
      " * Prec 82.920% \n",
      "Epoch: [3][0/391]\tTime 0.333 (0.333)\tData 0.302 (0.302)\tLoss 0.3961 (0.3961)\tPrec 87.500% (87.500%)\n",
      "Epoch: [3][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.3216 (0.3248)\tPrec 88.281% (89.248%)\n",
      "Epoch: [3][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.3182 (0.3262)\tPrec 89.062% (89.269%)\n",
      "Epoch: [3][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.3123 (0.3284)\tPrec 89.062% (89.210%)\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.3357 (0.3357)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.200% \n",
      "Epoch: [4][0/391]\tTime 0.282 (0.282)\tData 0.244 (0.244)\tLoss 0.2292 (0.2292)\tPrec 90.625% (90.625%)\n",
      "Epoch: [4][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.2879 (0.2793)\tPrec 88.281% (90.586%)\n",
      "Epoch: [4][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.3678 (0.2834)\tPrec 87.500% (90.524%)\n",
      "Epoch: [4][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.2637 (0.2852)\tPrec 92.188% (90.586%)\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.3229 (0.3229)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.860% \n",
      "Epoch: [5][0/391]\tTime 0.263 (0.263)\tData 0.225 (0.225)\tLoss 0.3189 (0.3189)\tPrec 86.719% (86.719%)\n",
      "Epoch: [5][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.2503 (0.2650)\tPrec 89.844% (91.043%)\n",
      "Epoch: [5][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.1710 (0.2682)\tPrec 93.750% (90.951%)\n",
      "Epoch: [5][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.3310 (0.2644)\tPrec 88.281% (91.051%)\n",
      "Test: [0/79]\tTime 0.249 (0.249)\tLoss 0.3008 (0.3008)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.340% \n",
      "Epoch: [6][0/391]\tTime 0.338 (0.338)\tData 0.299 (0.299)\tLoss 0.1258 (0.1258)\tPrec 96.875% (96.875%)\n",
      "Epoch: [6][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.2004 (0.2402)\tPrec 93.750% (92.002%)\n",
      "Epoch: [6][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.1699 (0.2465)\tPrec 93.750% (91.764%)\n",
      "Epoch: [6][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1813 (0.2474)\tPrec 94.531% (91.687%)\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.2928 (0.2928)\tPrec 86.719% (86.719%)\n",
      " * Prec 87.750% \n",
      "Epoch: [7][0/391]\tTime 0.356 (0.356)\tData 0.316 (0.316)\tLoss 0.1484 (0.1484)\tPrec 96.094% (96.094%)\n",
      "Epoch: [7][100/391]\tTime 0.048 (0.052)\tData 0.001 (0.004)\tLoss 0.2153 (0.2323)\tPrec 92.969% (92.327%)\n",
      "Epoch: [7][200/391]\tTime 0.049 (0.051)\tData 0.002 (0.003)\tLoss 0.2933 (0.2357)\tPrec 89.062% (92.102%)\n",
      "Epoch: [7][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.3160 (0.2400)\tPrec 89.844% (92.011%)\n",
      "Test: [0/79]\tTime 0.246 (0.246)\tLoss 0.4413 (0.4413)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.190% \n",
      "Epoch: [8][0/391]\tTime 0.373 (0.373)\tData 0.333 (0.333)\tLoss 0.2099 (0.2099)\tPrec 93.750% (93.750%)\n",
      "Epoch: [8][100/391]\tTime 0.050 (0.053)\tData 0.001 (0.005)\tLoss 0.1387 (0.2159)\tPrec 93.750% (92.837%)\n",
      "Epoch: [8][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1082 (0.2195)\tPrec 97.656% (92.642%)\n",
      "Epoch: [8][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.1701 (0.2219)\tPrec 94.531% (92.481%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.3168 (0.3168)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.050% \n",
      "Epoch: [9][0/391]\tTime 0.329 (0.329)\tData 0.293 (0.293)\tLoss 0.1447 (0.1447)\tPrec 95.312% (95.312%)\n",
      "Epoch: [9][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.1655 (0.2034)\tPrec 94.531% (93.224%)\n",
      "Epoch: [9][200/391]\tTime 0.049 (0.051)\tData 0.002 (0.003)\tLoss 0.2828 (0.2064)\tPrec 93.750% (93.163%)\n",
      "Epoch: [9][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0752 (0.2086)\tPrec 97.656% (93.070%)\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.3421 (0.3421)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.970% \n",
      "Epoch: [10][0/391]\tTime 0.322 (0.322)\tData 0.291 (0.291)\tLoss 0.1955 (0.1955)\tPrec 92.969% (92.969%)\n",
      "Epoch: [10][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.2180 (0.2011)\tPrec 91.406% (93.247%)\n",
      "Epoch: [10][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.1796 (0.2006)\tPrec 92.969% (93.241%)\n",
      "Epoch: [10][300/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.2487 (0.2054)\tPrec 89.062% (93.015%)\n",
      "Test: [0/79]\tTime 0.162 (0.162)\tLoss 0.3133 (0.3133)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.550% \n",
      "Epoch: [11][0/391]\tTime 0.310 (0.310)\tData 0.280 (0.280)\tLoss 0.1404 (0.1404)\tPrec 96.094% (96.094%)\n",
      "Epoch: [11][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.1596 (0.1873)\tPrec 94.531% (93.735%)\n",
      "Epoch: [11][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.1898 (0.1965)\tPrec 95.312% (93.354%)\n",
      "Epoch: [11][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1694 (0.1955)\tPrec 93.750% (93.381%)\n",
      "Test: [0/79]\tTime 0.143 (0.143)\tLoss 0.3160 (0.3160)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.560% \n",
      "Epoch: [12][0/391]\tTime 0.245 (0.245)\tData 0.214 (0.214)\tLoss 0.1353 (0.1353)\tPrec 96.094% (96.094%)\n",
      "Epoch: [12][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1742 (0.1835)\tPrec 94.531% (93.928%)\n",
      "Epoch: [12][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1853 (0.1856)\tPrec 91.406% (93.874%)\n",
      "Epoch: [12][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1617 (0.1877)\tPrec 92.969% (93.706%)\n",
      "Test: [0/79]\tTime 0.176 (0.176)\tLoss 0.3406 (0.3406)\tPrec 92.188% (92.188%)\n",
      " * Prec 85.260% \n",
      "Epoch: [13][0/391]\tTime 0.262 (0.262)\tData 0.223 (0.223)\tLoss 0.0731 (0.0731)\tPrec 97.656% (97.656%)\n",
      "Epoch: [13][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.2055 (0.1805)\tPrec 92.188% (93.936%)\n",
      "Epoch: [13][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.2580 (0.1831)\tPrec 92.188% (93.855%)\n",
      "Epoch: [13][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1848 (0.1839)\tPrec 92.969% (93.872%)\n",
      "Test: [0/79]\tTime 0.170 (0.170)\tLoss 0.3395 (0.3395)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.310% \n",
      "Epoch: [14][0/391]\tTime 0.263 (0.263)\tData 0.230 (0.230)\tLoss 0.1882 (0.1882)\tPrec 94.531% (94.531%)\n",
      "Epoch: [14][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.2131 (0.1645)\tPrec 92.188% (94.624%)\n",
      "Epoch: [14][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1301 (0.1700)\tPrec 96.875% (94.364%)\n",
      "Epoch: [14][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.2876 (0.1750)\tPrec 91.406% (94.186%)\n",
      "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.3134 (0.3134)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.540% \n",
      "Epoch: [15][0/391]\tTime 0.255 (0.255)\tData 0.222 (0.222)\tLoss 0.1281 (0.1281)\tPrec 96.094% (96.094%)\n",
      "Epoch: [15][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.004)\tLoss 0.1074 (0.1605)\tPrec 95.312% (94.616%)\n",
      "Epoch: [15][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1889 (0.1690)\tPrec 92.188% (94.294%)\n",
      "Epoch: [15][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.1257 (0.1672)\tPrec 92.969% (94.360%)\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.3638 (0.3638)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.450% \n",
      "Epoch: [16][0/391]\tTime 0.277 (0.277)\tData 0.245 (0.245)\tLoss 0.2048 (0.2048)\tPrec 92.969% (92.969%)\n",
      "Epoch: [16][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1547 (0.1548)\tPrec 94.531% (94.670%)\n",
      "Epoch: [16][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1781 (0.1563)\tPrec 94.531% (94.632%)\n",
      "Epoch: [16][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.2233 (0.1678)\tPrec 95.312% (94.274%)\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 0.3743 (0.3743)\tPrec 87.500% (87.500%)\n",
      " * Prec 88.310% \n",
      "Epoch: [17][0/391]\tTime 0.238 (0.238)\tData 0.202 (0.202)\tLoss 0.1709 (0.1709)\tPrec 92.188% (92.188%)\n",
      "Epoch: [17][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.1169 (0.1591)\tPrec 96.875% (94.678%)\n",
      "Epoch: [17][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.2158 (0.1612)\tPrec 90.625% (94.481%)\n",
      "Epoch: [17][300/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0796 (0.1668)\tPrec 97.656% (94.285%)\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.4126 (0.4126)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.450% \n",
      "Epoch: [18][0/391]\tTime 0.244 (0.244)\tData 0.208 (0.208)\tLoss 0.1334 (0.1334)\tPrec 94.531% (94.531%)\n",
      "Epoch: [18][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.004)\tLoss 0.1468 (0.1557)\tPrec 96.875% (94.609%)\n",
      "Epoch: [18][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1371 (0.1551)\tPrec 96.094% (94.671%)\n",
      "Epoch: [18][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1795 (0.1583)\tPrec 92.969% (94.536%)\n",
      "Test: [0/79]\tTime 0.138 (0.138)\tLoss 0.4061 (0.4061)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.520% \n",
      "Epoch: [19][0/391]\tTime 0.261 (0.261)\tData 0.230 (0.230)\tLoss 0.1747 (0.1747)\tPrec 96.094% (96.094%)\n",
      "Epoch: [19][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.1032 (0.1513)\tPrec 95.312% (94.663%)\n",
      "Epoch: [19][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.2005 (0.1515)\tPrec 93.750% (94.718%)\n",
      "Epoch: [19][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.2016 (0.1575)\tPrec 92.969% (94.555%)\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.3429 (0.3429)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.750% \n",
      "Epoch: [20][0/391]\tTime 0.285 (0.285)\tData 0.248 (0.248)\tLoss 0.2514 (0.2514)\tPrec 93.750% (93.750%)\n",
      "Epoch: [20][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.1091 (0.1473)\tPrec 95.312% (94.926%)\n",
      "Epoch: [20][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1552 (0.1509)\tPrec 95.312% (94.850%)\n",
      "Epoch: [20][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1808 (0.1551)\tPrec 96.875% (94.614%)\n",
      "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.3380 (0.3380)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.450% \n",
      "Epoch: [21][0/391]\tTime 0.355 (0.355)\tData 0.315 (0.315)\tLoss 0.1046 (0.1046)\tPrec 94.531% (94.531%)\n",
      "Epoch: [21][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.2189 (0.1345)\tPrec 92.188% (95.312%)\n",
      "Epoch: [21][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1649 (0.1415)\tPrec 94.531% (95.138%)\n",
      "Epoch: [21][300/391]\tTime 0.053 (0.050)\tData 0.001 (0.002)\tLoss 0.0842 (0.1432)\tPrec 96.875% (95.081%)\n",
      "Test: [0/79]\tTime 0.144 (0.144)\tLoss 0.3155 (0.3155)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.120% \n",
      "Epoch: [22][0/391]\tTime 0.269 (0.269)\tData 0.233 (0.233)\tLoss 0.1690 (0.1690)\tPrec 95.312% (95.312%)\n",
      "Epoch: [22][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1850 (0.1433)\tPrec 92.188% (95.096%)\n",
      "Epoch: [22][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.1410 (0.1445)\tPrec 95.312% (95.056%)\n",
      "Epoch: [22][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1625 (0.1428)\tPrec 94.531% (95.110%)\n",
      "Test: [0/79]\tTime 0.255 (0.255)\tLoss 0.3004 (0.3004)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.440% \n",
      "Epoch: [23][0/391]\tTime 0.353 (0.353)\tData 0.319 (0.319)\tLoss 0.1794 (0.1794)\tPrec 92.969% (92.969%)\n",
      "Epoch: [23][100/391]\tTime 0.052 (0.052)\tData 0.001 (0.004)\tLoss 0.0470 (0.1231)\tPrec 98.438% (95.769%)\n",
      "Epoch: [23][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.2840 (0.1354)\tPrec 89.844% (95.324%)\n",
      "Epoch: [23][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1321 (0.1417)\tPrec 96.094% (95.105%)\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.2258 (0.2258)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.170% \n",
      "Epoch: [24][0/391]\tTime 0.248 (0.248)\tData 0.208 (0.208)\tLoss 0.0492 (0.0492)\tPrec 99.219% (99.219%)\n",
      "Epoch: [24][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.004)\tLoss 0.1545 (0.1363)\tPrec 94.531% (95.258%)\n",
      "Epoch: [24][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1562 (0.1371)\tPrec 96.094% (95.289%)\n",
      "Epoch: [24][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1281 (0.1418)\tPrec 95.312% (95.157%)\n",
      "Test: [0/79]\tTime 0.149 (0.149)\tLoss 0.2906 (0.2906)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.720% \n",
      "Epoch: [25][0/391]\tTime 0.253 (0.253)\tData 0.220 (0.220)\tLoss 0.1495 (0.1495)\tPrec 96.875% (96.875%)\n",
      "Epoch: [25][100/391]\tTime 0.049 (0.051)\tData 0.002 (0.003)\tLoss 0.1364 (0.1327)\tPrec 93.750% (95.452%)\n",
      "Epoch: [25][200/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.1720 (0.1357)\tPrec 92.969% (95.254%)\n",
      "Epoch: [25][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0811 (0.1381)\tPrec 96.875% (95.206%)\n",
      "Test: [0/79]\tTime 0.178 (0.178)\tLoss 0.2227 (0.2227)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.670% \n",
      "Epoch: [26][0/391]\tTime 0.304 (0.304)\tData 0.269 (0.269)\tLoss 0.1449 (0.1449)\tPrec 94.531% (94.531%)\n",
      "Epoch: [26][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0931 (0.1242)\tPrec 97.656% (95.854%)\n",
      "Epoch: [26][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.1146 (0.1258)\tPrec 96.094% (95.674%)\n",
      "Epoch: [26][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0887 (0.1285)\tPrec 96.875% (95.567%)\n",
      "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.3198 (0.3198)\tPrec 92.969% (92.969%)\n",
      " * Prec 87.610% \n",
      "Epoch: [27][0/391]\tTime 0.230 (0.230)\tData 0.197 (0.197)\tLoss 0.1452 (0.1452)\tPrec 94.531% (94.531%)\n",
      "Epoch: [27][100/391]\tTime 0.051 (0.051)\tData 0.001 (0.003)\tLoss 0.1763 (0.1319)\tPrec 93.750% (95.614%)\n",
      "Epoch: [27][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1998 (0.1327)\tPrec 92.188% (95.542%)\n",
      "Epoch: [27][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1837 (0.1349)\tPrec 93.750% (95.442%)\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.3879 (0.3879)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.230% \n",
      "Epoch: [28][0/391]\tTime 0.283 (0.283)\tData 0.252 (0.252)\tLoss 0.1470 (0.1470)\tPrec 94.531% (94.531%)\n",
      "Epoch: [28][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0810 (0.1290)\tPrec 97.656% (95.715%)\n",
      "Epoch: [28][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.1453 (0.1258)\tPrec 94.531% (95.728%)\n",
      "Epoch: [28][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1424 (0.1262)\tPrec 94.531% (95.650%)\n",
      "Test: [0/79]\tTime 0.141 (0.141)\tLoss 0.3589 (0.3589)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.620% \n",
      "Epoch: [29][0/391]\tTime 0.255 (0.255)\tData 0.217 (0.217)\tLoss 0.1394 (0.1394)\tPrec 96.094% (96.094%)\n",
      "Epoch: [29][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0911 (0.1168)\tPrec 96.094% (95.916%)\n",
      "Epoch: [29][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0940 (0.1224)\tPrec 96.875% (95.740%)\n",
      "Epoch: [29][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.2143 (0.1284)\tPrec 94.531% (95.608%)\n",
      "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.3097 (0.3097)\tPrec 93.750% (93.750%)\n",
      " * Prec 88.610% \n",
      "Epoch: [30][0/391]\tTime 0.251 (0.251)\tData 0.211 (0.211)\tLoss 0.1547 (0.1547)\tPrec 96.094% (96.094%)\n",
      "Epoch: [30][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0888 (0.1168)\tPrec 96.094% (95.800%)\n",
      "Epoch: [30][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1417 (0.1214)\tPrec 94.531% (95.752%)\n",
      "Epoch: [30][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1182 (0.1239)\tPrec 96.875% (95.772%)\n",
      "Test: [0/79]\tTime 0.179 (0.179)\tLoss 0.3806 (0.3806)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.210% \n",
      "Epoch: [31][0/391]\tTime 0.316 (0.316)\tData 0.277 (0.277)\tLoss 0.0867 (0.0867)\tPrec 97.656% (97.656%)\n",
      "Epoch: [31][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1262 (0.1150)\tPrec 95.312% (96.202%)\n",
      "Epoch: [31][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1401 (0.1143)\tPrec 95.312% (96.152%)\n",
      "Epoch: [31][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1415 (0.1212)\tPrec 96.094% (95.909%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.3501 (0.3501)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.840% \n",
      "Epoch: [32][0/391]\tTime 0.256 (0.256)\tData 0.223 (0.223)\tLoss 0.0456 (0.0456)\tPrec 98.438% (98.438%)\n",
      "Epoch: [32][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0855 (0.1160)\tPrec 96.875% (96.009%)\n",
      "Epoch: [32][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1426 (0.1118)\tPrec 95.312% (96.117%)\n",
      "Epoch: [32][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0750 (0.1151)\tPrec 96.875% (96.037%)\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.4622 (0.4622)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.810% \n",
      "Epoch: [33][0/391]\tTime 0.250 (0.250)\tData 0.216 (0.216)\tLoss 0.1337 (0.1337)\tPrec 95.312% (95.312%)\n",
      "Epoch: [33][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.1091 (0.1230)\tPrec 94.531% (95.854%)\n",
      "Epoch: [33][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0887 (0.1235)\tPrec 98.438% (95.899%)\n",
      "Epoch: [33][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.0667 (0.1234)\tPrec 98.438% (95.928%)\n",
      "Test: [0/79]\tTime 0.261 (0.261)\tLoss 0.2165 (0.2165)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.370% \n",
      "Epoch: [34][0/391]\tTime 0.276 (0.276)\tData 0.236 (0.236)\tLoss 0.0940 (0.0940)\tPrec 96.094% (96.094%)\n",
      "Epoch: [34][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0932 (0.1092)\tPrec 97.656% (96.210%)\n",
      "Epoch: [34][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0979 (0.1131)\tPrec 96.875% (96.094%)\n",
      "Epoch: [34][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1284 (0.1160)\tPrec 95.312% (95.998%)\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.2611 (0.2611)\tPrec 92.969% (92.969%)\n",
      " * Prec 88.570% \n",
      "Epoch: [35][0/391]\tTime 0.333 (0.333)\tData 0.293 (0.293)\tLoss 0.1450 (0.1450)\tPrec 94.531% (94.531%)\n",
      "Epoch: [35][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0408 (0.1035)\tPrec 99.219% (96.434%)\n",
      "Epoch: [35][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.2060 (0.1041)\tPrec 93.750% (96.374%)\n",
      "Epoch: [35][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0798 (0.1091)\tPrec 96.875% (96.234%)\n",
      "Test: [0/79]\tTime 0.115 (0.115)\tLoss 0.1948 (0.1948)\tPrec 92.969% (92.969%)\n",
      " * Prec 88.450% \n",
      "Epoch: [36][0/391]\tTime 0.412 (0.412)\tData 0.381 (0.381)\tLoss 0.1083 (0.1083)\tPrec 94.531% (94.531%)\n",
      "Epoch: [36][100/391]\tTime 0.049 (0.053)\tData 0.001 (0.005)\tLoss 0.0566 (0.1140)\tPrec 97.656% (95.939%)\n",
      "Epoch: [36][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0679 (0.1118)\tPrec 98.438% (96.102%)\n",
      "Epoch: [36][300/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1537 (0.1128)\tPrec 96.094% (96.060%)\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.3257 (0.3257)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.500% \n",
      "Epoch: [37][0/391]\tTime 0.252 (0.252)\tData 0.221 (0.221)\tLoss 0.1034 (0.1034)\tPrec 96.875% (96.875%)\n",
      "Epoch: [37][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1558 (0.1007)\tPrec 93.750% (96.519%)\n",
      "Epoch: [37][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1399 (0.1022)\tPrec 95.312% (96.533%)\n",
      "Epoch: [37][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1698 (0.1074)\tPrec 92.969% (96.340%)\n",
      "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.2443 (0.2443)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.150% \n",
      "Epoch: [38][0/391]\tTime 0.251 (0.251)\tData 0.212 (0.212)\tLoss 0.0772 (0.0772)\tPrec 96.875% (96.875%)\n",
      "Epoch: [38][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1290 (0.1064)\tPrec 96.094% (96.310%)\n",
      "Epoch: [38][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1159 (0.1087)\tPrec 95.312% (96.300%)\n",
      "Epoch: [38][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1460 (0.1071)\tPrec 92.188% (96.296%)\n",
      "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.2701 (0.2701)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.750% \n",
      "Epoch: [39][0/391]\tTime 0.247 (0.247)\tData 0.213 (0.213)\tLoss 0.1194 (0.1194)\tPrec 95.312% (95.312%)\n",
      "Epoch: [39][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1320 (0.1081)\tPrec 93.750% (96.341%)\n",
      "Epoch: [39][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1198 (0.1065)\tPrec 97.656% (96.296%)\n",
      "Epoch: [39][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1666 (0.1057)\tPrec 94.531% (96.356%)\n",
      "Test: [0/79]\tTime 0.162 (0.162)\tLoss 0.3671 (0.3671)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.290% \n",
      "Epoch: [40][0/391]\tTime 0.283 (0.283)\tData 0.213 (0.213)\tLoss 0.1275 (0.1275)\tPrec 96.094% (96.094%)\n",
      "Epoch: [40][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0662 (0.1024)\tPrec 96.875% (96.481%)\n",
      "Epoch: [40][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1365 (0.1042)\tPrec 94.531% (96.374%)\n",
      "Epoch: [40][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1299 (0.1061)\tPrec 92.969% (96.351%)\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3850 (0.3850)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.390% \n",
      "Epoch: [41][0/391]\tTime 0.263 (0.263)\tData 0.231 (0.231)\tLoss 0.0635 (0.0635)\tPrec 97.656% (97.656%)\n",
      "Epoch: [41][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0641 (0.0919)\tPrec 98.438% (96.767%)\n",
      "Epoch: [41][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0827 (0.0975)\tPrec 96.094% (96.552%)\n",
      "Epoch: [41][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0477 (0.1011)\tPrec 98.438% (96.410%)\n",
      "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.2867 (0.2867)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.550% \n",
      "Epoch: [42][0/391]\tTime 0.259 (0.259)\tData 0.228 (0.228)\tLoss 0.0447 (0.0447)\tPrec 99.219% (99.219%)\n",
      "Epoch: [42][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.1154 (0.1024)\tPrec 96.875% (96.682%)\n",
      "Epoch: [42][200/391]\tTime 0.050 (0.051)\tData 0.002 (0.002)\tLoss 0.0909 (0.1057)\tPrec 96.875% (96.494%)\n",
      "Epoch: [42][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1110 (0.1058)\tPrec 96.875% (96.506%)\n",
      "Test: [0/79]\tTime 0.162 (0.162)\tLoss 0.2894 (0.2894)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.900% \n",
      "Epoch: [43][0/391]\tTime 0.237 (0.237)\tData 0.200 (0.200)\tLoss 0.1155 (0.1155)\tPrec 97.656% (97.656%)\n",
      "Epoch: [43][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0768 (0.0927)\tPrec 96.875% (96.844%)\n",
      "Epoch: [43][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0747 (0.0973)\tPrec 98.438% (96.603%)\n",
      "Epoch: [43][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1149 (0.1016)\tPrec 95.312% (96.457%)\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.2846 (0.2846)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.640% \n",
      "Epoch: [44][0/391]\tTime 0.391 (0.391)\tData 0.357 (0.357)\tLoss 0.1244 (0.1244)\tPrec 96.094% (96.094%)\n",
      "Epoch: [44][100/391]\tTime 0.049 (0.053)\tData 0.001 (0.005)\tLoss 0.1352 (0.0944)\tPrec 95.312% (96.666%)\n",
      "Epoch: [44][200/391]\tTime 0.049 (0.051)\tData 0.002 (0.003)\tLoss 0.1864 (0.0960)\tPrec 93.750% (96.638%)\n",
      "Epoch: [44][300/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.1301 (0.1004)\tPrec 95.312% (96.483%)\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.1900 (0.1900)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.110% \n",
      "Epoch: [45][0/391]\tTime 0.229 (0.229)\tData 0.188 (0.188)\tLoss 0.0324 (0.0324)\tPrec 98.438% (98.438%)\n",
      "Epoch: [45][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0521 (0.0938)\tPrec 98.438% (96.844%)\n",
      "Epoch: [45][200/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.1050 (0.1005)\tPrec 96.094% (96.587%)\n",
      "Epoch: [45][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.1043 (0.0970)\tPrec 93.750% (96.699%)\n",
      "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.2768 (0.2768)\tPrec 92.969% (92.969%)\n",
      " * Prec 88.920% \n",
      "Epoch: [46][0/391]\tTime 0.259 (0.259)\tData 0.219 (0.219)\tLoss 0.0159 (0.0159)\tPrec 100.000% (100.000%)\n",
      "Epoch: [46][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0533 (0.0851)\tPrec 98.438% (96.991%)\n",
      "Epoch: [46][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1060 (0.0928)\tPrec 96.094% (96.762%)\n",
      "Epoch: [46][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0760 (0.0961)\tPrec 96.094% (96.574%)\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.3591 (0.3591)\tPrec 87.500% (87.500%)\n",
      " * Prec 88.760% \n",
      "Epoch: [47][0/391]\tTime 0.300 (0.300)\tData 0.259 (0.259)\tLoss 0.0655 (0.0655)\tPrec 96.875% (96.875%)\n",
      "Epoch: [47][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1113 (0.0881)\tPrec 95.312% (97.037%)\n",
      "Epoch: [47][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0617 (0.0884)\tPrec 97.656% (96.992%)\n",
      "Epoch: [47][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1271 (0.0932)\tPrec 92.969% (96.802%)\n",
      "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.3476 (0.3476)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.030% \n",
      "Epoch: [48][0/391]\tTime 0.246 (0.246)\tData 0.213 (0.213)\tLoss 0.0906 (0.0906)\tPrec 96.094% (96.094%)\n",
      "Epoch: [48][100/391]\tTime 0.046 (0.051)\tData 0.001 (0.003)\tLoss 0.1170 (0.0968)\tPrec 94.531% (96.844%)\n",
      "Epoch: [48][200/391]\tTime 0.046 (0.050)\tData 0.001 (0.002)\tLoss 0.2518 (0.0990)\tPrec 92.188% (96.618%)\n",
      "Epoch: [48][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1573 (0.0978)\tPrec 94.531% (96.670%)\n",
      "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.3482 (0.3482)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.250% \n",
      "Epoch: [49][0/391]\tTime 0.204 (0.204)\tData 0.129 (0.129)\tLoss 0.0552 (0.0552)\tPrec 98.438% (98.438%)\n",
      "Epoch: [49][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0577 (0.0846)\tPrec 98.438% (97.115%)\n",
      "Epoch: [49][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0850 (0.0894)\tPrec 96.094% (96.848%)\n",
      "Epoch: [49][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0905 (0.0926)\tPrec 97.656% (96.714%)\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.3233 (0.3233)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.790% \n",
      "Epoch: [50][0/391]\tTime 0.296 (0.296)\tData 0.265 (0.265)\tLoss 0.0855 (0.0855)\tPrec 97.656% (97.656%)\n",
      "Epoch: [50][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0591 (0.0846)\tPrec 97.656% (97.068%)\n",
      "Epoch: [50][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0372 (0.0842)\tPrec 98.438% (97.058%)\n",
      "Epoch: [50][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0936 (0.0865)\tPrec 96.875% (97.015%)\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.3101 (0.3101)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.140% \n",
      "Epoch: [51][0/391]\tTime 0.344 (0.344)\tData 0.307 (0.307)\tLoss 0.0628 (0.0628)\tPrec 97.656% (97.656%)\n",
      "Epoch: [51][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0381 (0.0883)\tPrec 98.438% (96.852%)\n",
      "Epoch: [51][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0671 (0.0900)\tPrec 96.094% (96.828%)\n",
      "Epoch: [51][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.0777 (0.0908)\tPrec 98.438% (96.872%)\n",
      "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.3118 (0.3118)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.220% \n",
      "Epoch: [52][0/391]\tTime 0.319 (0.319)\tData 0.288 (0.288)\tLoss 0.0293 (0.0293)\tPrec 100.000% (100.000%)\n",
      "Epoch: [52][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1501 (0.0883)\tPrec 95.312% (97.092%)\n",
      "Epoch: [52][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1692 (0.0916)\tPrec 95.312% (96.809%)\n",
      "Epoch: [52][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0307 (0.0958)\tPrec 99.219% (96.719%)\n",
      "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.3249 (0.3249)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.340% \n",
      "Epoch: [53][0/391]\tTime 0.255 (0.255)\tData 0.216 (0.216)\tLoss 0.0284 (0.0284)\tPrec 100.000% (100.000%)\n",
      "Epoch: [53][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0593 (0.0847)\tPrec 97.656% (97.177%)\n",
      "Epoch: [53][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0831 (0.0850)\tPrec 96.875% (97.248%)\n",
      "Epoch: [53][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1267 (0.0869)\tPrec 96.094% (97.111%)\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.3053 (0.3053)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.780% \n",
      "Epoch: [54][0/391]\tTime 0.275 (0.275)\tData 0.242 (0.242)\tLoss 0.1436 (0.1436)\tPrec 95.312% (95.312%)\n",
      "Epoch: [54][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1306 (0.0846)\tPrec 95.312% (97.215%)\n",
      "Epoch: [54][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1025 (0.0872)\tPrec 96.875% (97.023%)\n",
      "Epoch: [54][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0736 (0.0896)\tPrec 96.094% (96.924%)\n",
      "Test: [0/79]\tTime 0.181 (0.181)\tLoss 0.3578 (0.3578)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.590% \n",
      "Epoch: [55][0/391]\tTime 0.253 (0.253)\tData 0.220 (0.220)\tLoss 0.0246 (0.0246)\tPrec 100.000% (100.000%)\n",
      "Epoch: [55][100/391]\tTime 0.049 (0.051)\tData 0.002 (0.003)\tLoss 0.0293 (0.0848)\tPrec 99.219% (97.115%)\n",
      "Epoch: [55][200/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0647 (0.0866)\tPrec 97.656% (97.034%)\n",
      "Epoch: [55][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.1003 (0.0895)\tPrec 96.094% (96.901%)\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3286 (0.3286)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.160% \n",
      "Epoch: [56][0/391]\tTime 0.254 (0.254)\tData 0.218 (0.218)\tLoss 0.0528 (0.0528)\tPrec 98.438% (98.438%)\n",
      "Epoch: [56][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0345 (0.0852)\tPrec 98.438% (97.161%)\n",
      "Epoch: [56][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0737 (0.0864)\tPrec 96.875% (97.100%)\n",
      "Epoch: [56][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0798 (0.0868)\tPrec 96.875% (97.036%)\n",
      "Test: [0/79]\tTime 0.160 (0.160)\tLoss 0.4490 (0.4490)\tPrec 87.500% (87.500%)\n",
      " * Prec 88.750% \n",
      "Epoch: [57][0/391]\tTime 0.256 (0.256)\tData 0.221 (0.221)\tLoss 0.1486 (0.1486)\tPrec 94.531% (94.531%)\n",
      "Epoch: [57][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0546 (0.0822)\tPrec 98.438% (97.192%)\n",
      "Epoch: [57][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0961 (0.0874)\tPrec 97.656% (97.023%)\n",
      "Epoch: [57][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0504 (0.0910)\tPrec 97.656% (96.875%)\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3455 (0.3455)\tPrec 92.969% (92.969%)\n",
      " * Prec 88.920% \n",
      "Epoch: [58][0/391]\tTime 0.260 (0.260)\tData 0.220 (0.220)\tLoss 0.1354 (0.1354)\tPrec 96.875% (96.875%)\n",
      "Epoch: [58][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0667 (0.0772)\tPrec 96.094% (97.362%)\n",
      "Epoch: [58][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0503 (0.0780)\tPrec 99.219% (97.306%)\n",
      "Epoch: [58][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0865 (0.0783)\tPrec 97.656% (97.389%)\n",
      "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.3389 (0.3389)\tPrec 92.969% (92.969%)\n",
      " * Prec 88.030% \n",
      "Epoch: [59][0/391]\tTime 0.260 (0.260)\tData 0.224 (0.224)\tLoss 0.0695 (0.0695)\tPrec 96.875% (96.875%)\n",
      "Epoch: [59][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.1049 (0.0843)\tPrec 97.656% (97.177%)\n",
      "Epoch: [59][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1645 (0.0872)\tPrec 95.312% (97.054%)\n",
      "Epoch: [59][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1656 (0.0866)\tPrec 95.312% (97.070%)\n",
      "Test: [0/79]\tTime 0.169 (0.169)\tLoss 0.3535 (0.3535)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.110% \n",
      "Epoch: [60][0/391]\tTime 0.314 (0.314)\tData 0.281 (0.281)\tLoss 0.0472 (0.0472)\tPrec 97.656% (97.656%)\n",
      "Epoch: [60][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1381 (0.0830)\tPrec 94.531% (97.006%)\n",
      "Epoch: [60][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0792 (0.0833)\tPrec 98.438% (97.065%)\n",
      "Epoch: [60][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1627 (0.0849)\tPrec 93.750% (97.026%)\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 0.3024 (0.3024)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.160% \n",
      "Epoch: [61][0/391]\tTime 0.267 (0.267)\tData 0.235 (0.235)\tLoss 0.0777 (0.0777)\tPrec 96.094% (96.094%)\n",
      "Epoch: [61][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0400 (0.0836)\tPrec 99.219% (97.107%)\n",
      "Epoch: [61][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0749 (0.0811)\tPrec 97.656% (97.178%)\n",
      "Epoch: [61][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0669 (0.0847)\tPrec 97.656% (97.067%)\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.3036 (0.3036)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.940% \n",
      "Epoch: [62][0/391]\tTime 0.277 (0.277)\tData 0.238 (0.238)\tLoss 0.0139 (0.0139)\tPrec 100.000% (100.000%)\n",
      "Epoch: [62][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0762 (0.0810)\tPrec 96.875% (97.169%)\n",
      "Epoch: [62][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0883 (0.0835)\tPrec 95.312% (97.050%)\n",
      "Epoch: [62][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0545 (0.0815)\tPrec 98.438% (97.129%)\n",
      "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.2600 (0.2600)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.040% \n",
      "Epoch: [63][0/391]\tTime 0.264 (0.264)\tData 0.224 (0.224)\tLoss 0.0732 (0.0732)\tPrec 97.656% (97.656%)\n",
      "Epoch: [63][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0431 (0.0802)\tPrec 99.219% (97.347%)\n",
      "Epoch: [63][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0859 (0.0758)\tPrec 94.531% (97.435%)\n",
      "Epoch: [63][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1105 (0.0780)\tPrec 97.656% (97.355%)\n",
      "Test: [0/79]\tTime 0.260 (0.260)\tLoss 0.4518 (0.4518)\tPrec 86.719% (86.719%)\n",
      " * Prec 88.270% \n",
      "Epoch: [64][0/391]\tTime 0.252 (0.252)\tData 0.216 (0.216)\tLoss 0.0855 (0.0855)\tPrec 96.094% (96.094%)\n",
      "Epoch: [64][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0428 (0.0755)\tPrec 98.438% (97.401%)\n",
      "Epoch: [64][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0641 (0.0792)\tPrec 97.656% (97.229%)\n",
      "Epoch: [64][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0973 (0.0781)\tPrec 96.875% (97.257%)\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.2775 (0.2775)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.440% \n",
      "Epoch: [65][0/391]\tTime 0.257 (0.257)\tData 0.221 (0.221)\tLoss 0.0730 (0.0730)\tPrec 98.438% (98.438%)\n",
      "Epoch: [65][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0549 (0.0793)\tPrec 98.438% (97.293%)\n",
      "Epoch: [65][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0649 (0.0786)\tPrec 97.656% (97.244%)\n",
      "Epoch: [65][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0097 (0.0807)\tPrec 100.000% (97.184%)\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.3491 (0.3491)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.240% \n",
      "Epoch: [66][0/391]\tTime 0.256 (0.256)\tData 0.224 (0.224)\tLoss 0.0573 (0.0573)\tPrec 97.656% (97.656%)\n",
      "Epoch: [66][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0440 (0.0704)\tPrec 98.438% (97.741%)\n",
      "Epoch: [66][200/391]\tTime 0.047 (0.050)\tData 0.001 (0.002)\tLoss 0.0442 (0.0725)\tPrec 98.438% (97.516%)\n",
      "Epoch: [66][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0561 (0.0762)\tPrec 99.219% (97.415%)\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 0.3839 (0.3839)\tPrec 88.281% (88.281%)\n",
      " * Prec 89.270% \n",
      "Epoch: [67][0/391]\tTime 0.301 (0.301)\tData 0.260 (0.260)\tLoss 0.0447 (0.0447)\tPrec 97.656% (97.656%)\n",
      "Epoch: [67][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.2073 (0.0809)\tPrec 95.312% (97.362%)\n",
      "Epoch: [67][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0770 (0.0798)\tPrec 98.438% (97.330%)\n",
      "Epoch: [67][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1624 (0.0803)\tPrec 96.094% (97.311%)\n",
      "Test: [0/79]\tTime 0.111 (0.111)\tLoss 0.3109 (0.3109)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.250% \n",
      "Epoch: [68][0/391]\tTime 0.304 (0.304)\tData 0.269 (0.269)\tLoss 0.0955 (0.0955)\tPrec 96.875% (96.875%)\n",
      "Epoch: [68][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0414 (0.0756)\tPrec 97.656% (97.285%)\n",
      "Epoch: [68][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0763 (0.0753)\tPrec 97.656% (97.380%)\n",
      "Epoch: [68][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0407 (0.0761)\tPrec 99.219% (97.332%)\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.4196 (0.4196)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.380% \n",
      "Epoch: [69][0/391]\tTime 0.288 (0.288)\tData 0.249 (0.249)\tLoss 0.0237 (0.0237)\tPrec 100.000% (100.000%)\n",
      "Epoch: [69][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0794 (0.0689)\tPrec 96.875% (97.734%)\n",
      "Epoch: [69][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1141 (0.0753)\tPrec 97.656% (97.446%)\n",
      "Epoch: [69][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1442 (0.0769)\tPrec 95.312% (97.399%)\n",
      "Test: [0/79]\tTime 0.143 (0.143)\tLoss 0.2818 (0.2818)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.320% \n",
      "Epoch: [70][0/391]\tTime 0.270 (0.270)\tData 0.230 (0.230)\tLoss 0.0360 (0.0360)\tPrec 99.219% (99.219%)\n",
      "Epoch: [70][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0663 (0.0750)\tPrec 97.656% (97.254%)\n",
      "Epoch: [70][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0853 (0.0743)\tPrec 97.656% (97.349%)\n",
      "Epoch: [70][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0448 (0.0782)\tPrec 98.438% (97.225%)\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.4353 (0.4353)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.390% \n",
      "Epoch: [71][0/391]\tTime 0.262 (0.262)\tData 0.223 (0.223)\tLoss 0.0489 (0.0489)\tPrec 98.438% (98.438%)\n",
      "Epoch: [71][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0707 (0.0795)\tPrec 97.656% (97.200%)\n",
      "Epoch: [71][200/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0661 (0.0749)\tPrec 98.438% (97.349%)\n",
      "Epoch: [71][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0323 (0.0753)\tPrec 100.000% (97.392%)\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3398 (0.3398)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.090% \n",
      "Epoch: [72][0/391]\tTime 0.251 (0.251)\tData 0.214 (0.214)\tLoss 0.1190 (0.1190)\tPrec 95.312% (95.312%)\n",
      "Epoch: [72][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0623 (0.0659)\tPrec 97.656% (97.749%)\n",
      "Epoch: [72][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0294 (0.0724)\tPrec 99.219% (97.547%)\n",
      "Epoch: [72][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1006 (0.0753)\tPrec 96.875% (97.402%)\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.2789 (0.2789)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.210% \n",
      "Epoch: [73][0/391]\tTime 0.253 (0.253)\tData 0.221 (0.221)\tLoss 0.1198 (0.1198)\tPrec 97.656% (97.656%)\n",
      "Epoch: [73][100/391]\tTime 0.048 (0.052)\tData 0.001 (0.003)\tLoss 0.0757 (0.0729)\tPrec 96.875% (97.494%)\n",
      "Epoch: [73][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0412 (0.0742)\tPrec 98.438% (97.458%)\n",
      "Epoch: [73][300/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0838 (0.0741)\tPrec 96.094% (97.384%)\n",
      "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.3369 (0.3369)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.770% \n",
      "Epoch: [74][0/391]\tTime 0.252 (0.252)\tData 0.213 (0.213)\tLoss 0.0419 (0.0419)\tPrec 98.438% (98.438%)\n",
      "Epoch: [74][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1036 (0.0676)\tPrec 95.312% (97.649%)\n",
      "Epoch: [74][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0744 (0.0710)\tPrec 97.656% (97.442%)\n",
      "Epoch: [74][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0375 (0.0728)\tPrec 99.219% (97.373%)\n",
      "Test: [0/79]\tTime 0.160 (0.160)\tLoss 0.3072 (0.3072)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.890% \n",
      "Epoch: [75][0/391]\tTime 0.263 (0.263)\tData 0.226 (0.226)\tLoss 0.0560 (0.0560)\tPrec 96.875% (96.875%)\n",
      "Epoch: [75][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.1683 (0.0703)\tPrec 95.312% (97.540%)\n",
      "Epoch: [75][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0212 (0.0706)\tPrec 100.000% (97.528%)\n",
      "Epoch: [75][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0716 (0.0712)\tPrec 97.656% (97.475%)\n",
      "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.3909 (0.3909)\tPrec 89.062% (89.062%)\n",
      " * Prec 88.740% \n",
      "Epoch: [76][0/391]\tTime 0.299 (0.299)\tData 0.261 (0.261)\tLoss 0.1436 (0.1436)\tPrec 95.312% (95.312%)\n",
      "Epoch: [76][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0434 (0.0723)\tPrec 99.219% (97.478%)\n",
      "Epoch: [76][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1686 (0.0712)\tPrec 93.750% (97.520%)\n",
      "Epoch: [76][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0322 (0.0726)\tPrec 98.438% (97.475%)\n",
      "Test: [0/79]\tTime 0.160 (0.160)\tLoss 0.2753 (0.2753)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.350% \n",
      "Epoch: [77][0/391]\tTime 0.318 (0.318)\tData 0.279 (0.279)\tLoss 0.0792 (0.0792)\tPrec 97.656% (97.656%)\n",
      "Epoch: [77][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0195 (0.0680)\tPrec 100.000% (97.618%)\n",
      "Epoch: [77][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.1129 (0.0702)\tPrec 96.875% (97.567%)\n",
      "Epoch: [77][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0784 (0.0694)\tPrec 98.438% (97.591%)\n",
      "Test: [0/79]\tTime 0.182 (0.182)\tLoss 0.2923 (0.2923)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.350% \n",
      "Epoch: [78][0/391]\tTime 0.248 (0.248)\tData 0.214 (0.214)\tLoss 0.0826 (0.0826)\tPrec 96.875% (96.875%)\n",
      "Epoch: [78][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0428 (0.0695)\tPrec 97.656% (97.679%)\n",
      "Epoch: [78][200/391]\tTime 0.046 (0.050)\tData 0.001 (0.002)\tLoss 0.0828 (0.0690)\tPrec 97.656% (97.703%)\n",
      "Epoch: [78][300/391]\tTime 0.046 (0.050)\tData 0.001 (0.002)\tLoss 0.1159 (0.0695)\tPrec 96.094% (97.669%)\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.2996 (0.2996)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.290% \n",
      "Epoch: [79][0/391]\tTime 0.263 (0.263)\tData 0.231 (0.231)\tLoss 0.0577 (0.0577)\tPrec 96.875% (96.875%)\n",
      "Epoch: [79][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.1008 (0.0601)\tPrec 95.312% (97.881%)\n",
      "Epoch: [79][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0334 (0.0650)\tPrec 99.219% (97.734%)\n",
      "Epoch: [79][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0457 (0.0658)\tPrec 98.438% (97.656%)\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.2573 (0.2573)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.310% \n",
      "Epoch: [80][0/391]\tTime 0.257 (0.257)\tData 0.222 (0.222)\tLoss 0.0525 (0.0525)\tPrec 98.438% (98.438%)\n",
      "Epoch: [80][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0827 (0.0682)\tPrec 97.656% (97.602%)\n",
      "Epoch: [80][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0481 (0.0673)\tPrec 97.656% (97.656%)\n",
      "Epoch: [80][300/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0834 (0.0705)\tPrec 96.875% (97.571%)\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.2492 (0.2492)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.660% \n",
      "Epoch: [81][0/391]\tTime 0.261 (0.261)\tData 0.225 (0.225)\tLoss 0.0694 (0.0694)\tPrec 97.656% (97.656%)\n",
      "Epoch: [81][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0989 (0.0655)\tPrec 96.875% (97.780%)\n",
      "Epoch: [81][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0248 (0.0681)\tPrec 100.000% (97.668%)\n",
      "Epoch: [81][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0662 (0.0691)\tPrec 96.875% (97.578%)\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.4511 (0.4511)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.780% \n",
      "Epoch: [82][0/391]\tTime 0.263 (0.263)\tData 0.227 (0.227)\tLoss 0.0888 (0.0888)\tPrec 97.656% (97.656%)\n",
      "Epoch: [82][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0281 (0.0618)\tPrec 99.219% (97.826%)\n",
      "Epoch: [82][200/391]\tTime 0.051 (0.051)\tData 0.001 (0.002)\tLoss 0.0931 (0.0677)\tPrec 97.656% (97.629%)\n",
      "Epoch: [82][300/391]\tTime 0.044 (0.050)\tData 0.001 (0.002)\tLoss 0.0724 (0.0695)\tPrec 97.656% (97.529%)\n",
      "Test: [0/79]\tTime 0.141 (0.141)\tLoss 0.3299 (0.3299)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.210% \n",
      "Epoch: [83][0/391]\tTime 0.250 (0.250)\tData 0.217 (0.217)\tLoss 0.0602 (0.0602)\tPrec 99.219% (99.219%)\n",
      "Epoch: [83][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1208 (0.0659)\tPrec 97.656% (97.757%)\n",
      "Epoch: [83][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0996 (0.0661)\tPrec 96.094% (97.695%)\n",
      "Epoch: [83][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0861 (0.0687)\tPrec 97.656% (97.545%)\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.2463 (0.2463)\tPrec 93.750% (93.750%)\n",
      " * Prec 88.960% \n",
      "Epoch: [84][0/391]\tTime 0.258 (0.258)\tData 0.224 (0.224)\tLoss 0.0569 (0.0569)\tPrec 97.656% (97.656%)\n",
      "Epoch: [84][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0783 (0.0637)\tPrec 96.875% (97.765%)\n",
      "Epoch: [84][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0282 (0.0634)\tPrec 99.219% (97.761%)\n",
      "Epoch: [84][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0972 (0.0656)\tPrec 96.875% (97.721%)\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.3596 (0.3596)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.200% \n",
      "Epoch: [85][0/391]\tTime 0.258 (0.258)\tData 0.225 (0.225)\tLoss 0.0563 (0.0563)\tPrec 97.656% (97.656%)\n",
      "Epoch: [85][100/391]\tTime 0.048 (0.052)\tData 0.001 (0.003)\tLoss 0.0662 (0.0607)\tPrec 99.219% (97.795%)\n",
      "Epoch: [85][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0522 (0.0619)\tPrec 98.438% (97.839%)\n",
      "Epoch: [85][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0309 (0.0651)\tPrec 98.438% (97.757%)\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.3419 (0.3419)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.310% \n",
      "Epoch: [86][0/391]\tTime 0.398 (0.398)\tData 0.364 (0.364)\tLoss 0.0765 (0.0765)\tPrec 96.875% (96.875%)\n",
      "Epoch: [86][100/391]\tTime 0.050 (0.053)\tData 0.001 (0.005)\tLoss 0.0963 (0.0571)\tPrec 96.094% (97.981%)\n",
      "Epoch: [86][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0705 (0.0597)\tPrec 96.094% (97.905%)\n",
      "Epoch: [86][300/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1314 (0.0613)\tPrec 96.875% (97.841%)\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.3843 (0.3843)\tPrec 86.719% (86.719%)\n",
      " * Prec 89.150% \n",
      "Epoch: [87][0/391]\tTime 0.194 (0.194)\tData 0.158 (0.158)\tLoss 0.0833 (0.0833)\tPrec 98.438% (98.438%)\n",
      "Epoch: [87][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0682 (0.0631)\tPrec 97.656% (97.788%)\n",
      "Epoch: [87][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0408 (0.0610)\tPrec 98.438% (97.878%)\n",
      "Epoch: [87][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0680 (0.0611)\tPrec 96.875% (97.854%)\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 0.4221 (0.4221)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.900% \n",
      "Epoch: [88][0/391]\tTime 0.301 (0.301)\tData 0.261 (0.261)\tLoss 0.0597 (0.0597)\tPrec 98.438% (98.438%)\n",
      "Epoch: [88][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0228 (0.0640)\tPrec 100.000% (97.896%)\n",
      "Epoch: [88][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1066 (0.0631)\tPrec 96.094% (97.905%)\n",
      "Epoch: [88][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0204 (0.0657)\tPrec 99.219% (97.786%)\n",
      "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.3247 (0.3247)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.010% \n",
      "Epoch: [89][0/391]\tTime 0.289 (0.289)\tData 0.256 (0.256)\tLoss 0.0343 (0.0343)\tPrec 99.219% (99.219%)\n",
      "Epoch: [89][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0569 (0.0647)\tPrec 97.656% (97.741%)\n",
      "Epoch: [89][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.1072 (0.0657)\tPrec 95.312% (97.722%)\n",
      "Epoch: [89][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0246 (0.0650)\tPrec 99.219% (97.739%)\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.4703 (0.4703)\tPrec 88.281% (88.281%)\n",
      " * Prec 89.040% \n",
      "Epoch: [90][0/391]\tTime 0.285 (0.285)\tData 0.253 (0.253)\tLoss 0.0220 (0.0220)\tPrec 100.000% (100.000%)\n",
      "Epoch: [90][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1073 (0.0696)\tPrec 97.656% (97.610%)\n",
      "Epoch: [90][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0941 (0.0661)\tPrec 96.875% (97.683%)\n",
      "Epoch: [90][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0721 (0.0649)\tPrec 96.875% (97.742%)\n",
      "Test: [0/79]\tTime 0.256 (0.256)\tLoss 0.4429 (0.4429)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.520% \n",
      "Epoch: [91][0/391]\tTime 0.268 (0.268)\tData 0.230 (0.230)\tLoss 0.0515 (0.0515)\tPrec 97.656% (97.656%)\n",
      "Epoch: [91][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0858 (0.0659)\tPrec 96.094% (97.649%)\n",
      "Epoch: [91][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0394 (0.0624)\tPrec 98.438% (97.804%)\n",
      "Epoch: [91][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0594 (0.0601)\tPrec 98.438% (97.885%)\n",
      "Test: [0/79]\tTime 0.166 (0.166)\tLoss 0.3721 (0.3721)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.350% \n",
      "Epoch: [92][0/391]\tTime 0.310 (0.310)\tData 0.238 (0.238)\tLoss 0.1115 (0.1115)\tPrec 96.875% (96.875%)\n",
      "Epoch: [92][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1102 (0.0571)\tPrec 95.312% (98.043%)\n",
      "Epoch: [92][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.1071 (0.0595)\tPrec 97.656% (97.952%)\n",
      "Epoch: [92][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0429 (0.0607)\tPrec 98.438% (97.861%)\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.3007 (0.3007)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.020% \n",
      "Epoch: [93][0/391]\tTime 0.360 (0.360)\tData 0.325 (0.325)\tLoss 0.0290 (0.0290)\tPrec 99.219% (99.219%)\n",
      "Epoch: [93][100/391]\tTime 0.050 (0.053)\tData 0.001 (0.004)\tLoss 0.1433 (0.0621)\tPrec 95.312% (97.896%)\n",
      "Epoch: [93][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0316 (0.0624)\tPrec 98.438% (97.882%)\n",
      "Epoch: [93][300/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0382 (0.0629)\tPrec 98.438% (97.856%)\n",
      "Test: [0/79]\tTime 0.185 (0.185)\tLoss 0.3650 (0.3650)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.720% \n",
      "Epoch: [94][0/391]\tTime 0.252 (0.252)\tData 0.217 (0.217)\tLoss 0.0389 (0.0389)\tPrec 99.219% (99.219%)\n",
      "Epoch: [94][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.1102 (0.0667)\tPrec 96.094% (97.625%)\n",
      "Epoch: [94][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0438 (0.0654)\tPrec 99.219% (97.683%)\n",
      "Epoch: [94][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0884 (0.0619)\tPrec 97.656% (97.838%)\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.3537 (0.3537)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.920% \n",
      "Epoch: [95][0/391]\tTime 0.264 (0.264)\tData 0.234 (0.234)\tLoss 0.0267 (0.0267)\tPrec 99.219% (99.219%)\n",
      "Epoch: [95][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.004)\tLoss 0.0593 (0.0551)\tPrec 96.875% (98.144%)\n",
      "Epoch: [95][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0184 (0.0569)\tPrec 100.000% (98.138%)\n",
      "Epoch: [95][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.1011 (0.0575)\tPrec 96.875% (98.136%)\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.3419 (0.3419)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.120% \n",
      "Epoch: [96][0/391]\tTime 0.259 (0.259)\tData 0.223 (0.223)\tLoss 0.0844 (0.0844)\tPrec 96.875% (96.875%)\n",
      "Epoch: [96][100/391]\tTime 0.049 (0.052)\tData 0.002 (0.003)\tLoss 0.0349 (0.0568)\tPrec 97.656% (98.020%)\n",
      "Epoch: [96][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1043 (0.0593)\tPrec 96.875% (97.921%)\n",
      "Epoch: [96][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0391 (0.0590)\tPrec 98.438% (97.947%)\n",
      "Test: [0/79]\tTime 0.160 (0.160)\tLoss 0.3625 (0.3625)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.900% \n",
      "Epoch: [97][0/391]\tTime 0.323 (0.323)\tData 0.290 (0.290)\tLoss 0.0385 (0.0385)\tPrec 99.219% (99.219%)\n",
      "Epoch: [97][100/391]\tTime 0.050 (0.052)\tData 0.002 (0.004)\tLoss 0.0820 (0.0568)\tPrec 96.875% (98.028%)\n",
      "Epoch: [97][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0793 (0.0622)\tPrec 98.438% (97.963%)\n",
      "Epoch: [97][300/391]\tTime 0.048 (0.050)\tData 0.002 (0.002)\tLoss 0.0351 (0.0605)\tPrec 99.219% (98.020%)\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.4100 (0.4100)\tPrec 88.281% (88.281%)\n",
      " * Prec 89.070% \n",
      "Epoch: [98][0/391]\tTime 0.317 (0.317)\tData 0.243 (0.243)\tLoss 0.0332 (0.0332)\tPrec 97.656% (97.656%)\n",
      "Epoch: [98][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0967 (0.0534)\tPrec 97.656% (98.352%)\n",
      "Epoch: [98][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0286 (0.0530)\tPrec 99.219% (98.270%)\n",
      "Epoch: [98][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1058 (0.0544)\tPrec 97.656% (98.191%)\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.3976 (0.3976)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.040% \n",
      "Epoch: [99][0/391]\tTime 0.257 (0.257)\tData 0.223 (0.223)\tLoss 0.1359 (0.1359)\tPrec 96.875% (96.875%)\n",
      "Epoch: [99][100/391]\tTime 0.049 (0.051)\tData 0.002 (0.004)\tLoss 0.0711 (0.0560)\tPrec 96.875% (98.012%)\n",
      "Epoch: [99][200/391]\tTime 0.051 (0.050)\tData 0.001 (0.002)\tLoss 0.0167 (0.0583)\tPrec 99.219% (97.959%)\n",
      "Epoch: [99][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1770 (0.0565)\tPrec 95.312% (98.066%)\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.3806 (0.3806)\tPrec 89.062% (89.062%)\n",
      " * Prec 88.760% \n",
      "Epoch: [100][0/391]\tTime 0.376 (0.376)\tData 0.336 (0.336)\tLoss 0.0857 (0.0857)\tPrec 96.094% (96.094%)\n",
      "Epoch: [100][100/391]\tTime 0.050 (0.053)\tData 0.001 (0.005)\tLoss 0.0284 (0.0578)\tPrec 99.219% (98.089%)\n",
      "Epoch: [100][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0454 (0.0610)\tPrec 98.438% (98.002%)\n",
      "Epoch: [100][300/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0946 (0.0620)\tPrec 97.656% (97.924%)\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.3691 (0.3691)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.190% \n",
      "Epoch: [101][0/391]\tTime 0.260 (0.260)\tData 0.228 (0.228)\tLoss 0.0216 (0.0216)\tPrec 99.219% (99.219%)\n",
      "Epoch: [101][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0478 (0.0505)\tPrec 98.438% (98.314%)\n",
      "Epoch: [101][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0591 (0.0541)\tPrec 98.438% (98.255%)\n",
      "Epoch: [101][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0635 (0.0558)\tPrec 96.875% (98.147%)\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.3600 (0.3600)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.400% \n",
      "Epoch: [102][0/391]\tTime 0.247 (0.247)\tData 0.214 (0.214)\tLoss 0.0273 (0.0273)\tPrec 98.438% (98.438%)\n",
      "Epoch: [102][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0164 (0.0530)\tPrec 99.219% (98.229%)\n",
      "Epoch: [102][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0702 (0.0552)\tPrec 96.875% (98.142%)\n",
      "Epoch: [102][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.0265 (0.0580)\tPrec 99.219% (98.027%)\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.3552 (0.3552)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.380% \n",
      "Epoch: [103][0/391]\tTime 0.353 (0.353)\tData 0.322 (0.322)\tLoss 0.0710 (0.0710)\tPrec 96.094% (96.094%)\n",
      "Epoch: [103][100/391]\tTime 0.050 (0.053)\tData 0.001 (0.004)\tLoss 0.1116 (0.0542)\tPrec 96.094% (97.935%)\n",
      "Epoch: [103][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0357 (0.0560)\tPrec 99.219% (97.991%)\n",
      "Epoch: [103][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0505 (0.0581)\tPrec 98.438% (97.975%)\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.3127 (0.3127)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.370% \n",
      "Epoch: [104][0/391]\tTime 0.251 (0.251)\tData 0.212 (0.212)\tLoss 0.0990 (0.0990)\tPrec 96.875% (96.875%)\n",
      "Epoch: [104][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0854 (0.0590)\tPrec 96.875% (97.912%)\n",
      "Epoch: [104][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0213 (0.0584)\tPrec 99.219% (97.987%)\n",
      "Epoch: [104][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0593 (0.0579)\tPrec 99.219% (98.004%)\n",
      "Test: [0/79]\tTime 0.172 (0.172)\tLoss 0.5651 (0.5651)\tPrec 86.719% (86.719%)\n",
      " * Prec 89.330% \n",
      "Epoch: [105][0/391]\tTime 0.256 (0.256)\tData 0.221 (0.221)\tLoss 0.0594 (0.0594)\tPrec 97.656% (97.656%)\n",
      "Epoch: [105][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.2202 (0.0597)\tPrec 94.531% (97.942%)\n",
      "Epoch: [105][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0366 (0.0601)\tPrec 100.000% (97.940%)\n",
      "Epoch: [105][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0964 (0.0586)\tPrec 97.656% (97.994%)\n",
      "Test: [0/79]\tTime 0.165 (0.165)\tLoss 0.3138 (0.3138)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.440% \n",
      "Epoch: [106][0/391]\tTime 0.335 (0.335)\tData 0.295 (0.295)\tLoss 0.0399 (0.0399)\tPrec 98.438% (98.438%)\n",
      "Epoch: [106][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0138 (0.0525)\tPrec 100.000% (98.151%)\n",
      "Epoch: [106][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0463 (0.0551)\tPrec 98.438% (98.119%)\n",
      "Epoch: [106][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0572 (0.0565)\tPrec 98.438% (98.056%)\n",
      "Test: [0/79]\tTime 0.209 (0.209)\tLoss 0.4078 (0.4078)\tPrec 88.281% (88.281%)\n",
      " * Prec 89.180% \n",
      "Epoch: [107][0/391]\tTime 0.321 (0.321)\tData 0.244 (0.244)\tLoss 0.0435 (0.0435)\tPrec 99.219% (99.219%)\n",
      "Epoch: [107][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0594 (0.0501)\tPrec 98.438% (98.213%)\n",
      "Epoch: [107][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0590 (0.0528)\tPrec 98.438% (98.200%)\n",
      "Epoch: [107][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0695 (0.0552)\tPrec 96.875% (98.103%)\n",
      "Test: [0/79]\tTime 0.271 (0.271)\tLoss 0.2451 (0.2451)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.550% \n",
      "Epoch: [108][0/391]\tTime 0.315 (0.315)\tData 0.281 (0.281)\tLoss 0.0149 (0.0149)\tPrec 99.219% (99.219%)\n",
      "Epoch: [108][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0400 (0.0558)\tPrec 98.438% (98.074%)\n",
      "Epoch: [108][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0543 (0.0531)\tPrec 96.875% (98.162%)\n",
      "Epoch: [108][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1186 (0.0534)\tPrec 96.875% (98.147%)\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.4149 (0.4149)\tPrec 88.281% (88.281%)\n",
      " * Prec 89.200% \n",
      "Epoch: [109][0/391]\tTime 0.254 (0.254)\tData 0.220 (0.220)\tLoss 0.0390 (0.0390)\tPrec 97.656% (97.656%)\n",
      "Epoch: [109][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0504 (0.0463)\tPrec 97.656% (98.376%)\n",
      "Epoch: [109][200/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.0427 (0.0502)\tPrec 99.219% (98.231%)\n",
      "Epoch: [109][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1025 (0.0513)\tPrec 96.875% (98.225%)\n",
      "Test: [0/79]\tTime 0.187 (0.187)\tLoss 0.3608 (0.3608)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.550% \n",
      "Epoch: [110][0/391]\tTime 0.290 (0.290)\tData 0.258 (0.258)\tLoss 0.0697 (0.0697)\tPrec 96.094% (96.094%)\n",
      "Epoch: [110][100/391]\tTime 0.042 (0.052)\tData 0.001 (0.004)\tLoss 0.0415 (0.0535)\tPrec 97.656% (98.144%)\n",
      "Epoch: [110][200/391]\tTime 0.043 (0.051)\tData 0.001 (0.002)\tLoss 0.0825 (0.0557)\tPrec 98.438% (98.033%)\n",
      "Epoch: [110][300/391]\tTime 0.042 (0.050)\tData 0.001 (0.002)\tLoss 0.0224 (0.0548)\tPrec 100.000% (98.108%)\n",
      "Test: [0/79]\tTime 0.210 (0.210)\tLoss 0.2868 (0.2868)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.540% \n",
      "Epoch: [111][0/391]\tTime 0.335 (0.335)\tData 0.261 (0.261)\tLoss 0.0907 (0.0907)\tPrec 96.875% (96.875%)\n",
      "Epoch: [111][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0492 (0.0510)\tPrec 99.219% (98.260%)\n",
      "Epoch: [111][200/391]\tTime 0.045 (0.051)\tData 0.001 (0.003)\tLoss 0.0769 (0.0516)\tPrec 96.094% (98.216%)\n",
      "Epoch: [111][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.0145 (0.0534)\tPrec 100.000% (98.209%)\n",
      "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.3811 (0.3811)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.210% \n",
      "Epoch: [112][0/391]\tTime 0.244 (0.244)\tData 0.208 (0.208)\tLoss 0.0505 (0.0505)\tPrec 97.656% (97.656%)\n",
      "Epoch: [112][100/391]\tTime 0.047 (0.051)\tData 0.001 (0.003)\tLoss 0.0941 (0.0562)\tPrec 96.094% (98.074%)\n",
      "Epoch: [112][200/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.0192 (0.0545)\tPrec 100.000% (98.177%)\n",
      "Epoch: [112][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.0394 (0.0542)\tPrec 98.438% (98.149%)\n",
      "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.3400 (0.3400)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.830% \n",
      "Epoch: [113][0/391]\tTime 0.261 (0.261)\tData 0.223 (0.223)\tLoss 0.0107 (0.0107)\tPrec 100.000% (100.000%)\n",
      "Epoch: [113][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0181 (0.0522)\tPrec 100.000% (98.236%)\n",
      "Epoch: [113][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0418 (0.0504)\tPrec 98.438% (98.286%)\n",
      "Epoch: [113][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1200 (0.0515)\tPrec 97.656% (98.243%)\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.5252 (0.5252)\tPrec 88.281% (88.281%)\n",
      " * Prec 89.290% \n",
      "Epoch: [114][0/391]\tTime 0.382 (0.382)\tData 0.350 (0.350)\tLoss 0.0409 (0.0409)\tPrec 98.438% (98.438%)\n",
      "Epoch: [114][100/391]\tTime 0.049 (0.053)\tData 0.001 (0.005)\tLoss 0.0637 (0.0565)\tPrec 96.094% (98.074%)\n",
      "Epoch: [114][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0648 (0.0543)\tPrec 96.094% (98.064%)\n",
      "Epoch: [114][300/391]\tTime 0.050 (0.051)\tData 0.002 (0.002)\tLoss 0.0384 (0.0540)\tPrec 99.219% (98.069%)\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.3223 (0.3223)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.320% \n",
      "Epoch: [115][0/391]\tTime 0.290 (0.290)\tData 0.251 (0.251)\tLoss 0.0163 (0.0163)\tPrec 99.219% (99.219%)\n",
      "Epoch: [115][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0667 (0.0542)\tPrec 98.438% (98.089%)\n",
      "Epoch: [115][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0345 (0.0531)\tPrec 99.219% (98.162%)\n",
      "Epoch: [115][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1253 (0.0540)\tPrec 96.094% (98.116%)\n",
      "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.3839 (0.3839)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.500% \n",
      "Epoch: [116][0/391]\tTime 0.320 (0.320)\tData 0.280 (0.280)\tLoss 0.0146 (0.0146)\tPrec 100.000% (100.000%)\n",
      "Epoch: [116][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0211 (0.0564)\tPrec 100.000% (98.260%)\n",
      "Epoch: [116][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0441 (0.0529)\tPrec 98.438% (98.305%)\n",
      "Epoch: [116][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0293 (0.0500)\tPrec 97.656% (98.375%)\n",
      "Test: [0/79]\tTime 0.152 (0.152)\tLoss 0.3298 (0.3298)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.420% \n",
      "Epoch: [117][0/391]\tTime 0.249 (0.249)\tData 0.210 (0.210)\tLoss 0.0252 (0.0252)\tPrec 100.000% (100.000%)\n",
      "Epoch: [117][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0183 (0.0524)\tPrec 100.000% (98.136%)\n",
      "Epoch: [117][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0444 (0.0539)\tPrec 99.219% (98.022%)\n",
      "Epoch: [117][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.0206 (0.0535)\tPrec 99.219% (98.090%)\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.2736 (0.2736)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.540% \n",
      "Epoch: [118][0/391]\tTime 0.290 (0.290)\tData 0.257 (0.257)\tLoss 0.0310 (0.0310)\tPrec 99.219% (99.219%)\n",
      "Epoch: [118][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0426 (0.0526)\tPrec 98.438% (98.314%)\n",
      "Epoch: [118][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0303 (0.0522)\tPrec 99.219% (98.294%)\n",
      "Epoch: [118][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1000 (0.0541)\tPrec 97.656% (98.238%)\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.3236 (0.3236)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.190% \n",
      "Epoch: [119][0/391]\tTime 0.338 (0.338)\tData 0.298 (0.298)\tLoss 0.0644 (0.0644)\tPrec 97.656% (97.656%)\n",
      "Epoch: [119][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0242 (0.0461)\tPrec 99.219% (98.507%)\n",
      "Epoch: [119][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0915 (0.0486)\tPrec 96.094% (98.383%)\n",
      "Epoch: [119][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0642 (0.0510)\tPrec 96.094% (98.269%)\n",
      "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.3918 (0.3918)\tPrec 86.719% (86.719%)\n",
      " * Prec 89.220% \n",
      "Epoch: [120][0/391]\tTime 0.243 (0.243)\tData 0.203 (0.203)\tLoss 0.0726 (0.0726)\tPrec 97.656% (97.656%)\n",
      "Epoch: [120][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0342 (0.0550)\tPrec 98.438% (98.004%)\n",
      "Epoch: [120][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0149 (0.0548)\tPrec 100.000% (98.053%)\n",
      "Epoch: [120][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0725 (0.0539)\tPrec 99.219% (98.090%)\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 0.3487 (0.3487)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.560% \n",
      "Epoch: [121][0/391]\tTime 0.255 (0.255)\tData 0.222 (0.222)\tLoss 0.1020 (0.1020)\tPrec 96.875% (96.875%)\n",
      "Epoch: [121][100/391]\tTime 0.057 (0.052)\tData 0.001 (0.003)\tLoss 0.0466 (0.0499)\tPrec 98.438% (98.321%)\n",
      "Epoch: [121][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0784 (0.0493)\tPrec 96.875% (98.360%)\n",
      "Epoch: [121][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0847 (0.0531)\tPrec 96.094% (98.243%)\n",
      "Test: [0/79]\tTime 0.170 (0.170)\tLoss 0.3735 (0.3735)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.170% \n",
      "Epoch: [122][0/391]\tTime 0.340 (0.340)\tData 0.303 (0.303)\tLoss 0.0370 (0.0370)\tPrec 99.219% (99.219%)\n",
      "Epoch: [122][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0251 (0.0447)\tPrec 99.219% (98.507%)\n",
      "Epoch: [122][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0157 (0.0493)\tPrec 100.000% (98.340%)\n",
      "Epoch: [122][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0921 (0.0491)\tPrec 96.094% (98.339%)\n",
      "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.3189 (0.3189)\tPrec 86.719% (86.719%)\n",
      " * Prec 88.750% \n",
      "Epoch: [123][0/391]\tTime 0.225 (0.225)\tData 0.186 (0.186)\tLoss 0.0705 (0.0705)\tPrec 99.219% (99.219%)\n",
      "Epoch: [123][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0900 (0.0475)\tPrec 99.219% (98.399%)\n",
      "Epoch: [123][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0483 (0.0486)\tPrec 97.656% (98.340%)\n",
      "Epoch: [123][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0142 (0.0476)\tPrec 100.000% (98.365%)\n",
      "Test: [0/79]\tTime 0.183 (0.183)\tLoss 0.2998 (0.2998)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.530% \n",
      "Epoch: [124][0/391]\tTime 0.255 (0.255)\tData 0.214 (0.214)\tLoss 0.0835 (0.0835)\tPrec 98.438% (98.438%)\n",
      "Epoch: [124][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1357 (0.0552)\tPrec 94.531% (98.144%)\n",
      "Epoch: [124][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0352 (0.0548)\tPrec 98.438% (98.142%)\n",
      "Epoch: [124][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0256 (0.0548)\tPrec 99.219% (98.149%)\n",
      "Test: [0/79]\tTime 0.169 (0.169)\tLoss 0.3536 (0.3536)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.990% \n",
      "Epoch: [125][0/391]\tTime 0.259 (0.259)\tData 0.223 (0.223)\tLoss 0.0132 (0.0132)\tPrec 100.000% (100.000%)\n",
      "Epoch: [125][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.1013 (0.0463)\tPrec 95.312% (98.515%)\n",
      "Epoch: [125][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0363 (0.0470)\tPrec 98.438% (98.441%)\n",
      "Epoch: [125][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0515 (0.0471)\tPrec 98.438% (98.380%)\n",
      "Test: [0/79]\tTime 0.112 (0.112)\tLoss 0.2276 (0.2276)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.320% \n",
      "Epoch: [126][0/391]\tTime 0.300 (0.300)\tData 0.269 (0.269)\tLoss 0.0794 (0.0794)\tPrec 98.438% (98.438%)\n",
      "Epoch: [126][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0658 (0.0459)\tPrec 96.875% (98.499%)\n",
      "Epoch: [126][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0640 (0.0473)\tPrec 98.438% (98.406%)\n",
      "Epoch: [126][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0803 (0.0482)\tPrec 98.438% (98.360%)\n",
      "Test: [0/79]\tTime 0.274 (0.274)\tLoss 0.3043 (0.3043)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.900% \n",
      "Epoch: [127][0/391]\tTime 0.273 (0.273)\tData 0.232 (0.232)\tLoss 0.0734 (0.0734)\tPrec 97.656% (97.656%)\n",
      "Epoch: [127][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.1132 (0.0528)\tPrec 95.312% (98.190%)\n",
      "Epoch: [127][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0454 (0.0526)\tPrec 97.656% (98.204%)\n",
      "Epoch: [127][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0787 (0.0490)\tPrec 97.656% (98.331%)\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 0.3678 (0.3678)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.340% \n",
      "Epoch: [128][0/391]\tTime 0.364 (0.364)\tData 0.331 (0.331)\tLoss 0.0397 (0.0397)\tPrec 98.438% (98.438%)\n",
      "Epoch: [128][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.005)\tLoss 0.0694 (0.0524)\tPrec 97.656% (98.182%)\n",
      "Epoch: [128][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0392 (0.0501)\tPrec 98.438% (98.266%)\n",
      "Epoch: [128][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0251 (0.0494)\tPrec 99.219% (98.292%)\n",
      "Test: [0/79]\tTime 0.176 (0.176)\tLoss 0.3019 (0.3019)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.910% \n",
      "Epoch: [129][0/391]\tTime 0.255 (0.255)\tData 0.218 (0.218)\tLoss 0.0442 (0.0442)\tPrec 98.438% (98.438%)\n",
      "Epoch: [129][100/391]\tTime 0.051 (0.051)\tData 0.001 (0.003)\tLoss 0.0300 (0.0449)\tPrec 99.219% (98.468%)\n",
      "Epoch: [129][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0435 (0.0446)\tPrec 98.438% (98.453%)\n",
      "Epoch: [129][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0318 (0.0483)\tPrec 99.219% (98.316%)\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.3759 (0.3759)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.290% \n",
      "Epoch: [130][0/391]\tTime 0.196 (0.196)\tData 0.128 (0.128)\tLoss 0.0895 (0.0895)\tPrec 97.656% (97.656%)\n",
      "Epoch: [130][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0444 (0.0470)\tPrec 98.438% (98.414%)\n",
      "Epoch: [130][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0389 (0.0486)\tPrec 97.656% (98.325%)\n",
      "Epoch: [130][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.0299 (0.0506)\tPrec 99.219% (98.245%)\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.2565 (0.2565)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.410% \n",
      "Epoch: [131][0/391]\tTime 0.288 (0.288)\tData 0.257 (0.257)\tLoss 0.0326 (0.0326)\tPrec 99.219% (99.219%)\n",
      "Epoch: [131][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.1122 (0.0485)\tPrec 95.312% (98.260%)\n",
      "Epoch: [131][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0224 (0.0486)\tPrec 100.000% (98.352%)\n",
      "Epoch: [131][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0672 (0.0485)\tPrec 96.094% (98.352%)\n",
      "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.3374 (0.3374)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.140% \n",
      "Epoch: [132][0/391]\tTime 0.257 (0.257)\tData 0.217 (0.217)\tLoss 0.0085 (0.0085)\tPrec 100.000% (100.000%)\n",
      "Epoch: [132][100/391]\tTime 0.051 (0.052)\tData 0.001 (0.003)\tLoss 0.0515 (0.0452)\tPrec 96.875% (98.515%)\n",
      "Epoch: [132][200/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0458 (0.0468)\tPrec 96.875% (98.383%)\n",
      "Epoch: [132][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0272 (0.0464)\tPrec 99.219% (98.391%)\n",
      "Test: [0/79]\tTime 0.255 (0.255)\tLoss 0.3988 (0.3988)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.520% \n",
      "Epoch: [133][0/391]\tTime 0.259 (0.259)\tData 0.219 (0.219)\tLoss 0.0160 (0.0160)\tPrec 100.000% (100.000%)\n",
      "Epoch: [133][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0135 (0.0453)\tPrec 100.000% (98.407%)\n",
      "Epoch: [133][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0349 (0.0460)\tPrec 98.438% (98.414%)\n",
      "Epoch: [133][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0391 (0.0475)\tPrec 99.219% (98.344%)\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.3709 (0.3709)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.620% \n",
      "Epoch: [134][0/391]\tTime 0.284 (0.284)\tData 0.252 (0.252)\tLoss 0.1094 (0.1094)\tPrec 96.875% (96.875%)\n",
      "Epoch: [134][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0476 (0.0500)\tPrec 97.656% (98.267%)\n",
      "Epoch: [134][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0446 (0.0503)\tPrec 97.656% (98.282%)\n",
      "Epoch: [134][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0755 (0.0499)\tPrec 96.875% (98.282%)\n",
      "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.3238 (0.3238)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.300% \n",
      "Epoch: [135][0/391]\tTime 0.250 (0.250)\tData 0.217 (0.217)\tLoss 0.0370 (0.0370)\tPrec 98.438% (98.438%)\n",
      "Epoch: [135][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0807 (0.0454)\tPrec 97.656% (98.399%)\n",
      "Epoch: [135][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1234 (0.0493)\tPrec 95.312% (98.290%)\n",
      "Epoch: [135][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.0561 (0.0486)\tPrec 98.438% (98.318%)\n",
      "Test: [0/79]\tTime 0.337 (0.337)\tLoss 0.3000 (0.3000)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.280% \n",
      "Epoch: [136][0/391]\tTime 0.288 (0.288)\tData 0.255 (0.255)\tLoss 0.0486 (0.0486)\tPrec 98.438% (98.438%)\n",
      "Epoch: [136][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0857 (0.0515)\tPrec 96.875% (98.167%)\n",
      "Epoch: [136][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0678 (0.0505)\tPrec 96.875% (98.193%)\n",
      "Epoch: [136][300/391]\tTime 0.051 (0.050)\tData 0.001 (0.002)\tLoss 0.0567 (0.0487)\tPrec 97.656% (98.282%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.3408 (0.3408)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.420% \n",
      "Epoch: [137][0/391]\tTime 0.246 (0.246)\tData 0.214 (0.214)\tLoss 0.0197 (0.0197)\tPrec 99.219% (99.219%)\n",
      "Epoch: [137][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0222 (0.0437)\tPrec 99.219% (98.422%)\n",
      "Epoch: [137][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0514 (0.0450)\tPrec 98.438% (98.395%)\n",
      "Epoch: [137][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0194 (0.0454)\tPrec 99.219% (98.401%)\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.3427 (0.3427)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.300% \n",
      "Epoch: [138][0/391]\tTime 0.386 (0.386)\tData 0.355 (0.355)\tLoss 0.0522 (0.0522)\tPrec 97.656% (97.656%)\n",
      "Epoch: [138][100/391]\tTime 0.049 (0.053)\tData 0.001 (0.005)\tLoss 0.0894 (0.0448)\tPrec 96.875% (98.414%)\n",
      "Epoch: [138][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0126 (0.0454)\tPrec 100.000% (98.403%)\n",
      "Epoch: [138][300/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0492 (0.0472)\tPrec 97.656% (98.347%)\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.2791 (0.2791)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.200% \n",
      "Epoch: [139][0/391]\tTime 0.255 (0.255)\tData 0.224 (0.224)\tLoss 0.0460 (0.0460)\tPrec 98.438% (98.438%)\n",
      "Epoch: [139][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0257 (0.0471)\tPrec 98.438% (98.430%)\n",
      "Epoch: [139][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1073 (0.0455)\tPrec 96.875% (98.430%)\n",
      "Epoch: [139][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0761 (0.0442)\tPrec 96.875% (98.487%)\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 0.4086 (0.4086)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.320% \n",
      "Epoch: [140][0/391]\tTime 0.298 (0.298)\tData 0.261 (0.261)\tLoss 0.0564 (0.0564)\tPrec 98.438% (98.438%)\n",
      "Epoch: [140][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0197 (0.0455)\tPrec 100.000% (98.438%)\n",
      "Epoch: [140][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0106 (0.0436)\tPrec 99.219% (98.515%)\n",
      "Epoch: [140][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0105 (0.0452)\tPrec 99.219% (98.425%)\n",
      "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.3374 (0.3374)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.690% \n",
      "Epoch: [141][0/391]\tTime 0.257 (0.257)\tData 0.223 (0.223)\tLoss 0.0086 (0.0086)\tPrec 100.000% (100.000%)\n",
      "Epoch: [141][100/391]\tTime 0.049 (0.051)\tData 0.002 (0.004)\tLoss 0.0370 (0.0437)\tPrec 98.438% (98.538%)\n",
      "Epoch: [141][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0490 (0.0444)\tPrec 98.438% (98.558%)\n",
      "Epoch: [141][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0118 (0.0448)\tPrec 99.219% (98.508%)\n",
      "Test: [0/79]\tTime 0.174 (0.174)\tLoss 0.3364 (0.3364)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.430% \n",
      "Epoch: [142][0/391]\tTime 0.251 (0.251)\tData 0.219 (0.219)\tLoss 0.0561 (0.0561)\tPrec 96.875% (96.875%)\n",
      "Epoch: [142][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0521 (0.0418)\tPrec 97.656% (98.422%)\n",
      "Epoch: [142][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0141 (0.0422)\tPrec 99.219% (98.465%)\n",
      "Epoch: [142][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0185 (0.0448)\tPrec 99.219% (98.378%)\n",
      "Test: [0/79]\tTime 0.115 (0.115)\tLoss 0.2626 (0.2626)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.150% \n",
      "Epoch: [143][0/391]\tTime 0.270 (0.270)\tData 0.229 (0.229)\tLoss 0.0326 (0.0326)\tPrec 98.438% (98.438%)\n",
      "Epoch: [143][100/391]\tTime 0.041 (0.052)\tData 0.001 (0.003)\tLoss 0.0166 (0.0489)\tPrec 100.000% (98.244%)\n",
      "Epoch: [143][200/391]\tTime 0.042 (0.050)\tData 0.001 (0.002)\tLoss 0.0029 (0.0474)\tPrec 100.000% (98.309%)\n",
      "Epoch: [143][300/391]\tTime 0.042 (0.050)\tData 0.001 (0.002)\tLoss 0.0352 (0.0466)\tPrec 99.219% (98.399%)\n",
      "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.3184 (0.3184)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.720% \n",
      "Epoch: [144][0/391]\tTime 0.242 (0.242)\tData 0.208 (0.208)\tLoss 0.0520 (0.0520)\tPrec 97.656% (97.656%)\n",
      "Epoch: [144][100/391]\tTime 0.049 (0.051)\tData 0.002 (0.003)\tLoss 0.0328 (0.0404)\tPrec 99.219% (98.530%)\n",
      "Epoch: [144][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0183 (0.0439)\tPrec 99.219% (98.465%)\n",
      "Epoch: [144][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0253 (0.0463)\tPrec 99.219% (98.409%)\n",
      "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.3572 (0.3572)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.190% \n",
      "Epoch: [145][0/391]\tTime 0.355 (0.355)\tData 0.319 (0.319)\tLoss 0.0478 (0.0478)\tPrec 97.656% (97.656%)\n",
      "Epoch: [145][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0471 (0.0445)\tPrec 99.219% (98.445%)\n",
      "Epoch: [145][200/391]\tTime 0.048 (0.051)\tData 0.001 (0.003)\tLoss 0.0794 (0.0441)\tPrec 98.438% (98.484%)\n",
      "Epoch: [145][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.1218 (0.0467)\tPrec 96.875% (98.419%)\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 0.3299 (0.3299)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.460% \n",
      "Epoch: [146][0/391]\tTime 0.249 (0.249)\tData 0.219 (0.219)\tLoss 0.0241 (0.0241)\tPrec 98.438% (98.438%)\n",
      "Epoch: [146][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0163 (0.0449)\tPrec 99.219% (98.453%)\n",
      "Epoch: [146][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0509 (0.0447)\tPrec 97.656% (98.391%)\n",
      "Epoch: [146][300/391]\tTime 0.051 (0.050)\tData 0.001 (0.002)\tLoss 0.0057 (0.0430)\tPrec 100.000% (98.471%)\n",
      "Test: [0/79]\tTime 0.113 (0.113)\tLoss 0.3950 (0.3950)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.610% \n",
      "Epoch: [147][0/391]\tTime 0.290 (0.290)\tData 0.255 (0.255)\tLoss 0.0139 (0.0139)\tPrec 99.219% (99.219%)\n",
      "Epoch: [147][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0461 (0.0460)\tPrec 98.438% (98.476%)\n",
      "Epoch: [147][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0361 (0.0466)\tPrec 98.438% (98.403%)\n",
      "Epoch: [147][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0230 (0.0458)\tPrec 99.219% (98.404%)\n",
      "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.3413 (0.3413)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.310% \n",
      "Epoch: [148][0/391]\tTime 0.252 (0.252)\tData 0.222 (0.222)\tLoss 0.1147 (0.1147)\tPrec 96.875% (96.875%)\n",
      "Epoch: [148][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0233 (0.0421)\tPrec 100.000% (98.577%)\n",
      "Epoch: [148][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0084 (0.0424)\tPrec 100.000% (98.550%)\n",
      "Epoch: [148][300/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0203 (0.0427)\tPrec 99.219% (98.531%)\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.3788 (0.3788)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.380% \n",
      "Epoch: [149][0/391]\tTime 0.301 (0.301)\tData 0.269 (0.269)\tLoss 0.0578 (0.0578)\tPrec 96.875% (96.875%)\n",
      "Epoch: [149][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0513 (0.0382)\tPrec 99.219% (98.693%)\n",
      "Epoch: [149][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0486 (0.0429)\tPrec 98.438% (98.504%)\n",
      "Epoch: [149][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0937 (0.0456)\tPrec 96.875% (98.406%)\n",
      "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.3011 (0.3011)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.360% \n",
      "Epoch: [150][0/391]\tTime 0.256 (0.256)\tData 0.223 (0.223)\tLoss 0.0440 (0.0440)\tPrec 98.438% (98.438%)\n",
      "Epoch: [150][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0836 (0.0468)\tPrec 96.094% (98.321%)\n",
      "Epoch: [150][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0090 (0.0463)\tPrec 100.000% (98.364%)\n",
      "Epoch: [150][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.0322 (0.0465)\tPrec 98.438% (98.341%)\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.3495 (0.3495)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.350% \n",
      "Epoch: [151][0/391]\tTime 0.276 (0.276)\tData 0.236 (0.236)\tLoss 0.1092 (0.1092)\tPrec 96.875% (96.875%)\n",
      "Epoch: [151][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0698 (0.0422)\tPrec 96.094% (98.546%)\n",
      "Epoch: [151][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0465 (0.0423)\tPrec 99.219% (98.519%)\n",
      "Epoch: [151][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.0511 (0.0430)\tPrec 96.875% (98.482%)\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.3881 (0.3881)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.730% \n",
      "Epoch: [152][0/391]\tTime 0.346 (0.346)\tData 0.309 (0.309)\tLoss 0.0439 (0.0439)\tPrec 98.438% (98.438%)\n",
      "Epoch: [152][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0881 (0.0449)\tPrec 97.656% (98.360%)\n",
      "Epoch: [152][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.1144 (0.0444)\tPrec 96.094% (98.438%)\n",
      "Epoch: [152][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0314 (0.0439)\tPrec 99.219% (98.489%)\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.4334 (0.4334)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.290% \n",
      "Epoch: [153][0/391]\tTime 0.260 (0.260)\tData 0.228 (0.228)\tLoss 0.0577 (0.0577)\tPrec 97.656% (97.656%)\n",
      "Epoch: [153][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0487 (0.0457)\tPrec 98.438% (98.414%)\n",
      "Epoch: [153][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0461 (0.0456)\tPrec 97.656% (98.426%)\n",
      "Epoch: [153][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0245 (0.0456)\tPrec 98.438% (98.419%)\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.2874 (0.2874)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.450% \n",
      "Epoch: [154][0/391]\tTime 0.256 (0.256)\tData 0.223 (0.223)\tLoss 0.0066 (0.0066)\tPrec 100.000% (100.000%)\n",
      "Epoch: [154][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0656 (0.0431)\tPrec 97.656% (98.615%)\n",
      "Epoch: [154][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0523 (0.0428)\tPrec 99.219% (98.558%)\n",
      "Epoch: [154][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0481 (0.0412)\tPrec 98.438% (98.643%)\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3679 (0.3679)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.400% \n",
      "Epoch: [155][0/391]\tTime 0.267 (0.267)\tData 0.226 (0.226)\tLoss 0.0348 (0.0348)\tPrec 99.219% (99.219%)\n",
      "Epoch: [155][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0273 (0.0432)\tPrec 98.438% (98.453%)\n",
      "Epoch: [155][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0131 (0.0461)\tPrec 100.000% (98.348%)\n",
      "Epoch: [155][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0131 (0.0452)\tPrec 100.000% (98.409%)\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.3179 (0.3179)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.170% \n",
      "Epoch: [156][0/391]\tTime 0.247 (0.247)\tData 0.214 (0.214)\tLoss 0.0175 (0.0175)\tPrec 99.219% (99.219%)\n",
      "Epoch: [156][100/391]\tTime 0.051 (0.051)\tData 0.001 (0.003)\tLoss 0.0266 (0.0405)\tPrec 99.219% (98.584%)\n",
      "Epoch: [156][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0244 (0.0427)\tPrec 99.219% (98.535%)\n",
      "Epoch: [156][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0479 (0.0432)\tPrec 97.656% (98.513%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.4462 (0.4462)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.410% \n",
      "Epoch: [157][0/391]\tTime 0.263 (0.263)\tData 0.223 (0.223)\tLoss 0.0138 (0.0138)\tPrec 99.219% (99.219%)\n",
      "Epoch: [157][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0349 (0.0438)\tPrec 98.438% (98.407%)\n",
      "Epoch: [157][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0242 (0.0432)\tPrec 98.438% (98.449%)\n",
      "Epoch: [157][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1243 (0.0429)\tPrec 95.312% (98.505%)\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.3717 (0.3717)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.240% \n",
      "Epoch: [158][0/391]\tTime 0.255 (0.255)\tData 0.224 (0.224)\tLoss 0.0599 (0.0599)\tPrec 97.656% (97.656%)\n",
      "Epoch: [158][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.004)\tLoss 0.0366 (0.0434)\tPrec 98.438% (98.376%)\n",
      "Epoch: [158][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0214 (0.0440)\tPrec 99.219% (98.426%)\n",
      "Epoch: [158][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0038 (0.0437)\tPrec 100.000% (98.450%)\n",
      "Test: [0/79]\tTime 0.152 (0.152)\tLoss 0.4062 (0.4062)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.670% \n",
      "Epoch: [159][0/391]\tTime 0.256 (0.256)\tData 0.223 (0.223)\tLoss 0.0270 (0.0270)\tPrec 99.219% (99.219%)\n",
      "Epoch: [159][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0659 (0.0408)\tPrec 96.094% (98.414%)\n",
      "Epoch: [159][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0423 (0.0398)\tPrec 99.219% (98.535%)\n",
      "Epoch: [159][300/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0433 (0.0418)\tPrec 98.438% (98.474%)\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.4077 (0.4077)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.630% \n",
      "Epoch: [160][0/391]\tTime 0.295 (0.295)\tData 0.257 (0.257)\tLoss 0.0611 (0.0611)\tPrec 98.438% (98.438%)\n",
      "Epoch: [160][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0105 (0.0443)\tPrec 100.000% (98.492%)\n",
      "Epoch: [160][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0479 (0.0470)\tPrec 99.219% (98.391%)\n",
      "Epoch: [160][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0349 (0.0475)\tPrec 98.438% (98.417%)\n",
      "Test: [0/79]\tTime 0.166 (0.166)\tLoss 0.3368 (0.3368)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.820% \n",
      "Epoch: [161][0/391]\tTime 0.258 (0.258)\tData 0.224 (0.224)\tLoss 0.0550 (0.0550)\tPrec 99.219% (99.219%)\n",
      "Epoch: [161][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0768 (0.0407)\tPrec 96.875% (98.530%)\n",
      "Epoch: [161][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0183 (0.0393)\tPrec 100.000% (98.585%)\n",
      "Epoch: [161][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0535 (0.0394)\tPrec 98.438% (98.637%)\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.4287 (0.4287)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.370% \n",
      "Epoch: [162][0/391]\tTime 0.270 (0.270)\tData 0.237 (0.237)\tLoss 0.0623 (0.0623)\tPrec 97.656% (97.656%)\n",
      "Epoch: [162][100/391]\tTime 0.058 (0.052)\tData 0.001 (0.004)\tLoss 0.0822 (0.0405)\tPrec 98.438% (98.584%)\n",
      "Epoch: [162][200/391]\tTime 0.057 (0.051)\tData 0.001 (0.002)\tLoss 0.0952 (0.0398)\tPrec 96.875% (98.612%)\n",
      "Epoch: [162][300/391]\tTime 0.057 (0.050)\tData 0.001 (0.002)\tLoss 0.0222 (0.0418)\tPrec 99.219% (98.552%)\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.4864 (0.4864)\tPrec 86.719% (86.719%)\n",
      " * Prec 89.600% \n",
      "Epoch: [163][0/391]\tTime 0.258 (0.258)\tData 0.224 (0.224)\tLoss 0.0186 (0.0186)\tPrec 99.219% (99.219%)\n",
      "Epoch: [163][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.1020 (0.0444)\tPrec 96.094% (98.368%)\n",
      "Epoch: [163][200/391]\tTime 0.050 (0.051)\tData 0.002 (0.002)\tLoss 0.0853 (0.0452)\tPrec 95.312% (98.395%)\n",
      "Epoch: [163][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0658 (0.0456)\tPrec 97.656% (98.388%)\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.4412 (0.4412)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.090% \n",
      "Epoch: [164][0/391]\tTime 0.236 (0.236)\tData 0.203 (0.203)\tLoss 0.0341 (0.0341)\tPrec 98.438% (98.438%)\n",
      "Epoch: [164][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0213 (0.0420)\tPrec 99.219% (98.615%)\n",
      "Epoch: [164][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0458 (0.0441)\tPrec 98.438% (98.469%)\n",
      "Epoch: [164][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0515 (0.0453)\tPrec 98.438% (98.422%)\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.3363 (0.3363)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.830% \n",
      "Epoch: [165][0/391]\tTime 0.295 (0.295)\tData 0.265 (0.265)\tLoss 0.0050 (0.0050)\tPrec 100.000% (100.000%)\n",
      "Epoch: [165][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0747 (0.0395)\tPrec 98.438% (98.592%)\n",
      "Epoch: [165][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.003)\tLoss 0.0186 (0.0414)\tPrec 100.000% (98.539%)\n",
      "Epoch: [165][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0290 (0.0446)\tPrec 98.438% (98.432%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.4003 (0.4003)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.830% \n",
      "Epoch: [166][0/391]\tTime 0.349 (0.349)\tData 0.308 (0.308)\tLoss 0.0124 (0.0124)\tPrec 100.000% (100.000%)\n",
      "Epoch: [166][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0725 (0.0357)\tPrec 97.656% (98.786%)\n",
      "Epoch: [166][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0680 (0.0394)\tPrec 96.875% (98.678%)\n",
      "Epoch: [166][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0057 (0.0401)\tPrec 100.000% (98.653%)\n",
      "Test: [0/79]\tTime 0.152 (0.152)\tLoss 0.4663 (0.4663)\tPrec 87.500% (87.500%)\n",
      " * Prec 89.710% \n",
      "Epoch: [167][0/391]\tTime 0.258 (0.258)\tData 0.219 (0.219)\tLoss 0.0491 (0.0491)\tPrec 97.656% (97.656%)\n",
      "Epoch: [167][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0571 (0.0457)\tPrec 96.875% (98.453%)\n",
      "Epoch: [167][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0717 (0.0449)\tPrec 96.875% (98.453%)\n",
      "Epoch: [167][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0093 (0.0442)\tPrec 100.000% (98.487%)\n",
      "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.3329 (0.3329)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.700% \n",
      "Epoch: [168][0/391]\tTime 0.235 (0.235)\tData 0.194 (0.194)\tLoss 0.0308 (0.0308)\tPrec 98.438% (98.438%)\n",
      "Epoch: [168][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0118 (0.0375)\tPrec 100.000% (98.739%)\n",
      "Epoch: [168][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0835 (0.0378)\tPrec 98.438% (98.717%)\n",
      "Epoch: [168][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0977 (0.0384)\tPrec 96.094% (98.718%)\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.3674 (0.3674)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.990% \n",
      "Epoch: [169][0/391]\tTime 0.342 (0.342)\tData 0.309 (0.309)\tLoss 0.0591 (0.0591)\tPrec 97.656% (97.656%)\n",
      "Epoch: [169][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0149 (0.0389)\tPrec 99.219% (98.685%)\n",
      "Epoch: [169][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0361 (0.0412)\tPrec 99.219% (98.682%)\n",
      "Epoch: [169][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0543 (0.0426)\tPrec 98.438% (98.578%)\n",
      "Test: [0/79]\tTime 0.160 (0.160)\tLoss 0.3981 (0.3981)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.810% \n",
      "Epoch: [170][0/391]\tTime 0.289 (0.289)\tData 0.257 (0.257)\tLoss 0.0135 (0.0135)\tPrec 99.219% (99.219%)\n",
      "Epoch: [170][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0200 (0.0331)\tPrec 100.000% (98.840%)\n",
      "Epoch: [170][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0205 (0.0365)\tPrec 99.219% (98.795%)\n",
      "Epoch: [170][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 0.0109 (0.0366)\tPrec 100.000% (98.785%)\n",
      "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.4442 (0.4442)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.780% \n",
      "Epoch: [171][0/391]\tTime 0.259 (0.259)\tData 0.228 (0.228)\tLoss 0.0644 (0.0644)\tPrec 96.875% (96.875%)\n",
      "Epoch: [171][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.004)\tLoss 0.0419 (0.0401)\tPrec 98.438% (98.724%)\n",
      "Epoch: [171][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0660 (0.0388)\tPrec 96.094% (98.737%)\n",
      "Epoch: [171][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.0278 (0.0378)\tPrec 99.219% (98.726%)\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.3334 (0.3334)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.560% \n",
      "Epoch: [172][0/391]\tTime 0.284 (0.284)\tData 0.252 (0.252)\tLoss 0.0325 (0.0325)\tPrec 99.219% (99.219%)\n",
      "Epoch: [172][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0474 (0.0420)\tPrec 98.438% (98.569%)\n",
      "Epoch: [172][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0089 (0.0415)\tPrec 100.000% (98.605%)\n",
      "Epoch: [172][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0565 (0.0416)\tPrec 97.656% (98.567%)\n",
      "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.5079 (0.5079)\tPrec 85.938% (85.938%)\n",
      " * Prec 89.330% \n",
      "Epoch: [173][0/391]\tTime 0.255 (0.255)\tData 0.221 (0.221)\tLoss 0.0617 (0.0617)\tPrec 96.094% (96.094%)\n",
      "Epoch: [173][100/391]\tTime 0.049 (0.051)\tData 0.002 (0.004)\tLoss 0.0448 (0.0355)\tPrec 97.656% (98.778%)\n",
      "Epoch: [173][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0214 (0.0383)\tPrec 98.438% (98.640%)\n",
      "Epoch: [173][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0490 (0.0378)\tPrec 98.438% (98.624%)\n",
      "Test: [0/79]\tTime 0.229 (0.229)\tLoss 0.3749 (0.3749)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.730% \n",
      "Epoch: [174][0/391]\tTime 0.332 (0.332)\tData 0.257 (0.257)\tLoss 0.0159 (0.0159)\tPrec 100.000% (100.000%)\n",
      "Epoch: [174][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0154 (0.0387)\tPrec 99.219% (98.693%)\n",
      "Epoch: [174][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0065 (0.0355)\tPrec 100.000% (98.772%)\n",
      "Epoch: [174][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0388 (0.0378)\tPrec 98.438% (98.723%)\n",
      "Test: [0/79]\tTime 0.149 (0.149)\tLoss 0.4670 (0.4670)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.250% \n",
      "Epoch: [175][0/391]\tTime 0.334 (0.334)\tData 0.303 (0.303)\tLoss 0.0714 (0.0714)\tPrec 98.438% (98.438%)\n",
      "Epoch: [175][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0260 (0.0408)\tPrec 98.438% (98.608%)\n",
      "Epoch: [175][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0056 (0.0414)\tPrec 100.000% (98.562%)\n",
      "Epoch: [175][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0308 (0.0432)\tPrec 97.656% (98.508%)\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.4031 (0.4031)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.330% \n",
      "Epoch: [176][0/391]\tTime 0.263 (0.263)\tData 0.224 (0.224)\tLoss 0.0458 (0.0458)\tPrec 99.219% (99.219%)\n",
      "Epoch: [176][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0408 (0.0450)\tPrec 98.438% (98.461%)\n",
      "Epoch: [176][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0406 (0.0415)\tPrec 98.438% (98.585%)\n",
      "Epoch: [176][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0042 (0.0409)\tPrec 100.000% (98.617%)\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.3617 (0.3617)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.470% \n",
      "Epoch: [177][0/391]\tTime 0.297 (0.297)\tData 0.257 (0.257)\tLoss 0.0304 (0.0304)\tPrec 99.219% (99.219%)\n",
      "Epoch: [177][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0703 (0.0386)\tPrec 98.438% (98.716%)\n",
      "Epoch: [177][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0056 (0.0369)\tPrec 100.000% (98.745%)\n",
      "Epoch: [177][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0354 (0.0392)\tPrec 98.438% (98.648%)\n",
      "Test: [0/79]\tTime 0.181 (0.181)\tLoss 0.4089 (0.4089)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.770% \n",
      "Epoch: [178][0/391]\tTime 0.254 (0.254)\tData 0.222 (0.222)\tLoss 0.0499 (0.0499)\tPrec 98.438% (98.438%)\n",
      "Epoch: [178][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.1181 (0.0388)\tPrec 97.656% (98.554%)\n",
      "Epoch: [178][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0139 (0.0394)\tPrec 99.219% (98.577%)\n",
      "Epoch: [178][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0910 (0.0416)\tPrec 96.875% (98.510%)\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.4295 (0.4295)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.740% \n",
      "Epoch: [179][0/391]\tTime 0.254 (0.254)\tData 0.222 (0.222)\tLoss 0.0588 (0.0588)\tPrec 98.438% (98.438%)\n",
      "Epoch: [179][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0097 (0.0409)\tPrec 100.000% (98.600%)\n",
      "Epoch: [179][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0244 (0.0404)\tPrec 99.219% (98.558%)\n",
      "Epoch: [179][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0064 (0.0417)\tPrec 100.000% (98.531%)\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.3736 (0.3736)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.260% \n",
      "Epoch: [180][0/391]\tTime 0.335 (0.335)\tData 0.258 (0.258)\tLoss 0.0266 (0.0266)\tPrec 98.438% (98.438%)\n",
      "Epoch: [180][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0373 (0.0397)\tPrec 98.438% (98.708%)\n",
      "Epoch: [180][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0579 (0.0395)\tPrec 96.875% (98.663%)\n",
      "Epoch: [180][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0520 (0.0399)\tPrec 98.438% (98.619%)\n",
      "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.3454 (0.3454)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.630% \n",
      "Epoch: [181][0/391]\tTime 0.293 (0.293)\tData 0.258 (0.258)\tLoss 0.0649 (0.0649)\tPrec 98.438% (98.438%)\n",
      "Epoch: [181][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0407 (0.0334)\tPrec 98.438% (98.832%)\n",
      "Epoch: [181][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0214 (0.0341)\tPrec 99.219% (98.842%)\n",
      "Epoch: [181][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0261 (0.0349)\tPrec 99.219% (98.809%)\n",
      "Test: [0/79]\tTime 0.210 (0.210)\tLoss 0.4393 (0.4393)\tPrec 87.500% (87.500%)\n",
      " * Prec 89.740% \n",
      "Epoch: [182][0/391]\tTime 0.283 (0.283)\tData 0.252 (0.252)\tLoss 0.0600 (0.0600)\tPrec 98.438% (98.438%)\n",
      "Epoch: [182][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0651 (0.0373)\tPrec 98.438% (98.631%)\n",
      "Epoch: [182][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0922 (0.0393)\tPrec 96.875% (98.566%)\n",
      "Epoch: [182][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0852 (0.0419)\tPrec 97.656% (98.487%)\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.3728 (0.3728)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.690% \n",
      "Epoch: [183][0/391]\tTime 0.254 (0.254)\tData 0.221 (0.221)\tLoss 0.0456 (0.0456)\tPrec 96.875% (96.875%)\n",
      "Epoch: [183][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0155 (0.0352)\tPrec 100.000% (98.801%)\n",
      "Epoch: [183][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0504 (0.0367)\tPrec 96.875% (98.764%)\n",
      "Epoch: [183][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0492 (0.0378)\tPrec 99.219% (98.715%)\n",
      "Test: [0/79]\tTime 0.165 (0.165)\tLoss 0.4856 (0.4856)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.150% \n",
      "Epoch: [184][0/391]\tTime 0.242 (0.242)\tData 0.207 (0.207)\tLoss 0.0217 (0.0217)\tPrec 99.219% (99.219%)\n",
      "Epoch: [184][100/391]\tTime 0.052 (0.051)\tData 0.001 (0.003)\tLoss 0.0649 (0.0392)\tPrec 97.656% (98.708%)\n",
      "Epoch: [184][200/391]\tTime 0.053 (0.050)\tData 0.001 (0.002)\tLoss 0.0300 (0.0364)\tPrec 99.219% (98.783%)\n",
      "Epoch: [184][300/391]\tTime 0.052 (0.050)\tData 0.001 (0.002)\tLoss 0.0093 (0.0370)\tPrec 100.000% (98.765%)\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.3392 (0.3392)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.510% \n",
      "Epoch: [185][0/391]\tTime 0.286 (0.286)\tData 0.245 (0.245)\tLoss 0.0816 (0.0816)\tPrec 98.438% (98.438%)\n",
      "Epoch: [185][100/391]\tTime 0.050 (0.052)\tData 0.002 (0.004)\tLoss 0.0501 (0.0349)\tPrec 97.656% (98.902%)\n",
      "Epoch: [185][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0042 (0.0356)\tPrec 100.000% (98.850%)\n",
      "Epoch: [185][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0232 (0.0366)\tPrec 99.219% (98.816%)\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.3995 (0.3995)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.460% \n",
      "Epoch: [186][0/391]\tTime 0.263 (0.263)\tData 0.223 (0.223)\tLoss 0.0261 (0.0261)\tPrec 98.438% (98.438%)\n",
      "Epoch: [186][100/391]\tTime 0.042 (0.051)\tData 0.001 (0.003)\tLoss 0.0257 (0.0384)\tPrec 99.219% (98.608%)\n",
      "Epoch: [186][200/391]\tTime 0.042 (0.050)\tData 0.001 (0.002)\tLoss 0.0402 (0.0352)\tPrec 99.219% (98.733%)\n",
      "Epoch: [186][300/391]\tTime 0.042 (0.050)\tData 0.001 (0.002)\tLoss 0.0375 (0.0366)\tPrec 98.438% (98.702%)\n",
      "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.3704 (0.3704)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.470% \n",
      "Epoch: [187][0/391]\tTime 0.257 (0.257)\tData 0.217 (0.217)\tLoss 0.0084 (0.0084)\tPrec 100.000% (100.000%)\n",
      "Epoch: [187][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0432 (0.0402)\tPrec 98.438% (98.615%)\n",
      "Epoch: [187][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0108 (0.0363)\tPrec 99.219% (98.698%)\n",
      "Epoch: [187][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0445 (0.0380)\tPrec 98.438% (98.653%)\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.2927 (0.2927)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.120% \n",
      "Epoch: [188][0/391]\tTime 0.256 (0.256)\tData 0.224 (0.224)\tLoss 0.0185 (0.0185)\tPrec 100.000% (100.000%)\n",
      "Epoch: [188][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0637 (0.0361)\tPrec 97.656% (98.770%)\n",
      "Epoch: [188][200/391]\tTime 0.049 (0.051)\tData 0.002 (0.002)\tLoss 0.0666 (0.0357)\tPrec 98.438% (98.842%)\n",
      "Epoch: [188][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0518 (0.0353)\tPrec 97.656% (98.806%)\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.4201 (0.4201)\tPrec 90.625% (90.625%)\n",
      " * Prec 90.040% \n",
      "Epoch: [189][0/391]\tTime 0.302 (0.302)\tData 0.263 (0.263)\tLoss 0.0773 (0.0773)\tPrec 97.656% (97.656%)\n",
      "Epoch: [189][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0096 (0.0306)\tPrec 99.219% (98.886%)\n",
      "Epoch: [189][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0449 (0.0351)\tPrec 96.875% (98.729%)\n",
      "Epoch: [189][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0160 (0.0365)\tPrec 99.219% (98.687%)\n",
      "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.4779 (0.4779)\tPrec 90.625% (90.625%)\n",
      " * Prec 90.100% \n",
      "Epoch: [190][0/391]\tTime 0.261 (0.261)\tData 0.224 (0.224)\tLoss 0.0985 (0.0985)\tPrec 96.875% (96.875%)\n",
      "Epoch: [190][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0598 (0.0371)\tPrec 97.656% (98.677%)\n",
      "Epoch: [190][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0612 (0.0371)\tPrec 97.656% (98.733%)\n",
      "Epoch: [190][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0311 (0.0375)\tPrec 98.438% (98.718%)\n",
      "Test: [0/79]\tTime 0.176 (0.176)\tLoss 0.3175 (0.3175)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.640% \n",
      "Epoch: [191][0/391]\tTime 0.255 (0.255)\tData 0.222 (0.222)\tLoss 0.0777 (0.0777)\tPrec 96.875% (96.875%)\n",
      "Epoch: [191][100/391]\tTime 0.046 (0.051)\tData 0.001 (0.003)\tLoss 0.0060 (0.0374)\tPrec 100.000% (98.623%)\n",
      "Epoch: [191][200/391]\tTime 0.051 (0.050)\tData 0.001 (0.002)\tLoss 0.0603 (0.0404)\tPrec 97.656% (98.577%)\n",
      "Epoch: [191][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0455 (0.0403)\tPrec 99.219% (98.611%)\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.4327 (0.4327)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.790% \n",
      "Epoch: [192][0/391]\tTime 0.250 (0.250)\tData 0.215 (0.215)\tLoss 0.0339 (0.0339)\tPrec 98.438% (98.438%)\n",
      "Epoch: [192][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0784 (0.0362)\tPrec 96.094% (98.731%)\n",
      "Epoch: [192][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0368 (0.0353)\tPrec 99.219% (98.799%)\n",
      "Epoch: [192][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0219 (0.0359)\tPrec 99.219% (98.775%)\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.2946 (0.2946)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.340% \n",
      "Epoch: [193][0/391]\tTime 0.313 (0.313)\tData 0.252 (0.252)\tLoss 0.0261 (0.0261)\tPrec 99.219% (99.219%)\n",
      "Epoch: [193][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0193 (0.0385)\tPrec 99.219% (98.708%)\n",
      "Epoch: [193][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0061 (0.0391)\tPrec 100.000% (98.647%)\n",
      "Epoch: [193][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.1108 (0.0384)\tPrec 95.312% (98.653%)\n",
      "Test: [0/79]\tTime 0.160 (0.160)\tLoss 0.3422 (0.3422)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.650% \n",
      "Epoch: [194][0/391]\tTime 0.255 (0.255)\tData 0.220 (0.220)\tLoss 0.0236 (0.0236)\tPrec 99.219% (99.219%)\n",
      "Epoch: [194][100/391]\tTime 0.049 (0.051)\tData 0.002 (0.003)\tLoss 0.0470 (0.0374)\tPrec 99.219% (98.724%)\n",
      "Epoch: [194][200/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0150 (0.0428)\tPrec 100.000% (98.496%)\n",
      "Epoch: [194][300/391]\tTime 0.051 (0.050)\tData 0.002 (0.002)\tLoss 0.0276 (0.0418)\tPrec 99.219% (98.557%)\n",
      "Test: [0/79]\tTime 0.135 (0.135)\tLoss 0.2475 (0.2475)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.710% \n",
      "Epoch: [195][0/391]\tTime 0.263 (0.263)\tData 0.221 (0.221)\tLoss 0.1075 (0.1075)\tPrec 96.875% (96.875%)\n",
      "Epoch: [195][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0643 (0.0410)\tPrec 98.438% (98.600%)\n",
      "Epoch: [195][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0037 (0.0409)\tPrec 100.000% (98.612%)\n",
      "Epoch: [195][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0582 (0.0393)\tPrec 97.656% (98.666%)\n",
      "Test: [0/79]\tTime 0.160 (0.160)\tLoss 0.4354 (0.4354)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.330% \n",
      "Epoch: [196][0/391]\tTime 0.299 (0.299)\tData 0.227 (0.227)\tLoss 0.0334 (0.0334)\tPrec 99.219% (99.219%)\n",
      "Epoch: [196][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0206 (0.0369)\tPrec 98.438% (98.646%)\n",
      "Epoch: [196][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.003)\tLoss 0.0294 (0.0406)\tPrec 98.438% (98.585%)\n",
      "Epoch: [196][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0326 (0.0403)\tPrec 98.438% (98.591%)\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.4638 (0.4638)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.130% \n",
      "Epoch: [197][0/391]\tTime 0.353 (0.353)\tData 0.320 (0.320)\tLoss 0.0389 (0.0389)\tPrec 98.438% (98.438%)\n",
      "Epoch: [197][100/391]\tTime 0.049 (0.053)\tData 0.001 (0.005)\tLoss 0.0133 (0.0393)\tPrec 100.000% (98.631%)\n",
      "Epoch: [197][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0119 (0.0388)\tPrec 100.000% (98.667%)\n",
      "Epoch: [197][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0211 (0.0379)\tPrec 99.219% (98.661%)\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.3477 (0.3477)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.410% \n",
      "Epoch: [198][0/391]\tTime 0.251 (0.251)\tData 0.218 (0.218)\tLoss 0.0072 (0.0072)\tPrec 100.000% (100.000%)\n",
      "Epoch: [198][100/391]\tTime 0.048 (0.051)\tData 0.001 (0.003)\tLoss 0.0215 (0.0375)\tPrec 100.000% (98.631%)\n",
      "Epoch: [198][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0073 (0.0361)\tPrec 100.000% (98.745%)\n",
      "Epoch: [198][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0788 (0.0384)\tPrec 96.875% (98.666%)\n",
      "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.4304 (0.4304)\tPrec 89.844% (89.844%)\n",
      " * Prec 90.140% \n",
      "Epoch: [199][0/391]\tTime 0.257 (0.257)\tData 0.220 (0.220)\tLoss 0.0274 (0.0274)\tPrec 99.219% (99.219%)\n",
      "Epoch: [199][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0176 (0.0328)\tPrec 99.219% (98.840%)\n",
      "Epoch: [199][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0229 (0.0343)\tPrec 99.219% (98.783%)\n",
      "Epoch: [199][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0260 (0.0356)\tPrec 99.219% (98.757%)\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.4034 (0.4034)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.660% \n",
      "Epoch: [200][0/391]\tTime 0.290 (0.290)\tData 0.251 (0.251)\tLoss 0.0532 (0.0532)\tPrec 97.656% (97.656%)\n",
      "Epoch: [200][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.1000 (0.0374)\tPrec 97.656% (98.693%)\n",
      "Epoch: [200][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0533 (0.0358)\tPrec 97.656% (98.776%)\n",
      "Epoch: [200][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0142 (0.0361)\tPrec 100.000% (98.749%)\n",
      "Test: [0/79]\tTime 0.170 (0.170)\tLoss 0.3544 (0.3544)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.420% \n",
      "Epoch: [201][0/391]\tTime 0.259 (0.259)\tData 0.219 (0.219)\tLoss 0.0361 (0.0361)\tPrec 98.438% (98.438%)\n",
      "Epoch: [201][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0393 (0.0432)\tPrec 97.656% (98.484%)\n",
      "Epoch: [201][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.1495 (0.0418)\tPrec 95.312% (98.472%)\n",
      "Epoch: [201][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0476 (0.0402)\tPrec 99.219% (98.554%)\n",
      "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.3450 (0.3450)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.900% \n",
      "Epoch: [202][0/391]\tTime 0.261 (0.261)\tData 0.221 (0.221)\tLoss 0.0291 (0.0291)\tPrec 99.219% (99.219%)\n",
      "Epoch: [202][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0477 (0.0356)\tPrec 98.438% (98.755%)\n",
      "Epoch: [202][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0212 (0.0350)\tPrec 99.219% (98.783%)\n",
      "Epoch: [202][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0292 (0.0356)\tPrec 97.656% (98.759%)\n",
      "Test: [0/79]\tTime 0.110 (0.110)\tLoss 0.3433 (0.3433)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.510% \n",
      "Epoch: [203][0/391]\tTime 0.315 (0.315)\tData 0.276 (0.276)\tLoss 0.0205 (0.0205)\tPrec 99.219% (99.219%)\n",
      "Epoch: [203][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0176 (0.0343)\tPrec 99.219% (98.809%)\n",
      "Epoch: [203][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0188 (0.0335)\tPrec 100.000% (98.861%)\n",
      "Epoch: [203][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0713 (0.0354)\tPrec 98.438% (98.824%)\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.3774 (0.3774)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.310% \n",
      "Epoch: [204][0/391]\tTime 0.293 (0.293)\tData 0.253 (0.253)\tLoss 0.0621 (0.0621)\tPrec 97.656% (97.656%)\n",
      "Epoch: [204][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0788 (0.0340)\tPrec 97.656% (98.878%)\n",
      "Epoch: [204][200/391]\tTime 0.051 (0.051)\tData 0.001 (0.002)\tLoss 0.0620 (0.0365)\tPrec 97.656% (98.756%)\n",
      "Epoch: [204][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0106 (0.0365)\tPrec 100.000% (98.744%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.3194 (0.3194)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.820% \n",
      "Epoch: [205][0/391]\tTime 0.236 (0.236)\tData 0.204 (0.204)\tLoss 0.0519 (0.0519)\tPrec 99.219% (99.219%)\n",
      "Epoch: [205][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0287 (0.0394)\tPrec 98.438% (98.623%)\n",
      "Epoch: [205][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0343 (0.0357)\tPrec 97.656% (98.748%)\n",
      "Epoch: [205][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0060 (0.0369)\tPrec 100.000% (98.702%)\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.4554 (0.4554)\tPrec 88.281% (88.281%)\n",
      " * Prec 88.560% \n",
      "Epoch: [206][0/391]\tTime 0.322 (0.322)\tData 0.251 (0.251)\tLoss 0.0465 (0.0465)\tPrec 99.219% (99.219%)\n",
      "Epoch: [206][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0205 (0.0364)\tPrec 99.219% (98.724%)\n",
      "Epoch: [206][200/391]\tTime 0.051 (0.051)\tData 0.001 (0.002)\tLoss 0.0220 (0.0349)\tPrec 99.219% (98.783%)\n",
      "Epoch: [206][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0433 (0.0374)\tPrec 97.656% (98.733%)\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.3437 (0.3437)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.090% \n",
      "Epoch: [207][0/391]\tTime 0.265 (0.265)\tData 0.226 (0.226)\tLoss 0.0178 (0.0178)\tPrec 99.219% (99.219%)\n",
      "Epoch: [207][100/391]\tTime 0.046 (0.051)\tData 0.001 (0.003)\tLoss 0.0178 (0.0314)\tPrec 99.219% (98.878%)\n",
      "Epoch: [207][200/391]\tTime 0.046 (0.050)\tData 0.001 (0.002)\tLoss 0.0051 (0.0347)\tPrec 100.000% (98.756%)\n",
      "Epoch: [207][300/391]\tTime 0.046 (0.050)\tData 0.001 (0.002)\tLoss 0.0126 (0.0359)\tPrec 100.000% (98.707%)\n",
      "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.3215 (0.3215)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.070% \n",
      "Epoch: [208][0/391]\tTime 0.396 (0.396)\tData 0.321 (0.321)\tLoss 0.0586 (0.0586)\tPrec 98.438% (98.438%)\n",
      "Epoch: [208][100/391]\tTime 0.049 (0.053)\tData 0.001 (0.004)\tLoss 0.0210 (0.0383)\tPrec 98.438% (98.685%)\n",
      "Epoch: [208][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0257 (0.0352)\tPrec 98.438% (98.791%)\n",
      "Epoch: [208][300/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0236 (0.0361)\tPrec 100.000% (98.749%)\n",
      "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.3174 (0.3174)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.560% \n",
      "Epoch: [209][0/391]\tTime 0.163 (0.163)\tData 0.132 (0.132)\tLoss 0.0424 (0.0424)\tPrec 97.656% (97.656%)\n",
      "Epoch: [209][100/391]\tTime 0.052 (0.051)\tData 0.001 (0.003)\tLoss 0.0740 (0.0350)\tPrec 98.438% (98.739%)\n",
      "Epoch: [209][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0252 (0.0354)\tPrec 98.438% (98.752%)\n",
      "Epoch: [209][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0390 (0.0365)\tPrec 99.219% (98.739%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.3638 (0.3638)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.020% \n",
      "Epoch: [210][0/391]\tTime 0.259 (0.259)\tData 0.219 (0.219)\tLoss 0.0184 (0.0184)\tPrec 99.219% (99.219%)\n",
      "Epoch: [210][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0234 (0.0324)\tPrec 99.219% (98.878%)\n",
      "Epoch: [210][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0480 (0.0326)\tPrec 97.656% (98.912%)\n",
      "Epoch: [210][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0591 (0.0332)\tPrec 98.438% (98.912%)\n",
      "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.3582 (0.3582)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.770% \n",
      "Epoch: [211][0/391]\tTime 0.211 (0.211)\tData 0.134 (0.134)\tLoss 0.1134 (0.1134)\tPrec 97.656% (97.656%)\n",
      "Epoch: [211][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0290 (0.0339)\tPrec 99.219% (98.940%)\n",
      "Epoch: [211][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0189 (0.0366)\tPrec 99.219% (98.787%)\n",
      "Epoch: [211][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0422 (0.0363)\tPrec 98.438% (98.793%)\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.3625 (0.3625)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.790% \n",
      "Epoch: [212][0/391]\tTime 0.355 (0.355)\tData 0.281 (0.281)\tLoss 0.0448 (0.0448)\tPrec 99.219% (99.219%)\n",
      "Epoch: [212][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0081 (0.0323)\tPrec 100.000% (98.863%)\n",
      "Epoch: [212][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0128 (0.0351)\tPrec 99.219% (98.834%)\n",
      "Epoch: [212][300/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.0171 (0.0361)\tPrec 99.219% (98.778%)\n",
      "Test: [0/79]\tTime 0.169 (0.169)\tLoss 0.3407 (0.3407)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.650% \n",
      "Epoch: [213][0/391]\tTime 0.263 (0.263)\tData 0.223 (0.223)\tLoss 0.0311 (0.0311)\tPrec 98.438% (98.438%)\n",
      "Epoch: [213][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0325 (0.0410)\tPrec 98.438% (98.631%)\n",
      "Epoch: [213][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0656 (0.0392)\tPrec 98.438% (98.686%)\n",
      "Epoch: [213][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0625 (0.0394)\tPrec 97.656% (98.671%)\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 0.4279 (0.4279)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.390% \n",
      "Epoch: [214][0/391]\tTime 0.282 (0.282)\tData 0.250 (0.250)\tLoss 0.0403 (0.0403)\tPrec 98.438% (98.438%)\n",
      "Epoch: [214][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0109 (0.0309)\tPrec 100.000% (98.940%)\n",
      "Epoch: [214][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0325 (0.0308)\tPrec 98.438% (98.923%)\n",
      "Epoch: [214][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0893 (0.0334)\tPrec 98.438% (98.853%)\n",
      "Test: [0/79]\tTime 0.165 (0.165)\tLoss 0.2739 (0.2739)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.750% \n",
      "Epoch: [215][0/391]\tTime 0.257 (0.257)\tData 0.222 (0.222)\tLoss 0.0180 (0.0180)\tPrec 100.000% (100.000%)\n",
      "Epoch: [215][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0735 (0.0320)\tPrec 98.438% (98.940%)\n",
      "Epoch: [215][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.1013 (0.0330)\tPrec 96.094% (98.892%)\n",
      "Epoch: [215][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0682 (0.0345)\tPrec 97.656% (98.814%)\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.4088 (0.4088)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.450% \n",
      "Epoch: [216][0/391]\tTime 0.264 (0.264)\tData 0.224 (0.224)\tLoss 0.0199 (0.0199)\tPrec 99.219% (99.219%)\n",
      "Epoch: [216][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0339 (0.0325)\tPrec 98.438% (98.956%)\n",
      "Epoch: [216][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0076 (0.0370)\tPrec 100.000% (98.748%)\n",
      "Epoch: [216][300/391]\tTime 0.046 (0.050)\tData 0.001 (0.002)\tLoss 0.1381 (0.0372)\tPrec 96.875% (98.772%)\n",
      "Test: [0/79]\tTime 0.113 (0.113)\tLoss 0.2960 (0.2960)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.070% \n",
      "Epoch: [217][0/391]\tTime 0.302 (0.302)\tData 0.261 (0.261)\tLoss 0.0926 (0.0926)\tPrec 97.656% (97.656%)\n",
      "Epoch: [217][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0405 (0.0350)\tPrec 98.438% (98.739%)\n",
      "Epoch: [217][200/391]\tTime 0.050 (0.051)\tData 0.002 (0.003)\tLoss 0.0227 (0.0363)\tPrec 99.219% (98.772%)\n",
      "Epoch: [217][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0264 (0.0360)\tPrec 99.219% (98.793%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.3244 (0.3244)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.540% \n",
      "Epoch: [218][0/391]\tTime 0.356 (0.356)\tData 0.323 (0.323)\tLoss 0.0125 (0.0125)\tPrec 99.219% (99.219%)\n",
      "Epoch: [218][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0322 (0.0336)\tPrec 98.438% (98.855%)\n",
      "Epoch: [218][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0127 (0.0324)\tPrec 99.219% (98.900%)\n",
      "Epoch: [218][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0171 (0.0333)\tPrec 99.219% (98.855%)\n",
      "Test: [0/79]\tTime 0.262 (0.262)\tLoss 0.2557 (0.2557)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.500% \n",
      "Epoch: [219][0/391]\tTime 0.307 (0.307)\tData 0.231 (0.231)\tLoss 0.0337 (0.0337)\tPrec 98.438% (98.438%)\n",
      "Epoch: [219][100/391]\tTime 0.049 (0.052)\tData 0.002 (0.004)\tLoss 0.0908 (0.0356)\tPrec 97.656% (98.778%)\n",
      "Epoch: [219][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0303 (0.0352)\tPrec 99.219% (98.756%)\n",
      "Epoch: [219][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0153 (0.0348)\tPrec 99.219% (98.788%)\n",
      "Test: [0/79]\tTime 0.116 (0.116)\tLoss 0.4020 (0.4020)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.480% \n",
      "Epoch: [220][0/391]\tTime 0.306 (0.306)\tData 0.273 (0.273)\tLoss 0.0169 (0.0169)\tPrec 99.219% (99.219%)\n",
      "Epoch: [220][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0239 (0.0368)\tPrec 99.219% (98.778%)\n",
      "Epoch: [220][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0195 (0.0348)\tPrec 99.219% (98.780%)\n",
      "Epoch: [220][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0082 (0.0351)\tPrec 100.000% (98.749%)\n",
      "Test: [0/79]\tTime 0.170 (0.170)\tLoss 0.3153 (0.3153)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.450% \n",
      "Epoch: [221][0/391]\tTime 0.258 (0.258)\tData 0.227 (0.227)\tLoss 0.0699 (0.0699)\tPrec 98.438% (98.438%)\n",
      "Epoch: [221][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0484 (0.0319)\tPrec 99.219% (98.979%)\n",
      "Epoch: [221][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0524 (0.0350)\tPrec 97.656% (98.850%)\n",
      "Epoch: [221][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0254 (0.0356)\tPrec 98.438% (98.814%)\n",
      "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.3297 (0.3297)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.890% \n",
      "Epoch: [222][0/391]\tTime 0.314 (0.314)\tData 0.274 (0.274)\tLoss 0.0358 (0.0358)\tPrec 99.219% (99.219%)\n",
      "Epoch: [222][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0208 (0.0334)\tPrec 99.219% (98.871%)\n",
      "Epoch: [222][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0760 (0.0348)\tPrec 96.875% (98.834%)\n",
      "Epoch: [222][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0081 (0.0353)\tPrec 100.000% (98.835%)\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.3857 (0.3857)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.860% \n",
      "Epoch: [223][0/391]\tTime 0.294 (0.294)\tData 0.218 (0.218)\tLoss 0.0362 (0.0362)\tPrec 97.656% (97.656%)\n",
      "Epoch: [223][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0096 (0.0376)\tPrec 99.219% (98.700%)\n",
      "Epoch: [223][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0392 (0.0372)\tPrec 97.656% (98.717%)\n",
      "Epoch: [223][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0218 (0.0361)\tPrec 100.000% (98.762%)\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.2721 (0.2721)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.000% \n",
      "Epoch: [224][0/391]\tTime 0.253 (0.253)\tData 0.220 (0.220)\tLoss 0.0110 (0.0110)\tPrec 100.000% (100.000%)\n",
      "Epoch: [224][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0448 (0.0294)\tPrec 97.656% (99.064%)\n",
      "Epoch: [224][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0224 (0.0302)\tPrec 99.219% (99.001%)\n",
      "Epoch: [224][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0159 (0.0325)\tPrec 100.000% (98.928%)\n",
      "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.4097 (0.4097)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.390% \n",
      "Epoch: [225][0/391]\tTime 0.263 (0.263)\tData 0.229 (0.229)\tLoss 0.0159 (0.0159)\tPrec 99.219% (99.219%)\n",
      "Epoch: [225][100/391]\tTime 0.050 (0.052)\tData 0.002 (0.003)\tLoss 0.0910 (0.0336)\tPrec 99.219% (98.863%)\n",
      "Epoch: [225][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0725 (0.0315)\tPrec 98.438% (98.958%)\n",
      "Epoch: [225][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0159 (0.0323)\tPrec 99.219% (98.933%)\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.3042 (0.3042)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.390% \n",
      "Epoch: [226][0/391]\tTime 0.259 (0.259)\tData 0.227 (0.227)\tLoss 0.0092 (0.0092)\tPrec 100.000% (100.000%)\n",
      "Epoch: [226][100/391]\tTime 0.053 (0.052)\tData 0.001 (0.004)\tLoss 0.0160 (0.0291)\tPrec 99.219% (98.979%)\n",
      "Epoch: [226][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0579 (0.0330)\tPrec 98.438% (98.865%)\n",
      "Epoch: [226][300/391]\tTime 0.045 (0.050)\tData 0.001 (0.002)\tLoss 0.0900 (0.0336)\tPrec 97.656% (98.835%)\n",
      "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.2663 (0.2663)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.100% \n",
      "Epoch: [227][0/391]\tTime 0.251 (0.251)\tData 0.211 (0.211)\tLoss 0.0175 (0.0175)\tPrec 99.219% (99.219%)\n",
      "Epoch: [227][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0841 (0.0361)\tPrec 96.875% (98.693%)\n",
      "Epoch: [227][200/391]\tTime 0.049 (0.050)\tData 0.002 (0.002)\tLoss 0.0455 (0.0322)\tPrec 98.438% (98.807%)\n",
      "Epoch: [227][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0195 (0.0332)\tPrec 98.438% (98.770%)\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.3347 (0.3347)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.750% \n",
      "Epoch: [228][0/391]\tTime 0.299 (0.299)\tData 0.231 (0.231)\tLoss 0.0389 (0.0389)\tPrec 97.656% (97.656%)\n",
      "Epoch: [228][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0665 (0.0376)\tPrec 97.656% (98.762%)\n",
      "Epoch: [228][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0964 (0.0353)\tPrec 97.656% (98.842%)\n",
      "Epoch: [228][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0545 (0.0371)\tPrec 98.438% (98.741%)\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.3797 (0.3797)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.720% \n",
      "Epoch: [229][0/391]\tTime 0.252 (0.252)\tData 0.220 (0.220)\tLoss 0.0250 (0.0250)\tPrec 99.219% (99.219%)\n",
      "Epoch: [229][100/391]\tTime 0.052 (0.051)\tData 0.001 (0.003)\tLoss 0.0515 (0.0383)\tPrec 98.438% (98.646%)\n",
      "Epoch: [229][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0626 (0.0379)\tPrec 97.656% (98.721%)\n",
      "Epoch: [229][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0154 (0.0369)\tPrec 100.000% (98.731%)\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.3360 (0.3360)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.240% \n",
      "Epoch: [230][0/391]\tTime 0.266 (0.266)\tData 0.234 (0.234)\tLoss 0.0170 (0.0170)\tPrec 100.000% (100.000%)\n",
      "Epoch: [230][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0115 (0.0323)\tPrec 99.219% (98.917%)\n",
      "Epoch: [230][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0612 (0.0317)\tPrec 97.656% (98.947%)\n",
      "Epoch: [230][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0283 (0.0339)\tPrec 99.219% (98.848%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.4076 (0.4076)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.690% \n",
      "Epoch: [231][0/391]\tTime 0.250 (0.250)\tData 0.218 (0.218)\tLoss 0.0504 (0.0504)\tPrec 98.438% (98.438%)\n",
      "Epoch: [231][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0114 (0.0395)\tPrec 100.000% (98.623%)\n",
      "Epoch: [231][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0056 (0.0359)\tPrec 100.000% (98.799%)\n",
      "Epoch: [231][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0178 (0.0366)\tPrec 100.000% (98.754%)\n",
      "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.3088 (0.3088)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.860% \n",
      "Epoch: [232][0/391]\tTime 0.340 (0.340)\tData 0.308 (0.308)\tLoss 0.0081 (0.0081)\tPrec 100.000% (100.000%)\n",
      "Epoch: [232][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0035 (0.0327)\tPrec 100.000% (98.933%)\n",
      "Epoch: [232][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0372 (0.0336)\tPrec 99.219% (98.877%)\n",
      "Epoch: [232][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0350 (0.0355)\tPrec 98.438% (98.806%)\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.3146 (0.3146)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.960% \n",
      "Epoch: [233][0/391]\tTime 0.282 (0.282)\tData 0.249 (0.249)\tLoss 0.0104 (0.0104)\tPrec 100.000% (100.000%)\n",
      "Epoch: [233][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0591 (0.0353)\tPrec 98.438% (98.778%)\n",
      "Epoch: [233][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0281 (0.0354)\tPrec 99.219% (98.834%)\n",
      "Epoch: [233][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0227 (0.0366)\tPrec 99.219% (98.783%)\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3522 (0.3522)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.350% \n",
      "Epoch: [234][0/391]\tTime 0.265 (0.265)\tData 0.225 (0.225)\tLoss 0.0759 (0.0759)\tPrec 96.094% (96.094%)\n",
      "Epoch: [234][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0234 (0.0313)\tPrec 98.438% (98.909%)\n",
      "Epoch: [234][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0134 (0.0314)\tPrec 99.219% (98.912%)\n",
      "Epoch: [234][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0276 (0.0328)\tPrec 98.438% (98.863%)\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.4002 (0.4002)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.470% \n",
      "Epoch: [235][0/391]\tTime 0.288 (0.288)\tData 0.256 (0.256)\tLoss 0.0283 (0.0283)\tPrec 99.219% (99.219%)\n",
      "Epoch: [235][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0165 (0.0376)\tPrec 100.000% (98.708%)\n",
      "Epoch: [235][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0141 (0.0365)\tPrec 99.219% (98.737%)\n",
      "Epoch: [235][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0070 (0.0354)\tPrec 100.000% (98.770%)\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.4109 (0.4109)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.300% \n",
      "Epoch: [236][0/391]\tTime 0.263 (0.263)\tData 0.223 (0.223)\tLoss 0.0152 (0.0152)\tPrec 99.219% (99.219%)\n",
      "Epoch: [236][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0245 (0.0347)\tPrec 99.219% (98.786%)\n",
      "Epoch: [236][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0150 (0.0351)\tPrec 100.000% (98.791%)\n",
      "Epoch: [236][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0297 (0.0354)\tPrec 99.219% (98.788%)\n",
      "Test: [0/79]\tTime 0.172 (0.172)\tLoss 0.3612 (0.3612)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.840% \n",
      "Epoch: [237][0/391]\tTime 0.283 (0.283)\tData 0.194 (0.194)\tLoss 0.0245 (0.0245)\tPrec 98.438% (98.438%)\n",
      "Epoch: [237][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0124 (0.0337)\tPrec 99.219% (98.778%)\n",
      "Epoch: [237][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0111 (0.0319)\tPrec 99.219% (98.884%)\n",
      "Epoch: [237][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0710 (0.0325)\tPrec 97.656% (98.879%)\n",
      "Test: [0/79]\tTime 0.169 (0.169)\tLoss 0.4108 (0.4108)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.870% \n",
      "Epoch: [238][0/391]\tTime 0.246 (0.246)\tData 0.211 (0.211)\tLoss 0.0248 (0.0248)\tPrec 99.219% (99.219%)\n",
      "Epoch: [238][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0101 (0.0283)\tPrec 100.000% (99.041%)\n",
      "Epoch: [238][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0351 (0.0313)\tPrec 99.219% (98.904%)\n",
      "Epoch: [238][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0043 (0.0324)\tPrec 100.000% (98.905%)\n",
      "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.3038 (0.3038)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.470% \n",
      "Epoch: [239][0/391]\tTime 0.285 (0.285)\tData 0.253 (0.253)\tLoss 0.0194 (0.0194)\tPrec 98.438% (98.438%)\n",
      "Epoch: [239][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.004)\tLoss 0.0242 (0.0332)\tPrec 99.219% (98.863%)\n",
      "Epoch: [239][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0935 (0.0344)\tPrec 96.094% (98.811%)\n",
      "Epoch: [239][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0656 (0.0322)\tPrec 98.438% (98.876%)\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.2853 (0.2853)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.710% \n",
      "Epoch: [240][0/391]\tTime 0.280 (0.280)\tData 0.239 (0.239)\tLoss 0.0493 (0.0493)\tPrec 98.438% (98.438%)\n",
      "Epoch: [240][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0103 (0.0353)\tPrec 100.000% (98.832%)\n",
      "Epoch: [240][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0174 (0.0315)\tPrec 99.219% (98.923%)\n",
      "Epoch: [240][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0163 (0.0337)\tPrec 99.219% (98.881%)\n",
      "Test: [0/79]\tTime 0.166 (0.166)\tLoss 0.4872 (0.4872)\tPrec 89.062% (89.062%)\n",
      " * Prec 89.870% \n",
      "Epoch: [241][0/391]\tTime 0.406 (0.406)\tData 0.366 (0.366)\tLoss 0.0375 (0.0375)\tPrec 98.438% (98.438%)\n",
      "Epoch: [241][100/391]\tTime 0.049 (0.053)\tData 0.001 (0.005)\tLoss 0.0583 (0.0314)\tPrec 98.438% (99.002%)\n",
      "Epoch: [241][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0305 (0.0333)\tPrec 99.219% (98.861%)\n",
      "Epoch: [241][300/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0353 (0.0333)\tPrec 98.438% (98.845%)\n",
      "Test: [0/79]\tTime 0.179 (0.179)\tLoss 0.3753 (0.3753)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.940% \n",
      "Epoch: [242][0/391]\tTime 0.261 (0.261)\tData 0.221 (0.221)\tLoss 0.0354 (0.0354)\tPrec 99.219% (99.219%)\n",
      "Epoch: [242][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0383 (0.0283)\tPrec 98.438% (99.064%)\n",
      "Epoch: [242][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0247 (0.0313)\tPrec 99.219% (98.916%)\n",
      "Epoch: [242][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0391 (0.0322)\tPrec 98.438% (98.925%)\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3760 (0.3760)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.270% \n",
      "Epoch: [243][0/391]\tTime 0.217 (0.217)\tData 0.179 (0.179)\tLoss 0.0082 (0.0082)\tPrec 100.000% (100.000%)\n",
      "Epoch: [243][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.003)\tLoss 0.0484 (0.0325)\tPrec 99.219% (98.909%)\n",
      "Epoch: [243][200/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0674 (0.0311)\tPrec 98.438% (98.954%)\n",
      "Epoch: [243][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0052 (0.0298)\tPrec 100.000% (98.983%)\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.4114 (0.4114)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.750% \n",
      "Epoch: [244][0/391]\tTime 0.384 (0.384)\tData 0.352 (0.352)\tLoss 0.0280 (0.0280)\tPrec 99.219% (99.219%)\n",
      "Epoch: [244][100/391]\tTime 0.050 (0.053)\tData 0.001 (0.005)\tLoss 0.0268 (0.0392)\tPrec 98.438% (98.739%)\n",
      "Epoch: [244][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.003)\tLoss 0.0241 (0.0351)\tPrec 99.219% (98.850%)\n",
      "Epoch: [244][300/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0036 (0.0351)\tPrec 100.000% (98.837%)\n",
      "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.4192 (0.4192)\tPrec 91.406% (91.406%)\n",
      " * Prec 89.460% \n",
      "Epoch: [245][0/391]\tTime 0.261 (0.261)\tData 0.221 (0.221)\tLoss 0.0351 (0.0351)\tPrec 99.219% (99.219%)\n",
      "Epoch: [245][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.003)\tLoss 0.0239 (0.0322)\tPrec 99.219% (98.855%)\n",
      "Epoch: [245][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0789 (0.0333)\tPrec 97.656% (98.826%)\n",
      "Epoch: [245][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0065 (0.0333)\tPrec 100.000% (98.829%)\n",
      "Test: [0/79]\tTime 0.271 (0.271)\tLoss 0.3174 (0.3174)\tPrec 90.625% (90.625%)\n",
      " * Prec 90.040% \n",
      "Epoch: [246][0/391]\tTime 0.347 (0.347)\tData 0.315 (0.315)\tLoss 0.0403 (0.0403)\tPrec 98.438% (98.438%)\n",
      "Epoch: [246][100/391]\tTime 0.042 (0.052)\tData 0.001 (0.004)\tLoss 0.0266 (0.0363)\tPrec 99.219% (98.762%)\n",
      "Epoch: [246][200/391]\tTime 0.042 (0.051)\tData 0.001 (0.003)\tLoss 0.0113 (0.0365)\tPrec 100.000% (98.741%)\n",
      "Epoch: [246][300/391]\tTime 0.042 (0.050)\tData 0.001 (0.002)\tLoss 0.0421 (0.0368)\tPrec 98.438% (98.718%)\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.3806 (0.3806)\tPrec 88.281% (88.281%)\n",
      " * Prec 89.680% \n",
      "Epoch: [247][0/391]\tTime 0.268 (0.268)\tData 0.231 (0.231)\tLoss 0.0090 (0.0090)\tPrec 100.000% (100.000%)\n",
      "Epoch: [247][100/391]\tTime 0.050 (0.052)\tData 0.001 (0.004)\tLoss 0.0292 (0.0317)\tPrec 99.219% (98.886%)\n",
      "Epoch: [247][200/391]\tTime 0.049 (0.051)\tData 0.001 (0.002)\tLoss 0.0206 (0.0323)\tPrec 98.438% (98.881%)\n",
      "Epoch: [247][300/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0080 (0.0327)\tPrec 100.000% (98.905%)\n",
      "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.2417 (0.2417)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.410% \n",
      "Epoch: [248][0/391]\tTime 0.255 (0.255)\tData 0.224 (0.224)\tLoss 0.0555 (0.0555)\tPrec 97.656% (97.656%)\n",
      "Epoch: [248][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.004)\tLoss 0.0466 (0.0352)\tPrec 97.656% (98.793%)\n",
      "Epoch: [248][200/391]\tTime 0.050 (0.050)\tData 0.001 (0.002)\tLoss 0.0278 (0.0350)\tPrec 99.219% (98.768%)\n",
      "Epoch: [248][300/391]\tTime 0.048 (0.050)\tData 0.001 (0.002)\tLoss 0.0065 (0.0355)\tPrec 100.000% (98.793%)\n",
      "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.3375 (0.3375)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.750% \n",
      "Epoch: [249][0/391]\tTime 0.274 (0.274)\tData 0.198 (0.198)\tLoss 0.0043 (0.0043)\tPrec 100.000% (100.000%)\n",
      "Epoch: [249][100/391]\tTime 0.049 (0.052)\tData 0.001 (0.003)\tLoss 0.0071 (0.0320)\tPrec 100.000% (98.956%)\n",
      "Epoch: [249][200/391]\tTime 0.050 (0.051)\tData 0.001 (0.002)\tLoss 0.0842 (0.0317)\tPrec 97.656% (98.916%)\n",
      "Epoch: [249][300/391]\tTime 0.049 (0.050)\tData 0.001 (0.002)\tLoss 0.0224 (0.0305)\tPrec 97.656% (98.931%)\n",
      "Test: [0/79]\tTime 0.169 (0.169)\tLoss 0.2953 (0.2953)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.710% \n",
      "Best accuracy after finetuning the structured-pruned model: 90.240%\n",
      "Saved pruned + fine-tuned model to: result/VGG16_quant/model_pruned_finetuned_pt1.pth.tar\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# print(\"Accuracy of the pre-trained model:\")\n",
    "# validate(testloader, model, criterion)\n",
    "\n",
    "# print(\"\\n Applying 80% Unstructured Pruning:\")\n",
    "# for name, module in model.named_modules():\n",
    "#     if isinstance(module, QuantConv2d):\n",
    "#         prune.random_unstructured(module, name='weight', amount=0.8)\n",
    "        \n",
    "# print(\"Checking the accuracy after performing 80% unstructured pruning:\")\n",
    "# validate(testloader, model, criterion)\n",
    "\n",
    "# print(\"\\n Training the unstructured-pruned model for gaining back accuracy :\")\n",
    "best_prec = 0\n",
    "epochs = 250\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     train(trainloader, model, criterion, optimizer, epoch)\n",
    "#     prec = validate(testloader, model, criterion)\n",
    "#     is_best = prec > best_prec\n",
    "#     best_prec = max(prec, best_prec)\n",
    "    \n",
    "# print(f\"Best accuracy after re-training the unstructured-pruned model:{best_prec:.3f}%\")\n",
    "\n",
    "# for name, module in model.named_modules():\n",
    "#     if isinstance(module, QuantConv2d):\n",
    "#         prune.remove(module, 'weight')\n",
    "\n",
    "print(\"\\n Loading the original model again for structured pruning: \")\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.cuda()\n",
    "\n",
    "print(\"\\nApplying 50% Structured Pruning:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, QuantConv2d):\n",
    "        prune.ln_structured(module, name='weight', amount=0.5, n=1, dim=0)\n",
    "        \n",
    "print(\"Checking the accuracy after performing 50% structured pruning:\")\n",
    "validate(testloader, model, criterion)\n",
    "\n",
    "print(\"\\n Training the structured-pruned model for gaining back accuracy :\")\n",
    "best_prec = 0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    prec = validate(testloader, model, criterion)\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec, best_prec)\n",
    "    \n",
    "print(f\"Best accuracy after finetuning the structured-pruned model: {best_prec:.3f}%\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, QuantConv2d):\n",
    "        prune.remove(module, 'weight')\n",
    "        \n",
    "# --- Save final pruned & fine-tuned model ---\n",
    "save_path = \"result/VGG16_quant/model_pruned_finetuned_pt1.pth.tar\"\n",
    "\n",
    "torch.save({\n",
    "    'state_dict': model.state_dict(),\n",
    "    'best_prec': best_prec,\n",
    "}, save_path)\n",
    "\n",
    "print(\"Saved pruned + fine-tuned model to:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbc57908-01e1-469d-9d13-48b2b0586db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Per-layer sparsity (zeros %) -----\n",
      "features.0: 50.00% sparse\n",
      "features.3: 50.00% sparse\n",
      "features.7: 50.00% sparse\n",
      "features.10: 50.00% sparse\n",
      "features.14: 50.00% sparse\n",
      "features.17: 50.00% sparse\n",
      "features.20: 50.00% sparse\n",
      "features.24: 50.00% sparse\n",
      "features.27: 50.00% sparse\n",
      "features.29: 50.00% sparse\n",
      "features.31: 50.00% sparse\n",
      "features.33: 50.00% sparse\n",
      "features.37: 50.00% sparse\n",
      "features.40: 50.00% sparse\n",
      "features.43: 50.00% sparse\n",
      "\n",
      "----- Total model sparsity -----\n",
      "Overall sparsity: 50.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'features.0': 50.0,\n",
       "  'features.3': 50.0,\n",
       "  'features.7': 50.0,\n",
       "  'features.10': 50.0,\n",
       "  'features.14': 50.0,\n",
       "  'features.17': 50.0,\n",
       "  'features.20': 50.0,\n",
       "  'features.24': 50.0,\n",
       "  'features.27': 50.0,\n",
       "  'features.29': 50.0,\n",
       "  'features.31': 50.0,\n",
       "  'features.33': 50.0,\n",
       "  'features.37': 50.0,\n",
       "  'features.40': 50.0,\n",
       "  'features.43': 50.0},\n",
       " 50.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant/model_pruned_finetuned_pt1.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "def compute_sparsity(model):\n",
    "    total_zeros = 0\n",
    "    total_params = 0\n",
    "    layer_sparsity = {}\n",
    "\n",
    "    print(\"\\n----- Per-layer sparsity (zeros %) -----\")\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantConv2d):\n",
    "            W = module.weight.detach().cpu()\n",
    "            zeros = torch.sum(W == 0).item()\n",
    "            params = W.numel()\n",
    "\n",
    "            sparsity = 100 * zeros / params\n",
    "            layer_sparsity[name] = sparsity\n",
    "            print(f\"{name}: {sparsity:.2f}% sparse\")\n",
    "\n",
    "            total_zeros += zeros\n",
    "            total_params += params\n",
    "\n",
    "    total_sparsity = 100 * total_zeros / total_params\n",
    "    print(\"\\n----- Total model sparsity -----\")\n",
    "    print(f\"Overall sparsity: {total_sparsity:.2f}%\")\n",
    "\n",
    "    return layer_sparsity, total_sparsity\n",
    "\n",
    "compute_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dca8210-bb55-4234-838f-09d546bbbe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Structured Filter Sparsity -----\n",
      "features.0: 32/64 filters removed (50.00%)\n",
      "features.3: 32/64 filters removed (50.00%)\n",
      "features.7: 64/128 filters removed (50.00%)\n",
      "features.10: 64/128 filters removed (50.00%)\n",
      "features.14: 128/256 filters removed (50.00%)\n",
      "features.17: 128/256 filters removed (50.00%)\n",
      "features.20: 128/256 filters removed (50.00%)\n",
      "features.24: 256/512 filters removed (50.00%)\n",
      "features.27: 4/8 filters removed (50.00%)\n",
      "features.29: 4/8 filters removed (50.00%)\n",
      "features.31: 256/512 filters removed (50.00%)\n",
      "features.33: 256/512 filters removed (50.00%)\n",
      "features.37: 256/512 filters removed (50.00%)\n",
      "features.40: 256/512 filters removed (50.00%)\n",
      "features.43: 256/512 filters removed (50.00%)\n"
     ]
    }
   ],
   "source": [
    "def compute_structured_filter_sparsity(model):\n",
    "    print(\"\\n----- Structured Filter Sparsity -----\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantConv2d):\n",
    "            W = module.weight.detach().cpu()    # shape: [C_out, C_in, k, k]\n",
    "            C_out = W.shape[0]\n",
    "\n",
    "            # A filter is dead if *all weights* in that channel = 0\n",
    "            dead_filters = 0\n",
    "            for c in range(C_out):\n",
    "                if torch.sum(W[c]) == 0:\n",
    "                    dead_filters += 1\n",
    "\n",
    "            sparsity = 100 * dead_filters / C_out\n",
    "            print(f\"{name}: {dead_filters}/{C_out} filters removed ({sparsity:.2f}%)\")\n",
    "compute_structured_filter_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60884c88-d29c-40e8-812e-f2ff33d29996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Per-layer sparsity (zeros %) -----\n",
      "features.0: 50.00% sparse\n",
      "features.3: 50.00% sparse\n",
      "features.7: 50.00% sparse\n",
      "features.10: 50.00% sparse\n",
      "features.14: 50.00% sparse\n",
      "features.17: 50.00% sparse\n",
      "features.20: 50.00% sparse\n",
      "features.24: 50.00% sparse\n",
      "features.27: 50.00% sparse\n",
      "features.29: 50.00% sparse\n",
      "features.31: 50.00% sparse\n",
      "features.33: 50.00% sparse\n",
      "features.37: 50.00% sparse\n",
      "features.40: 50.00% sparse\n",
      "features.43: 50.00% sparse\n",
      "\n",
      "----- Total model sparsity -----\n",
      "Overall sparsity: 50.00%\n",
      "----- Per-layer sparsity (%) -----\n",
      "features.0: 50.00%\n",
      "features.3: 50.00%\n",
      "features.7: 50.00%\n",
      "features.10: 50.00%\n",
      "features.14: 50.00%\n",
      "features.17: 50.00%\n",
      "features.20: 50.00%\n",
      "features.24: 50.00%\n",
      "features.27: 50.00%\n",
      "features.29: 50.00%\n",
      "features.31: 50.00%\n",
      "features.33: 50.00%\n",
      "features.37: 50.00%\n",
      "features.40: 50.00%\n",
      "features.43: 50.00%\n",
      "\n",
      "Total sparsity: 50.00%\n",
      "Estimated MACs after pruning: 12656.58 M\n",
      "Pruned model size (int4): 2.95 MB\n",
      "Estimated memory bandwidth reduction: 50.00%\n",
      "\n",
      "Final accuracy after pruning & fine-tuning: 89.71%\n",
      "Number of MACs before pruning:  12656579584\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------------\n",
    "# Load your pruned + fine-tuned model\n",
    "# ------------------------------\n",
    "PATH = \"result/VGG16_quant/model_pruned_finetuned_pt1.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval().cuda()\n",
    "\n",
    "# ---------------- Compute Metrics ----------------\n",
    "\n",
    "# Layer-wise and total sparsity\n",
    "layer_sparsity, total_sparsity = compute_sparsity(model)\n",
    "print(\"----- Per-layer sparsity (%) -----\")\n",
    "for layer, sp in layer_sparsity.items():\n",
    "    print(f\"{layer}: {sp:.2f}%\")\n",
    "print(f\"\\nTotal sparsity: {total_sparsity:.2f}%\")\n",
    "\n",
    "# Compute MACs\n",
    "macs_after = compute_macs_vgg_quant(model)\n",
    "print(f\"Estimated MACs after pruning: {macs_after/1e6:.2f} M\")\n",
    "\n",
    "# Compute model size reduction (assume 4-bit weights for quantized model)\n",
    "pruned_model_size = compute_model_size(model, bits_per_weight=4)\n",
    "print(f\"Pruned model size (int4): {pruned_model_size:.2f} MB\")\n",
    "\n",
    "# Memory bandwidth reduction is roughly proportional to model size reduction\n",
    "print(f\"Estimated memory bandwidth reduction: {total_sparsity:.2f}%\")\n",
    "\n",
    "# Final accuracy\n",
    "def validate(model, testloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "accuracy = validate(model, testloader, criterion)\n",
    "print(f\"\\nFinal accuracy after pruning & fine-tuning: {accuracy:.2f}%\")\n",
    "print('Number of MACs before pruning: ', compute_macs_vgg_quant(model, input_size=(3,32,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07ef76-e133-4e70-8091-886fcc70666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Make sure your model and captured hook are ready\n",
    "x_q = captured['x']               # Already quantized activations from hook\n",
    "weight_q = conv_layer.weight_q    # Already quantized weights\n",
    "\n",
    "# Flatten and convert to CPU for saving\n",
    "x_flat = x_q.detach().cpu().numpy().flatten()\n",
    "w_flat = weight_q.detach().cpu().numpy().flatten()\n",
    "\n",
    "# Save to text files\n",
    "import os\n",
    "save_dir = 'result/VGG16_quant/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "x_path = os.path.join(save_dir, 'feature29_input_activations.txt')\n",
    "w_path = os.path.join(save_dir, 'feature29_weights.txt')\n",
    "\n",
    "# Save as float values\n",
    "with open(x_path, 'w') as f:\n",
    "    for val in x_flat:\n",
    "        f.write(f\"{val}\\n\")\n",
    "\n",
    "with open(w_path, 'w') as f:\n",
    "    for val in w_flat:\n",
    "        f.write(f\"{val}\\n\")\n",
    "\n",
    "print(f\"Saved {x_flat.shape[0]} input activations to {x_path}\")\n",
    "print(f\"Saved {w_flat.shape[0]} weights to {w_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53db0f-ff94-4bc5-bad1-5f73edf16816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bit = 4\n",
    "weight_q = conv_layer.weight_q # quantized value is stored during the training\n",
    "w_alpha = conv_layer.weight_quant.wgt_alpha.data  # alpha is defined in your model already. bring it out here\n",
    "w_delta = 2 * w_alpha / (2**w_bit - 1)    # delta can be calculated by using alpha and w_bit\n",
    "weight_int = torch.round(weight_q / w_delta).to(torch.int32) # w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bit = 4    \n",
    "x = x = captured['x']   # input of the 2nd conv layer\n",
    "x_alpha  = conv_layer.act_alpha.data\n",
    "x_delta = 2 * x_alpha / (2**x_bit - 1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = torch.round(x_q / x_delta).to(torch.int32)\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "\n",
    "output_int =  F.conv2d(x_int.float(), weight_int.float(), bias=None,\n",
    "                      stride=conv_layer.stride, padding=conv_layer.padding)    # output_int can be calculated with conv_int and x_int\n",
    "output_recovered = output_int * (x_delta * w_delta)  # recover with x_delta and w_delta\n",
    "print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
    "conv_ref.weight = model.features[3].weight_q \n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight floating number version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
    "weight = model.features[3].weight\n",
    "mean = weight.data.mean()\n",
    "std = weight.data.std()\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight.add(-mean).div(std))\n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
